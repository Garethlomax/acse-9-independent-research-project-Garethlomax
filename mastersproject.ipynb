{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mastersproject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "mUR7Uvu8WwEb"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/offline_local/mastersproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3b-9exMWkjz"
      },
      "source": [
        "# test implementation of lstm, convlstm and cnn lstm in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mUR7Uvu8WwEb"
      },
      "source": [
        "# IMPORT - TORCH AND MOVING MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMb14uDjWeG7",
        "outputId": "d65f9c40-d23e-4eda-b05f-3495dae6468a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7CqnY3RW_Rb",
        "outputId": "9654e56e-9cb1-4d93-a7db-bbd7e463fc71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# !ls\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 398,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvDANand_K3",
        "colab_type": "code",
        "outputId": "aa167943-73fd-4bdd-c6fe-1809b8cc0910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.995s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN47F0kLKj7J",
        "colab_type": "text"
      },
      "source": [
        "# Snippet to investigate gpu ram \n",
        "\n",
        "\n",
        "CITE THIS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53M1Gf1DKnUF",
        "colab_type": "code",
        "outputId": "11548968-d858-4586-f86b-52304c3d2240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 317.2 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwI0bjfLKr-5",
        "colab_type": "text"
      },
      "source": [
        "# OTHER IMPORTS'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AetKjh8KoRU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-1LkLNBEk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchvision import models\n",
        "# alexnet = models.AlexNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-0M6ZfwEIEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn([1,10,1,64,64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5jC195UEgfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a366b69-c82a-45ef-a044-485ec4acc84e"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyJGZtMHEwM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bdb95142-b973-487d-9058-d7caf15f9218"
      },
      "source": [
        "a = [1,2,3,4,5]\n",
        "print(a[-1:])\n",
        "print(a[:1])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxcGZH8NEgob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88562e4c-f480-4a62-ac3c-57169e89ab96"
      },
      "source": [
        "x[:,-1:,:,:,:].shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeIo5I79EgmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72fd025f-50fa-4d64-8d74-d6a208b28817"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jj9K0iqKYuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd0f797-4d2c-407c-fa42-3b0cf1530df3"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98WsXAiBEk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pwd\n",
        "# %cd ../MovingMNIST-master/\n",
        "# from MovingMNIST import MovingMNIST\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tExiB9XBEk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# printm()\n",
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=False)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=False)\n",
        "# printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO311itUBEk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxJbjX0BElB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn((1,3,256,256))\n",
        "# alexnet(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-DiqxICNDikN"
      },
      "source": [
        "# CUDA CODE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uD6EFhtQDmc7",
        "outputId": "958846ad-9d63-426e-f5a5-c2f1a1b2d91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wA2sMmtzDvj3"
      },
      "source": [
        "# MOVING MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oEqF2KZrYOvW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RfsF_VXAx_Xf",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oWYcd-zmEtgl",
        "colab": {}
      },
      "source": [
        "# len(train_set)\n",
        "# of dimensions, sample, test data / predictordata, frame\n",
        "#train_set[0][2].shape\n",
        "# size = train_set[8999][0].element_size() * train_set[8999][0].nelement() #print(x.element_size() * x.nelement()/ 1000000)\n",
        "# print(size * 9000 * 2/ 1000000)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cw6wDKD-Y7ac",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# examining video sequences.\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][0][i].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][1][i].numpy())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDjblSs5EsvY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkMf7emiZZec",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6vvT7_DcRpo"
      },
      "source": [
        "# LSTM CELL AND MODEL\n",
        "\n",
        "Based on lstm model from weather paper and others. \n",
        "\n",
        "pseudo code for lstm: \n",
        "\n",
        "http://people.idsia.ch/~juergen/lstm/sld024.htm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A9AYovA7cXI-",
        "colab": {}
      },
      "source": [
        "# now we start lstm cell\n",
        "\n",
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 64, 64]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kbiJUARC7QKG",
        "outputId": "d7e4e2ef-1591-4c96-bf53-346804c0788e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "9000/ 20 * 0.6\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFctO7TLDrw",
        "colab_type": "code",
        "outputId": "c0ba376e-1ada-4d3b-caa9-c0b9d4bd94f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 321.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGqsUxMBJtoJ",
        "colab": {}
      },
      "source": [
        "# test1 = (LSTMunit(20,20,5).double()).cuda()\n",
        "# test1 = (LSTMunit(20,20,5).float()).cuda()\n",
        "# # x = torch.randn((20,30,20,32,32)).double()\n",
        "\n",
        "\n",
        "# x = torch.randn((20, 20, 32,32)).double()\n",
        "# x = x.cuda()\n",
        "# print(x.element_size() * x.nelement()/ 1000000)\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "\n",
        "# for i in range(20):\n",
        "#     x, _ = test1(x,x,x)\n",
        "# \"the code you want to test stays here\"\n",
        "# end = time.time()\n",
        "# print(end - start)\n",
        "# ans, _ = test1(x,x,x)\n",
        "# print(ans.shape)\n",
        "# shape = [1,1,8,8]\n",
        "# summary(test1, [(1,224,224),(3,224,224),(3,224,224)])\n",
        "\n",
        "# from torchvision import models\n",
        "# vgg = models.vgg16().to(device)\n",
        "\n",
        "# summary(vgg, (3, 224, 224))\n",
        "\n",
        "# alexnet = models.AlexNet().to(device)\n",
        "# summary(alexnet, (3,224,224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l6I58R442CHb",
        "colab": {}
      },
      "source": [
        "# from pytorch_modelsize import SizeEstimator\n",
        "# se = SizeEstimator(test1, input_size=(20, 20, 32,32))\n",
        "# print(se.estimate_size())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mip77CMw2D7i",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmoXagUwYwXx",
        "colab": {}
      },
      "source": [
        "# #input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "# shape = [20,1,32,32]\n",
        "# \"\"\"TODO: IMPORTANT: STRIDE MUST BE KEPT AT 1 TO NOT DEPRECIATE THE SHAPE OF THE INPUT.\"\"\"\n",
        "# x = torch.randn(shape)\n",
        "# h = torch.randn(shape)\n",
        "# c= torch.randn(shape)\n",
        "\n",
        "# # STRIDE ISNT WORKING - IS ONLY 1. \n",
        "# test = LSTMunit(1, 3, 5, 1)\n",
        "# hout, cout = test(x,h,c)\n",
        "\n",
        "\n",
        "# hout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYWXitQGhii7",
        "colab": {}
      },
      "source": [
        "# hout.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3aG2Xl1BElt",
        "colab_type": "code",
        "outputId": "10ecd9fe-dc4e-490d-c6a7-eb42dfb3d69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "isinstance([], list)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqGgsBQMwpVv"
      },
      "source": [
        "# LSTM FULL UNIT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bMDqSonOo7Ze"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67Hc3mTpwr8e",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "            if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#             elif copy_in[i] == [0,0]:\n",
        "            elif isinstance(copy_in[i], list):\n",
        "\n",
        "                assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "                # if no copying in in alternate format\n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "            else: # copy in the provided vectors\n",
        "                assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "                \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "                empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \n",
        "            else:\n",
        "                internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rC1P1MgAsLPf",
        "colab": {}
      },
      "source": [
        "# bytes = torch.cuda.memory_allocated()\n",
        "# print(\"amount of memory allocated: \", bytes / 1073741824)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJoBkQ0cp3S9",
        "outputId": "7274c0c7-35ca-495f-a3bf-010f85a50e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "shape = [2,4,1,8,8] # batch size, seq length, 1 layer, 32 by 32 image.\n",
        "# import numpy as np\n",
        "# x = (torch.randn(shape).double()).cuda()\n",
        "\n",
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "printm()\n",
        "# test2 = (LSTMmain(shape, 1, 3, 5, [1], test_input = [1], debug = False)).to(device)\n",
        "# test2.cuda()\n",
        "# # x.cuda()\n",
        "# print(\"IS X CUDA NOW?\")\n",
        "# print(x.is_cuda)\n",
        "\n",
        "\n",
        "# print(\"x_shape:\")\n",
        "\n",
        "# print(x.shape)\n",
        "\n",
        "# ans, _ = test2(x, copy_in = False, copy_out = [False, False, False])\n",
        "\n",
        "# ans.shape\n",
        "# ans = ans.double()\n",
        "# t1 = np.zeros(shape)\n",
        "# t1 = torch.tensor(t1, requires_grad = True).to(device)\n",
        "# t1 = t1.cuda()\n",
        "# t1 = torch.FloatTensor([2,4,1,8,8], dtype = torch.float, requires_grad = True)\n",
        "# print(t1.requires_grad)\n",
        "# # print(ans.requires_grad)\n",
        "# # res = torch.autograd.gradcheck(test2, (t1,), eps=1e-4, raise_exception=True)\n",
        "# print(res)\n",
        "# print(res)\n",
        "# torch.autograd.gradcheck(test2, (ans,))\n",
        "# a = list(x.shape)\n",
        "# print(a[:0] + a[1:])\n",
        "\n",
        "# a = torch.randn([20,19,32,32])\n",
        "# for i in range(19):\n",
        "\n",
        "#     print(a[:,i])\n",
        "# a = test2(x)\n",
        "# b = test2(x)\n",
        "\n",
        "\n",
        "\n",
        "# a - b"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.2 GB  | Proc size: 1.9 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2jYzORXfpun0",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3EmcUGjKINT",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQn2rMzeIz50",
        "colab": {}
      },
      "source": [
        "# print(test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eC_TKY5fHfT2",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(param.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZ0mkGyCPsPw",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLJmUSsev_DA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gI4KvWRsvOz2",
        "colab": {}
      },
      "source": [
        "# summary(test2, input_size = (1,4,1,8,8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRD3CpXTDrGG"
      },
      "source": [
        "# ENCODER DECODER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YyQn8-wODw9v",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "class LSTMencdec(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape).cuda()\n",
        "        \n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzRHb4knBBda",
        "colab_type": "text"
      },
      "source": [
        "# one step conditional lstm \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7_szunBEEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LopG6SRbBEmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# structure = np.array([[2,2,2,0],[0,2,2,1]])\n",
        "# shape = [2,4,1,8,8]\n",
        "\n",
        "# x = torch.randn(shape).double()\n",
        "\n",
        "\n",
        "# print(structure.shape)\n",
        "# # this we will also use an input channel no of 1. \n",
        "# # we then anticipate channels 2, 2 -> decoder : 2, 1 -> output. last channel of decoder is 1 as we then need to narrow down \n",
        "# # to parameters of the size we need for output\n",
        "# test = LSTMencdec(structure, 1)\n",
        "# test(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jgCP0n9TIfA",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# a = np.array([[1,2,3,4],[4,5,6,7]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7CQVfd8TOcs",
        "colab": {}
      },
      "source": [
        "# # b = a[0]\n",
        "# b[b!=1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQwsOj4r6OUS"
      },
      "source": [
        "# Dataset and Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AeW7rJXktINC"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-17XQIt95pP8",
        "colab": {}
      },
      "source": [
        "# train_set.\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)\n",
        "# is of shape list(1000), tuple - start and finish. \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C73qzDLrtGTZ",
        "colab": {}
      },
      "source": [
        "# test_set[1][1].shape\n",
        "# input is batch_size, tuple(prev, after), seq(10,), height, width\n",
        "# need to go to batch, seq, chan, height, width.\n",
        "\n",
        "# print(test_set[:][0].shape) # of size 1000, 10, 64, 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DyY_jsKM6Nx7",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# # test_set[0][0].shape\n",
        "# class SequenceDataset(Dataset):\n",
        "#     \"\"\"simple data set wrapper \n",
        "#     for the moving mnist dataset\n",
        "    \n",
        "#     we use this as need to insert channel dimension in the data\"\"\"\n",
        "#     def __init__(self, data, transform = None):\n",
        "        \n",
        "#         self.input_sequence = data[:][0].unsqueeze(2)\n",
        "        \n",
        "#         self.output_sequence = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "        \n",
        "#         assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "# #         print(self.input_sequence.shape)\n",
        "        \n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.input_sequence)\n",
        "    \n",
        "#     def __getitem__(self, i):\n",
        "#         \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "#         This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "#         i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "#         return self.input_sequence[i], self.output_sequence[i]\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hz3G4TczPorI",
        "colab": {}
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"simple data set wrapper \n",
        "    for the moving mnist dataset\n",
        "    \n",
        "    we use this as need to insert channel dimension in the data\"\"\"\n",
        "    def __init__(self, predictor, ground_truth, transform = None):\n",
        "        \n",
        "        self.input_sequence = predictor\n",
        "        \n",
        "        self.output_sequence = ground_truth\n",
        "        \n",
        "        assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "#         print(self.input_sequence.shape)\n",
        "        \n",
        "       \n",
        "    def __len__(self):\n",
        "        return len(self.input_sequence)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "        This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "        i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "        return self.input_sequence[i], self.output_sequence[i]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAApP8cVeRvm",
        "colab_type": "text"
      },
      "source": [
        "## HDF5 DATASET AND INITIALISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaI2yzIJeU2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYzmYB8IeZW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTOK8ayBZnEY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYd_uHpidAXR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sNDP894hbI3D"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNgX8A8FanJx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ybkVJYQacyZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEDjKHnLZt6y",
        "colab": {}
      },
      "source": [
        "# test[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmnFDE8ktM_l"
      },
      "source": [
        "## Data Loader\n",
        "num_workers parameter is useful for bypassing large data set issues. this is very relevant for future work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0hbwutatPLh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ytcMV0ijN6Dj"
      },
      "source": [
        "#TRAINING FUNCTIONS - ENC DEC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6CfY7VYvKok"
      },
      "source": [
        "## Load in datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1hUMkrgvHdQ",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LfR9IrqGFiOi",
        "outputId": "4eced075-dbeb-44c0-a351-70cf6f54d349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = 10\n",
        "dummy_array = np.zeros(data)\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state = 32)\n",
        "generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "indices = [(a, b) for a, b in generator][0]\n",
        "print(indices)\n",
        "\n",
        "# for a, b in generator:\n",
        "#     print(a, b)"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([9, 4, 6, 5, 3, 1, 2, 0, 8]), array([7]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8v6uTVC5v-92"
      },
      "source": [
        "## Function to shuffle the dataset \n",
        "\n",
        "layter include kfold validaiton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEEzT6P4wEU8",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nDowTHWTL-C2"
      },
      "source": [
        "## Unsqueeze data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6d-Tge-wL_vE",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezTixs-hOmpD"
      },
      "source": [
        "## Produce Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q74kom8gPgdK",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SljjESCUcWf4"
      },
      "source": [
        "### Test of produce dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izVjB0kvIDIY",
        "outputId": "4de3bd77-1091-4e88-defb-07ab8e222412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## test \n",
        "printm()\n",
        "train, valid = initialise_dataset_HDF5()\n",
        "\n",
        "printm()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.2 GB  | Proc size: 1.9 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n",
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n",
            "Gen RAM Free: 11.2 GB  | Proc size: 1.9 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HnTh0evpbpOR",
        "outputId": "b651c66d-e68c-4e45-a31e-d9a83e6a6ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# plt.imshow(train[0][0][0][0])\n",
        "train[0][0][0][0][30][30]"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.7695, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0vGAsBmUccBp"
      },
      "source": [
        "As we can see below, works as intended. note that shape now has a channel , which must be specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XzkEbuwtZMM4",
        "colab": {}
      },
      "source": [
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][0][i][0].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][1][i][0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mp2UWjXiM_bb"
      },
      "source": [
        "## patch size alteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvak3Av-IpF6",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dX5ny69vOpe"
      },
      "source": [
        "## Define Training Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCMtJpBXXvdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6afdb91a-3bc2-40ac-bdd5-cfd4334f6866"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "        loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "        # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, criterion = nn.MSELoss()):\n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad: # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x.to(device) # send to cuda.\n",
        "            y.to(device)\n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction, y)\n",
        "            \n",
        "            \n",
        "    return validloss, validaccuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_ssim\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_ssim\"+str(epoch)+\".pth\")\n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6youMS9_Z3CA",
        "colab_type": "text"
      },
      "source": [
        "# TRYING TO UNDERSTAND WHY IVE SOMEHOW BUILT A SHIT AUTOENCODER BY ACCIDENT \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s38IdcKKbLBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 20, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtRC7hZfZ9l-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "7cd91534-515a-46a5-9de9-f2b13ee96e13"
      },
      "source": [
        "for x, y in train_loader:\n",
        "    \n",
        "    print(y.shape)\n",
        "    print(y[:,:1,:,:,:].shape)\n",
        "    plt.figure()\n",
        "    \n",
        "    plt.imshow(x[:,-1:,:,:,:][0][0][0])\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.imshow(x[:,:1,:,:,:][0][0][0])\n",
        "    plt.figure()\n",
        "    plt.imshow(y[:,:1,:,:,:][0][0][0])\n",
        "    break\n",
        "        \n"
      ],
      "execution_count": 429,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 10, 1, 64, 64])\n",
            "torch.Size([20, 1, 1, 64, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCfp/QShahECIAiIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z5oT3A-OeF_B"
      },
      "source": [
        "# TEST OF FULL ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Gey5vMBEm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, valid = initialise_dataset(train_set)\n",
        "# print(train[0][0].dtype)\n",
        "\n",
        "# dat = DataLoader(train, batch_size = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhpOWkfoBEnA",
        "colab_type": "code",
        "outputId": "69a66f24-efcd-460d-bf82-86a4c76fdabd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 431,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/data/models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 431
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3glozZf5uttV",
        "colab_type": "code",
        "outputId": "90d282eb-4475-497e-f2ef-78b0241ecaec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 432,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.2 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mmHSWVWjN-TJ",
        "outputId": "ef84b559-174b-4d30-f2a0-d2916ce0063f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# changed \n",
        "structure = np.array([[6,12,0,0],[0,12,6,1]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1).to(device)\n",
        "print(\"seq length\", test_model.decoder.seq_length)\n",
        "# optim = torch.optim.Adam(test_model.parameters())\n",
        "\n",
        "\n",
        "\n",
        "# train_enc = train_enc_dec(test_model,)\n",
        "\n",
        "\n",
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\n",
        "\n",
        "model, optimizer = train_main(test_model, 1, train, valid, epochs = 50, batch_size = 50)\n",
        "\n",
        "\n",
        "# torch.save(optimizer.state_dict(), F\"Finished_opt_bce.pth\")\n",
        "# torch.save(model.state_dict(), F\"Finished_mod_bce.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 433,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 0\n",
            "12 12\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[ 6 12]\n",
            "[12  1]\n",
            "[False, True]\n",
            "[True, False]\n",
            "seq length 1\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: -0.12551810476490705\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: -0.14803754458609936\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: -0.2640547118221218\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: -0.34401383080634595\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: -0.40948509790400367\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: -0.4614202366085327\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: -0.5006175588099924\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: -0.528399175225153\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: -0.5518944921010752\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: -0.5711944730016063\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: -0.5876937719819756\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: -0.601573148248134\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: -0.6130304391544327\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: -0.6230075004039903\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: -0.6322555868992512\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: -0.6405992758872558\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: -0.6475930104579007\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: -0.6538738728760083\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: -0.6598674505062084\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: -0.6648809697577448\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: -0.6696394856780752\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: -0.6740733086629219\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: -0.6779682407353761\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: -0.6814974206044013\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: -0.6839997944864606\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: -0.6872049079735126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: -0.6899531888511305\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: -0.6926370803735672\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: -0.6950212962729114\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: -0.6974098607364905\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: -0.6994571992577983\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: -0.701149353247186\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: -0.7033248361481783\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: -0.7053303368504721\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: -0.7071655193144539\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: -0.7088768486912227\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: -0.7102416692248863\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: -0.7116589498541369\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: -0.7130554912778095\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: -0.714304913889174\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: -0.7154479780253316\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: -0.7166779240982881\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: -0.7177282696182461\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: -0.7188482074891928\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: -0.719587149477909\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: -0.7204057509449016\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: -0.7210010381371761\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: -0.7219454968226783\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: -0.7226934556350514\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: -0.7236309033797658\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: -0.7244212206579262\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: -0.7252969748562617\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: -0.726080283082999\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: -0.7267324713579489\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: -0.727383220399424\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: -0.7280201350596428\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: -0.728733786786468\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: -0.7293802661139901\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: -0.7298467323638368\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: -0.7305001404846276\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: -0.7310861952277887\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: -0.7316122572169494\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: -0.7320692383387573\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: -0.7325948350903017\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: -0.7330607218031467\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: -0.733561013788627\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: -0.7339590710700423\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: -0.7343737989397007\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: -0.7348188017432509\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: -0.7352818621880775\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: -0.7356339358518156\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: -0.7360049205680234\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: -0.7364119532570658\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: -0.7367787461008256\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: -0.7371867419727872\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: -0.7375166851084458\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: -0.7379119629494362\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: -0.7383837977916891\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: -0.7387074739081074\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: -0.7390410105298109\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: -0.7392579284569806\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: -0.7396000966720921\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: -0.739938618150902\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: -0.7402084918598225\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: -0.740499565785473\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: -0.7408277947513141\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: -0.7410358526607858\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: -0.741293883138487\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: -0.7415407944693255\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: -0.7418291182749003\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: -0.7421029214465836\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: -0.7423973760499366\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: -0.7424825947775144\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: -0.7426552626526886\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: -0.742953496108614\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: -0.7432024586809444\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: -0.7434422546574574\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: -0.7436332813056656\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: -0.7438687583618115\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: -0.7441557121175315\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: -0.744399581680254\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: -0.7446830372588389\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: -0.74486725223911\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: -0.7450308503309636\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: -0.7452966424120178\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: -0.7454243521509739\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: -0.7455651899873271\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: -0.7457379078284939\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: -0.7458588323140732\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: -0.7459896423252541\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: -0.7462308732657612\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: -0.7464150784362321\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: -0.74666651666612\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: -0.7467309257400893\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: -0.7469554258610476\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: -0.7470623040904871\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: -0.7471974263659262\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: -0.7473273651557688\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: -0.74747308379084\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: -0.7475950417405507\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: -0.7478543748457996\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: -0.7480196912359848\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: -0.7481889169259878\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: -0.748298250401747\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: -0.7483967979360313\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: -0.7485897816431867\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: -0.7487505733705878\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: -0.7488709087254438\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: -0.7490104354271764\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: -0.7491629370292138\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: -0.7492841965328709\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: -0.7493349692867451\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: -0.7494592175407156\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: -0.749572699546297\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: -0.7496251963401633\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: -0.7497383818364286\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: -0.7497879969776873\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: -0.749914578162826\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: -0.7500486999996192\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: -0.7501644008883288\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: -0.7502906496053584\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: -0.750415164942293\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: -0.7505009040195839\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: -0.7505535993930702\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: -0.7506856939282176\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: -0.7508161568109927\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: -0.7509435589744865\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: -0.7510374272833034\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: -0.7511425722476931\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: -0.7511973943724888\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: -0.7512650102825216\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: -0.7513550498947966\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: -0.7515146036163808\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: -0.7516232041393579\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: -0.7517200532518693\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: -0.7518641241307532\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: -0.7518738570971558\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: -0.7519846800294758\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: -0.7520349203632241\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: -0.7521162533100141\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: -0.7522186353849522\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: -0.7522815633709156\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: -0.7665311550059973\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: -0.7651145863641469\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: -0.7688182702813654\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: -0.7651394067595604\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: -0.7642838476693756\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: -0.7655189885604406\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: -0.7659575083935943\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: -0.7649617621806428\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: -0.7649683649493495\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: -0.7644205809597442\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: -0.7641311552219978\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: -0.7641327253266746\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: -0.7641204639285402\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: -0.7645038306354286\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: -0.7653290395072532\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: -0.765440174245655\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: -0.7651711103169463\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: -0.7647032974029421\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: -0.7646042859887352\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: -0.7642584825168205\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: -0.7643459199707924\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: -0.7641956811246223\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: -0.764221562409408\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: -0.7642301340037356\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: -0.7642102721891189\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: -0.7640133619374156\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: -0.7642622338791235\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: -0.7638879395315145\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: -0.7639417779171234\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: -0.7636766493987387\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: -0.7636764045226145\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: -0.7639836022523275\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: -0.7641286805702127\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: -0.7641527948566019\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: -0.764126521934203\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: -0.7640979015687619\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: -0.7643045657771851\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: -0.7645018899550713\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: -0.7646295528541247\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: -0.7646503972620742\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: -0.7647238839078376\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: -0.7645495644958302\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: -0.7644586511171859\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: -0.7644664773156471\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: -0.7644224810323723\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: -0.7644788321191228\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: -0.7644532077064354\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: -0.7644265246543637\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: -0.764518006546776\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: -0.7645052348002261\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: -0.7645587376737446\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: -0.764510380125453\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: -0.7643246235088584\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: -0.7644073929069433\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: -0.7643446263223286\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: -0.7642954189956447\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: -0.7642227152562944\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: -0.7642811719736106\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: -0.7642468109608136\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: -0.7643410045049034\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: -0.7644488415414087\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: -0.764443278911259\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: -0.7643664118867025\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: -0.7644471954539431\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: -0.7645479845407958\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: -0.7644444253783147\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: -0.7644995820129877\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: -0.7644789519889268\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: -0.7645172744136012\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: -0.7644443579838226\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: -0.7643275876592094\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: -0.7643335663590632\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: -0.7645136324447477\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: -0.7645366668090909\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: -0.7645346540554541\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: -0.7645570037199413\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: -0.7645870003893979\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: -0.7645997173738124\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: -0.7646527956813433\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: -0.7647243843015197\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: -0.7646308538846227\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: -0.7646607682244969\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: -0.7646331850422642\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: -0.7646997047626198\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: -0.7647481896906182\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: -0.7647961258688909\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: -0.764724310093623\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: -0.764649368352691\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: -0.7646431254913248\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: -0.7646034690945915\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: -0.7647088067405546\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: -0.764650902009455\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: -0.7646527995645586\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: -0.7646467885679065\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: -0.764677296438501\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: -0.7646209483829353\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: -0.7646909203438237\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: -0.7646284746171806\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: -0.7646599203070582\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: -0.7646392321705529\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: -0.7646216343312224\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: -0.7645435508287409\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: -0.7645164518914709\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: -0.7644881912944317\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: -0.764560956665865\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: -0.7645870796586639\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: -0.7645306109852086\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: -0.7645551168789553\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: -0.7644897949056416\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: -0.7644898081105892\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: -0.7645820372338917\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: -0.7646013887531727\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: -0.7646657757355457\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: -0.7645947524014918\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: -0.7646414571767602\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: -0.7646464303049886\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: -0.7646734564201371\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: -0.7646929546245214\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: -0.7646057287495281\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: -0.7646062469369147\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: -0.7646833673215523\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: -0.7647078589966142\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: -0.7647414156381103\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: -0.7647260454723306\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: -0.7647414877146953\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: -0.7647080634763735\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: -0.7646603588253181\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: -0.764609986442655\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: -0.7646761179422283\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: -0.7646578202602072\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: -0.7647050924171116\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: -0.7646670184386987\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: -0.7646549046278905\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: -0.7646163266007825\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: -0.764630269725328\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: -0.7646353869072091\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: -0.7646366427718697\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: -0.7646752536579705\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: -0.7647566497922089\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: -0.7647742751220716\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: -0.7647423602288945\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: -0.7646976400883034\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: -0.764664888668006\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: -0.7646352094965027\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: -0.7646240391355157\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: -0.7646261335843332\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: -0.7645786435863131\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: -0.7645788208087453\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: -0.7645474621692634\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: -0.7645570160859126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: -0.7644875662786359\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: -0.7644939985927096\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: -0.7644688288717798\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: -0.7644978624034714\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: -0.7644787154440357\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: -0.7644434529727385\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: -0.7644343366776281\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: -0.764416447353422\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: -0.7644679453210532\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: -0.7645007436555937\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: -0.7645274545798644\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: -0.7645934181460421\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: -0.7578640694945602\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: -0.7645785040290597\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: -0.7623145338829335\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: -0.7640455594213597\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: -0.763360393571404\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: -0.762623252572154\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: -0.7629123476000187\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: -0.7623346121086911\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: -0.7633404689427177\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: -0.7633811275360992\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: -0.7639547530965384\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: -0.7642995483009584\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: -0.7644157802900307\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: -0.764375346837203\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: -0.7644558596452298\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: -0.764077422114961\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: -0.7640092789718234\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: -0.7639693366253657\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: -0.763956785041461\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: -0.7639669451618835\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: -0.7643557898597445\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: -0.7643429189139578\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: -0.7644114992506372\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: -0.7639594883462911\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: -0.7639814903743749\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: -0.7638873308810172\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: -0.7642730892474181\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: -0.7644143822998338\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: -0.7645525124257636\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: -0.7647427850304902\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: -0.7645500192097991\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: -0.7641472614869073\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: -0.7641469414485007\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: -0.7642049961131735\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: -0.764015021791084\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: -0.7640640848019621\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: -0.7645256909042397\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: -0.7645742720044694\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: -0.7646105261446667\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: -0.7645987665722194\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: -0.7646851047416214\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: -0.764512103298175\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: -0.7643915721177463\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: -0.7643542618536965\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: -0.764511648291666\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: -0.7645962990439784\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: -0.7646611145679792\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: -0.7646020891536636\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: -0.7645426792194014\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: -0.7643807088271845\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: -0.7643931956972153\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: -0.7643829939367104\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: -0.7643587327687926\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: -0.7641928455637022\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: -0.7642471870687727\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: -0.764297947236075\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: -0.7641607732325779\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: -0.764411432671752\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: -0.7646152201202159\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: -0.7646249547722281\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: -0.7646904887749476\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: -0.7645814960906457\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: -0.7645744537469833\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: -0.7646757450688069\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: -0.7646821557030248\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: -0.7647250065649402\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: -0.7646105899920128\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: -0.7647465456949023\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: -0.7649396779246319\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: -0.7648701106881745\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: -0.7648492672510856\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: -0.7648295456218768\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: -0.7647632368673921\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: -0.7646194988558642\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: -0.7646878405921842\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: -0.7648061123651433\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: -0.7648319473738793\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: -0.7647321359439994\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: -0.7646258548892457\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: -0.7645680635509764\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: -0.7646232693476334\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: -0.7646938895894354\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: -0.7645962558652101\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: -0.7645651138126994\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: -0.7646452720881294\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: -0.7646908621438195\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: -0.7647428731775239\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: -0.7648387149628225\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: -0.7649174755226241\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: -0.7648377806232738\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: -0.7648492192468767\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: -0.7647822585634133\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: -0.7647705948584235\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: -0.7647804420045876\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: -0.7648055252374724\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: -0.7648327779298132\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: -0.7647789689362512\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: -0.7647525966086169\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: -0.7647412091859864\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: -0.7646927566010604\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: -0.7647254880295289\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: -0.7648008869282906\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: -0.7647823486560965\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: -0.7647946601414812\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: -0.7647180937221442\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: -0.7647786125074121\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: -0.7648101244610386\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: -0.7647732264446835\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: -0.7647684265084732\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: -0.7647169682131211\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: -0.7646527343620582\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: -0.7646155537977883\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: -0.7645963878674078\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: -0.7645902055751369\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: -0.7645906603670959\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: -0.7646578829541055\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: -0.7647261147925941\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: -0.7647222466048259\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: -0.7646497217547231\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: -0.764615759598514\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: -0.7645769713233659\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: -0.7645972767312864\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: -0.7645930094409816\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: -0.7646188159547396\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: -0.764699414348633\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: -0.7647538354105464\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: -0.7646963741314108\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: -0.7646822367280148\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: -0.7647238095211719\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: -0.764660112303215\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: -0.7646614362007892\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: -0.7646280847880537\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: -0.7645759738657774\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: -0.764635837780557\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: -0.7646473133157302\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: -0.7646142519353185\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: -0.7646038522022761\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: -0.7646051651451748\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: -0.7646675814503788\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: -0.7646984039959841\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: -0.7646196008806962\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: -0.7646542156852955\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: -0.7646424321919141\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: -0.764679351914104\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: -0.7647127141910621\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: -0.7646944408353724\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: -0.7646471218348128\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: -0.7646099092256015\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: -0.7646227728961101\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: -0.7646510384670968\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: -0.7646794757069141\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: -0.7646797946867092\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: -0.7646251109182146\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: -0.7645683395194982\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: -0.7645716333602612\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: -0.764575592400709\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: -0.7645536228407582\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: -0.7645526263295529\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: -0.7645376389359116\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: -0.7645727714711217\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: -0.7646278325019672\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: -0.7646034658515325\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: -0.7656206417328245\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: -0.7694837085088739\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: -0.7715350483368045\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: -0.768341418208303\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: -0.7659915347540938\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: -0.7668491378951995\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: -0.7666361095130233\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: -0.7670283600585117\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: -0.7662966637993727\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: -0.7663025244173672\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: -0.7660666938609559\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: -0.7666654410003609\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: -0.7670056637577114\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: -0.7666592964779037\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: -0.766677676003743\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: -0.7666941364955209\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: -0.7666774136195034\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: -0.7660300775472513\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: -0.7661144496266801\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: -0.765634966013622\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: -0.7655957129294244\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: -0.7650680251850901\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: -0.7648075214257826\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: -0.7644029785907791\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: -0.7647546619108374\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: -0.7650084231851582\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: -0.7649855901118114\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: -0.7651008530817212\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: -0.7651344161680416\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: -0.7650089665815295\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: -0.7651930462460919\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: -0.7651019745170126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: -0.7650191457929403\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: -0.7650363913262125\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: -0.7650022083495154\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: -0.7650407380986055\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: -0.7651936395661372\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: -0.7650814924201009\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: -0.7650220932297921\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: -0.7651641471821622\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: -0.7650069674520543\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: -0.7648350438311237\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: -0.7649551404538388\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: -0.7650789285589688\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: -0.7651428666435076\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: -0.7650763178796436\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: -0.7651894149141236\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: -0.7651164613202596\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: -0.7649856345390308\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: -0.7650075089803445\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: -0.7651779265757813\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: -0.7649993419805057\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: -0.7651080745010235\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: -0.7649985056118426\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: -0.7651232703420817\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: -0.7651902268316207\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: -0.7651343693935403\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: -0.7651862380719262\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: -0.7650260222219535\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: -0.7651092121324403\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: -0.7653042141801725\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: -0.7654328394997009\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: -0.7655084931758707\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: -0.7654350052453069\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: -0.7654608356129334\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: -0.7654415440813765\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: -0.7654214214432906\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: -0.7654283245295781\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: -0.7654798779152527\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: -0.7655291807900612\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: -0.7655433700635039\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: -0.7655222568084918\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: -0.7654566683446915\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: -0.7654089787100177\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: -0.7653851534779479\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: -0.7654338245334473\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: -0.7655049405759878\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: -0.7654539670888646\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: -0.7655624929550102\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: -0.7654335789217049\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: -0.7654883170825676\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: -0.7654090343997999\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: -0.7653094364227055\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: -0.7653366478525415\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: -0.7653401245489373\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: -0.7652906142170209\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: -0.7652058300005481\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: -0.7651888185184516\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: -0.7651118128659872\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: -0.7651242538951902\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: -0.765136986154508\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: -0.7651000141648765\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: -0.7649596236520917\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: -0.7650038080072304\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: -0.7650732513504079\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: -0.7650410270539738\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: -0.7650916345546384\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: -0.7650818576355127\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: -0.7650725702183557\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: -0.7651142813448089\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: -0.765103073469815\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: -0.7650509196767394\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: -0.7650505466815973\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: -0.7651456303251334\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: -0.7650563560556485\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: -0.7650402942044806\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: -0.7650472352273536\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: -0.7651042762332254\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: -0.7651048004185587\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: -0.7650019752261925\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: -0.7649677905755127\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: -0.7649622152220182\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: -0.7648726701681341\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: -0.7649641200459625\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: -0.7650038395129634\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: -0.7650876796449182\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: -0.7650960408539409\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: -0.7650845280861688\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: -0.7651448348085684\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: -0.765129611400868\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: -0.765110791536642\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: -0.7651544251003668\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: -0.7652078292878962\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: -0.7651772534635745\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: -0.7651664308277848\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: -0.7651394622460309\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: -0.7651146915802155\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: -0.7650634759034376\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: -0.7650309316498296\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: -0.7650654698267408\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: -0.7650235719784393\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: -0.765084902354033\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: -0.7651309897937307\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: -0.7650924790457362\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: -0.76511498104868\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: -0.7650783095005655\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: -0.7651413800323063\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: -0.7651889739095943\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: -0.7651253404514541\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: -0.7651127982949864\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: -0.7651421207102007\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: -0.7650746230396152\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: -0.7651089725544105\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: -0.7651375765213319\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: -0.7651306210281562\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: -0.7651331499185532\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: -0.765121521941919\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: -0.7650324492491635\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: -0.7650253001426597\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: -0.765024428111142\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: -0.7650386169699802\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: -0.765015660703333\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: -0.7650619650694466\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: -0.7650317948655457\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: -0.7650452513598514\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: -0.7650874224567704\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: -0.7650884964613617\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: -0.7651155167009726\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: -0.7651211754181466\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: -0.7651685068235855\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: -0.7651899042420576\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: -0.7651767138779082\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: -0.7685400656641146\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: -0.7686475561186511\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: -0.7673934577147228\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: -0.7660948787618447\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: -0.766351548791909\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: -0.7684400455644403\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: -0.767436340393953\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: -0.7672698319300946\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: -0.7669223298562042\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: -0.7673940130407313\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: -0.7674119000616565\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: -0.7671793430760605\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: -0.7674852027232466\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: -0.7681258790567946\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: -0.7681264016787693\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: -0.7675250143560598\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: -0.7670186287434786\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: -0.7675169224512222\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: -0.767639191100933\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: -0.7678639758257678\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: -0.7672346201430406\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: -0.767658059386844\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: -0.7677053164248104\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: -0.7675285029884135\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: -0.7673854130162013\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: -0.7673369313115036\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: -0.7670861023910818\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: -0.7671852546963974\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: -0.7672351068518419\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: -0.7670771077541455\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: -0.767104818296904\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: -0.7666367490931687\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: -0.7665221812476396\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: -0.7662229905488265\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: -0.7658745251665493\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: -0.766109785763752\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: -0.7661735011971857\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: -0.7660085188390324\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: -0.7659902854990552\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: -0.7658450459703052\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: -0.7658796696014922\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: -0.7656517229574628\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: -0.7657790811361836\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: -0.7655809601096468\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: -0.7656152355278462\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: -0.76572546779634\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: -0.7655391396010564\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: -0.7654345097317905\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: -0.7654505623879725\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: -0.7654039575350131\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: -0.7653545901476103\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: -0.7652557860386473\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: -0.7653584340124701\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: -0.7651946410932213\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: -0.7650702789813224\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: -0.7650203877465337\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: -0.764921378415503\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: -0.7649477087311323\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: -0.7648779818183256\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: -0.7648409055368113\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: -0.7647497834687503\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: -0.7648919782284246\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: -0.7648344165453461\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: -0.7647618457214198\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: -0.7646976156838882\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: -0.7646903584437721\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: -0.7646698107564328\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: -0.7646659624324716\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: -0.7646378244848355\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: -0.7646318968488222\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: -0.7646147363658304\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: -0.7646281025425726\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: -0.7646238337204697\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: -0.7645124046866284\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: -0.7644541275386183\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: -0.7644903857440934\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: -0.7644955181375659\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: -0.7645599656990993\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: -0.7646190975164828\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: -0.7646113864405024\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: -0.764660710690129\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: -0.7646461605265452\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: -0.7646299844414182\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: -0.7645745340676714\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: -0.7646497423240602\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: -0.7646382159059801\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: -0.7647659744077546\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: -0.7648353686685186\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: -0.7647581552182448\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: -0.7648311398285259\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: -0.7648249267149783\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: -0.7648964744822537\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: -0.7648378542440725\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: -0.7647932937356052\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: -0.7647881292534464\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: -0.7648489792020566\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: -0.7649203576223448\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: -0.7649025556586407\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: -0.7649236439451944\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: -0.7649248097056417\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: -0.7650106636777924\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: -0.7650931330659843\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: -0.7650449870218928\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: -0.7650392400297987\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: -0.7650837178309382\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: -0.7650633401625114\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: -0.7651370528320465\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: -0.7650655396116575\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: -0.7650025695115421\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: -0.7650578242197967\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: -0.765028416141423\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: -0.7649627537177592\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: -0.7649587348053063\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: -0.7648752921727667\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: -0.7649101855387144\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: -0.764932782435451\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: -0.7648592364341213\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: -0.7648571679820567\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: -0.7648446731555488\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: -0.7648362356760282\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: -0.764882260592293\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: -0.7648026097732932\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: -0.764793111171296\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: -0.7648066814512857\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: -0.764841035500511\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: -0.7647401830656537\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: -0.7647262487678733\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: -0.7647267087114358\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: -0.7646759370730131\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: -0.7646290593034443\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: -0.76465657737698\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: -0.7646577363746404\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: -0.7646105243931492\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: -0.764666267135126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: -0.7646575154407581\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: -0.7646947235596756\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: -0.7646830074017222\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: -0.7647629246015999\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: -0.7647074509635272\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: -0.7647270939101064\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: -0.764702229939764\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: -0.7646781657012275\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: -0.7646551797392502\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: -0.76464982801506\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: -0.764638669060422\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: -0.7647144456550697\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: -0.7647063097401214\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: -0.7646808684993315\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: -0.7646967579217475\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: -0.7646414737574623\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: -0.7646383948427341\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: -0.7646043133438336\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: -0.7646102166433852\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: -0.7646018069726286\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: -0.764625700284416\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: -0.7645729435470755\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: -0.7645430515678145\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: -0.764574160115167\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: -0.7646227144266493\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: -0.7646918213745245\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: -0.7646965451922044\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: -0.764709707618474\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: -0.7653999760196598\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: -0.767892950964876\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: -0.7651616132127551\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: -0.7649929168211205\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: -0.7648253178073177\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: -0.7658968770850594\n",
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-433-9a4a4308fd10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-427-23a321582f51>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(model, params, train, valid, epochs, batch_size)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtrain_enc_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-427-23a321582f51>\u001b[0m in \u001b[0;36mtrain_enc_dec\u001b[0;34m(model, optimizer, dataloader, loss_func)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# differentiates to find minimum.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m#         printm()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLKD_Cy6qqD8",
        "colab_type": "code",
        "outputId": "a3781a05-c46c-4c27-94bf-715def0f8b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = LSTMencdec_onestep(structure, 1).to(device)\n",
        "test_model.load_state_dict(torch.load(F\"Test_ssim4.pth\"))\n",
        "test_model.eval()\n",
        "\n",
        "\n"
      ],
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 0\n",
            "12 12\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[ 6 12]\n",
            "[12  1]\n",
            "[False, True]\n",
            "[True, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMencdec_onestep(\n",
              "  (encoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(6, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(12, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(12, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(12, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(12, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elrw2cLWztFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o = 0\n",
        "for a, b in train_loader:\n",
        "#     o += 1\n",
        "# #     print(y.shape)\n",
        "# #     print(y[:,:1,:,:,:].shape)\n",
        "# #     plt.figure()\n",
        "# #     plt.imshow(x[:,-1:,:,:,:][0][0][0])\n",
        "# #     plt.figure()\n",
        "# #     plt.imshow(y[:,:1,:,:,:][0][0][0])\n",
        "#     if o == 3:\n",
        "#         break\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UbcsLGPefcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21XyFadhziml",
        "colab_type": "code",
        "outputId": "f0439721-92ad-42f6-cc35-f2610cb57af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "#     x = test_model(torch.unsqueeze(train[0][0], 0).cuda())\n",
        "    x = test_model(a.cuda())"
      ],
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8jPr5uKUpO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape\n",
        "x = x.cpu()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_1o8PO1SAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "outputId": "ce3c69cc-1bc0-4eb3-96ea-f1d598d068ca"
      },
      "source": [
        "fig, axes = plt.subplots(10, 2, figsize = (2,10))\n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(b[0][i][0], aspect = \"auto\")\n",
        "    axes[i,1].imshow(x[0][i][0], aspect = \"auto\")\n",
        "  "
      ],
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-438-a3fdba69ff11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJgAAAJCCAYAAAA8+A5BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 144x720 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OqeeAjeRTPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[0][0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV5QpmlwFmGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLHga3C0TVi",
        "colab_type": "code",
        "outputId": "f5e7e694-5556-46f7-f20d-480fd522c435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "x = x.cpu()\n",
        "\n",
        "plt.imshow(train[0][1][0][0])\n",
        "plt.figure()\n",
        "pic = plt.imshow(x[0][0][0])\n",
        "\n",
        "k = nn.MSELoss()(train[0][0][-1][0], x[0][0][0])\n",
        "ssim_loss = pytorch_ssim.SSIM()\n",
        "\n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "\n",
        "ssim_value = - ssim_out.data.item()\n",
        "print(ssim_value)\n",
        "print(k)"
      ],
      "execution_count": 440,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "tensor(1.1902, dtype=torch.float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNSwfoU_Ce5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.axes()\n",
        "ax.imshow(x[0][0][0])\n",
        "plt.savefig(\"37_612121_6_epochs.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3NydKqB-cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62N-g91UfiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBgcIbETUhw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8vrB_e20nO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3XwzBgcC_3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model.to(device)\n",
        "for params in test_model.parameters():\n",
        "    print(params.is_cuda)\n",
        "    print(params.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnlctWooTK3",
        "colab_type": "text"
      },
      "source": [
        "# MODEL PRINTING FOR ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1apcf2oWJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn([1,10,1,64,64]).double()\n",
        "# y = test_model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo39qN4HpFs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchviz import make_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRE_QxLIpNy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph = make_dot(y, params = dict(test_model.named_parameters()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzQ4YMZrw5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZxUfjhBrZJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph.render(format = 'pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ga1gBkBqiUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-94sHHGnxSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0__frqSBEnF",
        "colab_type": "text"
      },
      "source": [
        "C:\\Users\\Gareth\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([1, 10, 1, 64, 64])) that is different to the input size (torch.Size([1, 10, 2, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
        "  return F.mse_loss(input, target, reduction=self.reduction) \n",
        "  \n",
        "error message from above - why is there an increase in channle size / number?? \n"
      ]
    }
  ]
}