{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mastersproject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "mUR7Uvu8WwEb"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/offline_local/mastersproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3b-9exMWkjz"
      },
      "source": [
        "# test implementation of lstm, convlstm and cnn lstm in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mUR7Uvu8WwEb"
      },
      "source": [
        "# IMPORT - TORCH AND MOVING MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMb14uDjWeG7",
        "outputId": "d65f9c40-d23e-4eda-b05f-3495dae6468a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7CqnY3RW_Rb",
        "outputId": "f5d64d69-0cfb-4f9d-f6c6-5e2700789fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# !ls\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvDANand_K3",
        "colab_type": "code",
        "outputId": "aa167943-73fd-4bdd-c6fe-1809b8cc0910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.995s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN47F0kLKj7J",
        "colab_type": "text"
      },
      "source": [
        "# Snippet to investigate gpu ram \n",
        "\n",
        "\n",
        "CITE THIS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53M1Gf1DKnUF",
        "colab_type": "code",
        "outputId": "11548968-d858-4586-f86b-52304c3d2240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 317.2 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwI0bjfLKr-5",
        "colab_type": "text"
      },
      "source": [
        "# OTHER IMPORTS'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AetKjh8KoRU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-1LkLNBEk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchvision import models\n",
        "# alexnet = models.AlexNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-0M6ZfwEIEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn([1,10,1,64,64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5jC195UEgfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a366b69-c82a-45ef-a044-485ec4acc84e"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyJGZtMHEwM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bdb95142-b973-487d-9058-d7caf15f9218"
      },
      "source": [
        "a = [1,2,3,4,5]\n",
        "print(a[-1:])\n",
        "print(a[:1])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxcGZH8NEgob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88562e4c-f480-4a62-ac3c-57169e89ab96"
      },
      "source": [
        "x[:,-1:,:,:,:].shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeIo5I79EgmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72fd025f-50fa-4d64-8d74-d6a208b28817"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jj9K0iqKYuO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd0f797-4d2c-407c-fa42-3b0cf1530df3"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98WsXAiBEk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pwd\n",
        "# %cd ../MovingMNIST-master/\n",
        "# from MovingMNIST import MovingMNIST\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tExiB9XBEk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# printm()\n",
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=False)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=False)\n",
        "# printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO311itUBEk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxJbjX0BElB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn((1,3,256,256))\n",
        "# alexnet(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-DiqxICNDikN"
      },
      "source": [
        "# CUDA CODE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uD6EFhtQDmc7",
        "outputId": "958846ad-9d63-426e-f5a5-c2f1a1b2d91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wA2sMmtzDvj3"
      },
      "source": [
        "# MOVING MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oEqF2KZrYOvW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RfsF_VXAx_Xf",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oWYcd-zmEtgl",
        "colab": {}
      },
      "source": [
        "# len(train_set)\n",
        "# of dimensions, sample, test data / predictordata, frame\n",
        "#train_set[0][2].shape\n",
        "# size = train_set[8999][0].element_size() * train_set[8999][0].nelement() #print(x.element_size() * x.nelement()/ 1000000)\n",
        "# print(size * 9000 * 2/ 1000000)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cw6wDKD-Y7ac",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# examining video sequences.\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][0][i].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][1][i].numpy())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDjblSs5EsvY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkMf7emiZZec",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6vvT7_DcRpo"
      },
      "source": [
        "# LSTM CELL AND MODEL\n",
        "\n",
        "Based on lstm model from weather paper and others. \n",
        "\n",
        "pseudo code for lstm: \n",
        "\n",
        "http://people.idsia.ch/~juergen/lstm/sld024.htm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A9AYovA7cXI-",
        "colab": {}
      },
      "source": [
        "# now we start lstm cell\n",
        "\n",
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 64, 64]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kbiJUARC7QKG",
        "outputId": "d7e4e2ef-1591-4c96-bf53-346804c0788e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "9000/ 20 * 0.6\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFctO7TLDrw",
        "colab_type": "code",
        "outputId": "c0ba376e-1ada-4d3b-caa9-c0b9d4bd94f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 321.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGqsUxMBJtoJ",
        "colab": {}
      },
      "source": [
        "# test1 = (LSTMunit(20,20,5).double()).cuda()\n",
        "# test1 = (LSTMunit(20,20,5).float()).cuda()\n",
        "# # x = torch.randn((20,30,20,32,32)).double()\n",
        "\n",
        "\n",
        "# x = torch.randn((20, 20, 32,32)).double()\n",
        "# x = x.cuda()\n",
        "# print(x.element_size() * x.nelement()/ 1000000)\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "\n",
        "# for i in range(20):\n",
        "#     x, _ = test1(x,x,x)\n",
        "# \"the code you want to test stays here\"\n",
        "# end = time.time()\n",
        "# print(end - start)\n",
        "# ans, _ = test1(x,x,x)\n",
        "# print(ans.shape)\n",
        "# shape = [1,1,8,8]\n",
        "# summary(test1, [(1,224,224),(3,224,224),(3,224,224)])\n",
        "\n",
        "# from torchvision import models\n",
        "# vgg = models.vgg16().to(device)\n",
        "\n",
        "# summary(vgg, (3, 224, 224))\n",
        "\n",
        "# alexnet = models.AlexNet().to(device)\n",
        "# summary(alexnet, (3,224,224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l6I58R442CHb",
        "colab": {}
      },
      "source": [
        "# from pytorch_modelsize import SizeEstimator\n",
        "# se = SizeEstimator(test1, input_size=(20, 20, 32,32))\n",
        "# print(se.estimate_size())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mip77CMw2D7i",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmoXagUwYwXx",
        "colab": {}
      },
      "source": [
        "# #input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "# shape = [20,1,32,32]\n",
        "# \"\"\"TODO: IMPORTANT: STRIDE MUST BE KEPT AT 1 TO NOT DEPRECIATE THE SHAPE OF THE INPUT.\"\"\"\n",
        "# x = torch.randn(shape)\n",
        "# h = torch.randn(shape)\n",
        "# c= torch.randn(shape)\n",
        "\n",
        "# # STRIDE ISNT WORKING - IS ONLY 1. \n",
        "# test = LSTMunit(1, 3, 5, 1)\n",
        "# hout, cout = test(x,h,c)\n",
        "\n",
        "\n",
        "# hout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYWXitQGhii7",
        "colab": {}
      },
      "source": [
        "# hout.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3aG2Xl1BElt",
        "colab_type": "code",
        "outputId": "10ecd9fe-dc4e-490d-c6a7-eb42dfb3d69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "isinstance([], list)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqGgsBQMwpVv"
      },
      "source": [
        "# LSTM FULL UNIT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bMDqSonOo7Ze"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67Hc3mTpwr8e",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, debug = False, save_outputs = True, decoder = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        \n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "            if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#             elif copy_in[i] == [0,0]:\n",
        "            elif isinstance(copy_in[i], list):\n",
        "\n",
        "                assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "                # if no copying in in alternate format\n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "            else: # copy in the provided vectors\n",
        "                assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "                \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "                empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \n",
        "            else:\n",
        "                internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rC1P1MgAsLPf",
        "colab": {}
      },
      "source": [
        "# bytes = torch.cuda.memory_allocated()\n",
        "# print(\"amount of memory allocated: \", bytes / 1073741824)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJoBkQ0cp3S9",
        "outputId": "b1aa0ec0-25e7-4f38-d33a-10774e005e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "shape = [2,4,1,8,8] # batch size, seq length, 1 layer, 32 by 32 image.\n",
        "# import numpy as np\n",
        "# x = (torch.randn(shape).double()).cuda()\n",
        "\n",
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "printm()\n",
        "# test2 = (LSTMmain(shape, 1, 3, 5, [1], test_input = [1], debug = False)).to(device)\n",
        "# test2.cuda()\n",
        "# # x.cuda()\n",
        "# print(\"IS X CUDA NOW?\")\n",
        "# print(x.is_cuda)\n",
        "\n",
        "\n",
        "# print(\"x_shape:\")\n",
        "\n",
        "# print(x.shape)\n",
        "\n",
        "# ans, _ = test2(x, copy_in = False, copy_out = [False, False, False])\n",
        "\n",
        "# ans.shape\n",
        "# ans = ans.double()\n",
        "# t1 = np.zeros(shape)\n",
        "# t1 = torch.tensor(t1, requires_grad = True).to(device)\n",
        "# t1 = t1.cuda()\n",
        "# t1 = torch.FloatTensor([2,4,1,8,8], dtype = torch.float, requires_grad = True)\n",
        "# print(t1.requires_grad)\n",
        "# # print(ans.requires_grad)\n",
        "# # res = torch.autograd.gradcheck(test2, (t1,), eps=1e-4, raise_exception=True)\n",
        "# print(res)\n",
        "# print(res)\n",
        "# torch.autograd.gradcheck(test2, (ans,))\n",
        "# a = list(x.shape)\n",
        "# print(a[:0] + a[1:])\n",
        "\n",
        "# a = torch.randn([20,19,32,32])\n",
        "# for i in range(19):\n",
        "\n",
        "#     print(a[:,i])\n",
        "# a = test2(x)\n",
        "# b = test2(x)\n",
        "\n",
        "\n",
        "\n",
        "# a - b"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.7 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2jYzORXfpun0",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3EmcUGjKINT",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQn2rMzeIz50",
        "colab": {}
      },
      "source": [
        "# print(test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eC_TKY5fHfT2",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(param.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZ0mkGyCPsPw",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLJmUSsev_DA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gI4KvWRsvOz2",
        "colab": {}
      },
      "source": [
        "# summary(test2, input_size = (1,4,1,8,8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRD3CpXTDrGG"
      },
      "source": [
        "# ENCODER DECODER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YyQn8-wODw9v",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "class LSTMencdec(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape).cuda()\n",
        "        \n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzRHb4knBBda",
        "colab_type": "text"
      },
      "source": [
        "# one step conditional lstm \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7_szunBEEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LopG6SRbBEmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# structure = np.array([[2,2,2,0],[0,2,2,1]])\n",
        "# shape = [2,4,1,8,8]\n",
        "\n",
        "# x = torch.randn(shape).double()\n",
        "\n",
        "\n",
        "# print(structure.shape)\n",
        "# # this we will also use an input channel no of 1. \n",
        "# # we then anticipate channels 2, 2 -> decoder : 2, 1 -> output. last channel of decoder is 1 as we then need to narrow down \n",
        "# # to parameters of the size we need for output\n",
        "# test = LSTMencdec(structure, 1)\n",
        "# test(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jgCP0n9TIfA",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# a = np.array([[1,2,3,4],[4,5,6,7]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7CQVfd8TOcs",
        "colab": {}
      },
      "source": [
        "# # b = a[0]\n",
        "# b[b!=1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQwsOj4r6OUS"
      },
      "source": [
        "# Dataset and Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AeW7rJXktINC"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-17XQIt95pP8",
        "colab": {}
      },
      "source": [
        "# train_set.\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)\n",
        "# is of shape list(1000), tuple - start and finish. \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C73qzDLrtGTZ",
        "colab": {}
      },
      "source": [
        "# test_set[1][1].shape\n",
        "# input is batch_size, tuple(prev, after), seq(10,), height, width\n",
        "# need to go to batch, seq, chan, height, width.\n",
        "\n",
        "# print(test_set[:][0].shape) # of size 1000, 10, 64, 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DyY_jsKM6Nx7",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# # test_set[0][0].shape\n",
        "# class SequenceDataset(Dataset):\n",
        "#     \"\"\"simple data set wrapper \n",
        "#     for the moving mnist dataset\n",
        "    \n",
        "#     we use this as need to insert channel dimension in the data\"\"\"\n",
        "#     def __init__(self, data, transform = None):\n",
        "        \n",
        "#         self.input_sequence = data[:][0].unsqueeze(2)\n",
        "        \n",
        "#         self.output_sequence = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "        \n",
        "#         assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "# #         print(self.input_sequence.shape)\n",
        "        \n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.input_sequence)\n",
        "    \n",
        "#     def __getitem__(self, i):\n",
        "#         \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "#         This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "#         i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "#         return self.input_sequence[i], self.output_sequence[i]\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hz3G4TczPorI",
        "colab": {}
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"simple data set wrapper \n",
        "    for the moving mnist dataset\n",
        "    \n",
        "    we use this as need to insert channel dimension in the data\"\"\"\n",
        "    def __init__(self, predictor, ground_truth, transform = None):\n",
        "        \n",
        "        self.input_sequence = predictor\n",
        "        \n",
        "        self.output_sequence = ground_truth\n",
        "        \n",
        "        assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "#         print(self.input_sequence.shape)\n",
        "        \n",
        "       \n",
        "    def __len__(self):\n",
        "        return len(self.input_sequence)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "        This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "        i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "        return self.input_sequence[i], self.output_sequence[i]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAApP8cVeRvm",
        "colab_type": "text"
      },
      "source": [
        "## HDF5 DATASET AND INITIALISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaI2yzIJeU2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYzmYB8IeZW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTOK8ayBZnEY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYd_uHpidAXR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sNDP894hbI3D"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNgX8A8FanJx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ybkVJYQacyZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEDjKHnLZt6y",
        "colab": {}
      },
      "source": [
        "# test[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmnFDE8ktM_l"
      },
      "source": [
        "## Data Loader\n",
        "num_workers parameter is useful for bypassing large data set issues. this is very relevant for future work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0hbwutatPLh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ytcMV0ijN6Dj"
      },
      "source": [
        "#TRAINING FUNCTIONS - ENC DEC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6CfY7VYvKok"
      },
      "source": [
        "## Load in datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1hUMkrgvHdQ",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LfR9IrqGFiOi",
        "outputId": "3b0d1415-d640-4e02-8f6d-422f62dbad3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = 10\n",
        "dummy_array = np.zeros(data)\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state = 32)\n",
        "generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "indices = [(a, b) for a, b in generator][0]\n",
        "print(indices)\n",
        "\n",
        "# for a, b in generator:\n",
        "#     print(a, b)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([9, 4, 6, 5, 3, 1, 2, 0, 8]), array([7]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8v6uTVC5v-92"
      },
      "source": [
        "## Function to shuffle the dataset \n",
        "\n",
        "layter include kfold validaiton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEEzT6P4wEU8",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nDowTHWTL-C2"
      },
      "source": [
        "## Unsqueeze data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6d-Tge-wL_vE",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezTixs-hOmpD"
      },
      "source": [
        "## Produce Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q74kom8gPgdK",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SljjESCUcWf4"
      },
      "source": [
        "### Test of produce dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izVjB0kvIDIY",
        "outputId": "c782931a-97d3-49e9-9582-3348e55cbb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## test \n",
        "printm()\n",
        "train, valid = initialise_dataset_HDF5()\n",
        "\n",
        "printm()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.7 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n",
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n",
            "Gen RAM Free: 11.6 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HnTh0evpbpOR",
        "outputId": "c943b07d-244a-44f8-f350-829592764649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# plt.imshow(train[0][0][0][0])\n",
        "train[0][0][0][0][30][30]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.7695, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0vGAsBmUccBp"
      },
      "source": [
        "As we can see below, works as intended. note that shape now has a channel , which must be specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XzkEbuwtZMM4",
        "colab": {}
      },
      "source": [
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][0][i][0].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][1][i][0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mp2UWjXiM_bb"
      },
      "source": [
        "## patch size alteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvak3Av-IpF6",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dX5ny69vOpe"
      },
      "source": [
        "## Define Training Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCMtJpBXXvdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e4d9a94-da2e-4e85-cdda-fce85b3cce17"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        #last image sequence.\n",
        "        loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "        # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        tot_loss += loss \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, criterion = nn.MSELoss()):\n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad: # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x.to(device) # send to cuda.\n",
        "            y.to(device)\n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction, y)\n",
        "            \n",
        "            \n",
        "    return validloss, validaccuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader)\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_train_onestep\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_train_model_onestep\"+str(epoch)+\".pth\")\n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z5oT3A-OeF_B"
      },
      "source": [
        "# TEST OF FULL ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Gey5vMBEm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, valid = initialise_dataset(train_set)\n",
        "# print(train[0][0].dtype)\n",
        "\n",
        "# dat = DataLoader(train, batch_size = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhpOWkfoBEnA",
        "colab_type": "code",
        "outputId": "0dc55940-913b-4550-ab94-e57b3e0133cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/data/models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3glozZf5uttV",
        "colab_type": "code",
        "outputId": "430c81df-ccdb-4091-f2b7-da9609e7c28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.5 GB  | Proc size: 2.1 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mmHSWVWjN-TJ",
        "outputId": "2298cc79-5f7c-4eba-e1ed-0cabac9d0a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "structure = np.array([[4,8,0],[0,8,1]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1).to(device)\n",
        "\n",
        "# optim = torch.optim.Adam(test_model.parameters())\n",
        "\n",
        "\n",
        "\n",
        "# train_enc = train_enc_dec(test_model,)\n",
        "\n",
        "\n",
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\n",
        "\n",
        "model, optimizer = train_main(test_model, 1, train, valid, epochs = 20, batch_size = 50)\n",
        "\n",
        "\n",
        "# torch.save(optimizer.state_dict(), F\"Finished_opt_bce.pth\")\n",
        "# torch.save(model.state_dict(), F\"Finished_mod_bce.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 0\n",
            "8 8\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[4 8]\n",
            "[8 1]\n",
            "[False, True]\n",
            "[True, False]\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(1.0247, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.9943, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.9880, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.9849, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.9812, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.9779, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.9730, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.9545, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.9384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.9213, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.9057, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.8917, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.8830, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.8730, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.8683, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.8600, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.8525, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.8437, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.8404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.8341, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.8311, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.8253, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.8209, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.8146, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.8104, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.8074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.8040, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.8010, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.7988, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.7962, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.7938, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.7925, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.7914, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.7905, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.7890, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.7875, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.7864, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.7846, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.7840, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.7817, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.7804, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.7791, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.7781, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.7781, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.7765, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.7754, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.7738, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.7718, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.7716, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.7709, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.7696, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.7685, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.7685, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.7676, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.7674, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.7668, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.7668, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.7664, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.7652, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.7644, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.7634, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.7624, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.7620, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.7609, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.7603, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.7594, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.7587, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.7572, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.7569, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.7553, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.7552, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.7547, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.7546, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.7544, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.7543, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.7536, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.7536, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.7535, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.7531, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.7525, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.7521, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.7513, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.7509, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.7506, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.7503, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.7500, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.7499, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.7499, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.7492, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.7493, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.7488, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.7485, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.7482, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.7478, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.7482, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.7478, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.7477, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.7474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.7472, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.7469, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.7470, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.7468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.7467, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.7465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.7463, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.7456, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.7451, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.7446, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.7441, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.7438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.7431, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.7430, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.7426, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.7416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.7417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.7416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.7411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.7407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.7408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.7405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.7405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.7401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.7400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.7394, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.7392, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.7391, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.7391, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.7389, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.7389, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.7389, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.7388, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.7385, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.7385, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.7384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.7383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.7384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.7381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.7375, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.7375, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.7372, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.7369, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.7367, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.7362, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.7360, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.7361, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.7360, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.7361, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.7355, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.7351, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.7350, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.7346, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.7346, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.7345, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.7342, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.7340, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.7341, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.7339, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.7338, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.7336, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.7331, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.7330, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.7252, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.7167, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.7090, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.7126, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.7136, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.7095, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.7091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.7098, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.7097, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.7104, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.7091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.7102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.7088, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.7096, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.7108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.7104, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.7112, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.7108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.7112, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.7125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.7134, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.7148, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.7142, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.7152, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.7139, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.7140, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.7143, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.7151, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.7152, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.7128, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.7135, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.7120, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.7125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.7104, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.7098, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.7096, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.7094, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.7096, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.7107, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.7107, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.7109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.7097, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.7107, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.7108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.7114, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.7109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.7105, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.7100, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.7109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.7105, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.7107, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.7100, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.7096, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.7098, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.7102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.7105, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.7109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.7116, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.7119, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.7115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.7116, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.7115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.7118, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.7115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.7114, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.7108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.7111, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.7109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.7105, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.7109, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.7112, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.7115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.7115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.7118, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.7120, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.7122, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.7123, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.7124, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.7124, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.7124, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.7128, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.7131, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.7129, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.7130, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.7129, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.7126, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.7127, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.7125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.7124, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.7124, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.7125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.7125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.7123, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.7121, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.7119, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.7119, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.7119, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.7117, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.7118, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.7113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.7111, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.7111, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.7108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.7106, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.7102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.7100, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.7102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.7103, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.7102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.7100, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.7098, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.7099, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.7099, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.7096, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.7095, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.7089, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.7090, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.7092, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.7093, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.7091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.7092, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.7092, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.7090, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.7091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.7091, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.7094, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.7095, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.7096, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.7095, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.7095, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.7012, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.7343, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.7143, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.7144, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.7126, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.7132, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.7122, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.7110, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.7120, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.7148, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.7181, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.7161, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.7193, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.7206, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.7199, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.7184, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.7173, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.7139, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.7121, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.7101, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.7089, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.7076, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.7058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.7063, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.7063, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.7080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.7075, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.7072, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.7063, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.7066, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.7062, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.7061, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.7051, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.7057, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.7071, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.7083, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.7087, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.7088, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.7089, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.7076, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.7075, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.7083, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.7081, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.7079, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.7083, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.7084, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.7080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.7078, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.7080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.7083, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.7078, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.7079, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.7083, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.7086, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.7085, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.7088, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.7081, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.7076, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.7078, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.7076, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.7070, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.7067, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.7070, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.7064, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.7068, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.7068, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.7068, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.7071, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.7073, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.7079, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.7071, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.7067, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.7067, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.7072, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.7079, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.7080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.7079, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.7077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.7074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.7075, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.7072, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.7071, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.7063, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.7065, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.7065, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.7064, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.7061, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.7060, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.7058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.7057, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.7051, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.7058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.7058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.7058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.7059, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.7060, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.7058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.7056, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.7056, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.7051, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.7052, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.7051, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.7049, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.7048, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.7046, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.7045, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.7046, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.7045, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.7049, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.7050, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.7049, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.7049, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.7050, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.7051, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.7050, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.7050, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.7048, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.7050, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.7049, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.7049, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.7052, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.7056, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.7054, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.7053, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.7057, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.7056, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.7056, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.7328, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.6954, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.6941, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.6986, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.6926, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.6911, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.6937, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.6933, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.6935, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.6952, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.6981, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.6975, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.6981, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.6989, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.7034, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.7030, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.7050, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.7055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-611932508417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-3196efc6ad73>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(model, params, train, valid, epochs, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mtrain_enc_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-3196efc6ad73>\u001b[0m in \u001b[0;36mtrain_enc_dec\u001b[0;34m(model, optimizer, dataloader, loss_func)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# steps forward the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# uses loss.backward() to give gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# loss is negative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLKD_Cy6qqD8",
        "colab_type": "code",
        "outputId": "eb0e94d8-3e3d-45ef-d3c6-2193d68da442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = LSTMencdec(structure, 1).to(device)\n",
        "test_model.load_state_dict(torch.load(F\"Test_train_model_onestep2.pth\"))\n",
        "test_model.eval()\n",
        "\n",
        "\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 0\n",
            "8 8\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[4 8]\n",
            "[8 1]\n",
            "[False, True]\n",
            "[True, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMencdec(\n",
              "  (encoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elrw2cLWztFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.imshow(train[0][0][0][0])\n",
        "train[0][0].shape\n",
        "train[0][0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21XyFadhziml",
        "colab_type": "code",
        "outputId": "e8a2c2af-cf6b-4f32-e296-c75b14f5e541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    x = test_model(torch.unsqueeze(train[0][0], 0).cuda())\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_1o8PO1SAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dabdc2cd-0595-46d0-dec8-f6833b8d6b29"
      },
      "source": [
        "for i in range(10):\n",
        "    plt.figure()\n",
        "    plt.imshow(train[0][0][i][0])"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfJEAKfgiAVLwRQKk4IsESMEXCdD/A3PzITN1YFohAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nswT9P0CiyV7ofrbXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3yxBDr5Zghx8swT9PzLtD/ueh4tQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmyXIhW+WIBe+WYL+Hzt1No4v/FoLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJYHUQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCfp/QShahECIAiIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OqeeAjeRTPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76757273-31d7-4459-a592-1560a16f859b"
      },
      "source": [
        "train[0][0].shape"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLHga3C0TVi",
        "colab_type": "code",
        "outputId": "884fcd1d-8b4b-466e-e01c-b649554d7968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "x = x.cpu()\n",
        "\n",
        "plt.imshow(train[0][0][0][0])\n",
        "plt.figure()\n",
        "plt.imshow(x[0][0][0])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc598955828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8vrB_e20nO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3XwzBgcC_3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model.to(device)\n",
        "for params in test_model.parameters():\n",
        "    print(params.is_cuda)\n",
        "    print(params.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnlctWooTK3",
        "colab_type": "text"
      },
      "source": [
        "# MODEL PRINTING FOR ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1apcf2oWJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn([1,10,1,64,64]).double()\n",
        "# y = test_model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo39qN4HpFs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchviz import make_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRE_QxLIpNy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph = make_dot(y, params = dict(test_model.named_parameters()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzQ4YMZrw5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZxUfjhBrZJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph.render(format = 'pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ga1gBkBqiUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-94sHHGnxSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0__frqSBEnF",
        "colab_type": "text"
      },
      "source": [
        "C:\\Users\\Gareth\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([1, 10, 1, 64, 64])) that is different to the input size (torch.Size([1, 10, 2, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
        "  return F.mse_loss(input, target, reduction=self.reduction) \n",
        "  \n",
        "error message from above - why is there an increase in channle size / number?? \n"
      ]
    }
  ]
}