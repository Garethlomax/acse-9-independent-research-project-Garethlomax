{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mastersproject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "mUR7Uvu8WwEb"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/offline_local/mastersproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3b-9exMWkjz"
      },
      "source": [
        "# test implementation of lstm, convlstm and cnn lstm in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mUR7Uvu8WwEb"
      },
      "source": [
        "# IMPORT - TORCH AND MOVING MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMb14uDjWeG7",
        "outputId": "3077026a-fcb8-4430-d9bb-8a443829885e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7CqnY3RW_Rb",
        "outputId": "4e68f87c-92f1-438d-96ab-50108f431034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# !ls\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvDANand_K3",
        "colab_type": "code",
        "outputId": "821703b0-cf8f-450f-ec15-917fb3d7610f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.433s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN47F0kLKj7J",
        "colab_type": "text"
      },
      "source": [
        "# Snippet to investigate gpu ram \n",
        "\n",
        "\n",
        "CITE THIS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53M1Gf1DKnUF",
        "colab_type": "code",
        "outputId": "ab1e6a2c-7f4d-48ec-ae05-7733b29681fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 303.6 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwI0bjfLKr-5",
        "colab_type": "text"
      },
      "source": [
        "# OTHER IMPORTS'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AetKjh8KoRU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-1LkLNBEk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "alexnet = models.AlexNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98WsXAiBEk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pwd\n",
        "# %cd ../MovingMNIST-master/\n",
        "# from MovingMNIST import MovingMNIST\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tExiB9XBEk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# printm()\n",
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=False)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=False)\n",
        "# printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO311itUBEk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxJbjX0BElB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn((1,3,256,256))\n",
        "# alexnet(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-DiqxICNDikN"
      },
      "source": [
        "# CUDA CODE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uD6EFhtQDmc7",
        "outputId": "15b6cc10-41d2-47ec-8ef7-1a73aff686ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wA2sMmtzDvj3"
      },
      "source": [
        "# MOVING MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oEqF2KZrYOvW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RfsF_VXAx_Xf",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oWYcd-zmEtgl",
        "colab": {}
      },
      "source": [
        "# len(train_set)\n",
        "# of dimensions, sample, test data / predictordata, frame\n",
        "#train_set[0][2].shape\n",
        "# size = train_set[8999][0].element_size() * train_set[8999][0].nelement() #print(x.element_size() * x.nelement()/ 1000000)\n",
        "# print(size * 9000 * 2/ 1000000)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cw6wDKD-Y7ac",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# examining video sequences.\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][0][i].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][1][i].numpy())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDjblSs5EsvY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkMf7emiZZec",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6vvT7_DcRpo"
      },
      "source": [
        "# LSTM CELL AND MODEL\n",
        "\n",
        "Based on lstm model from weather paper and others. \n",
        "\n",
        "pseudo code for lstm: \n",
        "\n",
        "http://people.idsia.ch/~juergen/lstm/sld024.htm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A9AYovA7cXI-",
        "colab": {}
      },
      "source": [
        "# now we start lstm cell\n",
        "\n",
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 64, 64]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kbiJUARC7QKG",
        "outputId": "57f9c701-7615-4f39-f364-566338abeacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "9000/ 20 * 0.6\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFctO7TLDrw",
        "colab_type": "code",
        "outputId": "4e7a219e-c3a4-44d7-f19e-c1aa1ebe0527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.5 GB  | Proc size: 552.8 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGqsUxMBJtoJ",
        "colab": {}
      },
      "source": [
        "# test1 = (LSTMunit(20,20,5).double()).cuda()\n",
        "# test1 = (LSTMunit(20,20,5).float()).cuda()\n",
        "# # x = torch.randn((20,30,20,32,32)).double()\n",
        "\n",
        "\n",
        "# x = torch.randn((20, 20, 32,32)).double()\n",
        "# x = x.cuda()\n",
        "# print(x.element_size() * x.nelement()/ 1000000)\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "\n",
        "# for i in range(20):\n",
        "#     x, _ = test1(x,x,x)\n",
        "# \"the code you want to test stays here\"\n",
        "# end = time.time()\n",
        "# print(end - start)\n",
        "# ans, _ = test1(x,x,x)\n",
        "# print(ans.shape)\n",
        "# shape = [1,1,8,8]\n",
        "# summary(test1, [(1,224,224),(3,224,224),(3,224,224)])\n",
        "\n",
        "# from torchvision import models\n",
        "# vgg = models.vgg16().to(device)\n",
        "\n",
        "# summary(vgg, (3, 224, 224))\n",
        "\n",
        "# alexnet = models.AlexNet().to(device)\n",
        "# summary(alexnet, (3,224,224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l6I58R442CHb",
        "colab": {}
      },
      "source": [
        "# from pytorch_modelsize import SizeEstimator\n",
        "# se = SizeEstimator(test1, input_size=(20, 20, 32,32))\n",
        "# print(se.estimate_size())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mip77CMw2D7i",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmoXagUwYwXx",
        "colab": {}
      },
      "source": [
        "# #input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "# shape = [20,1,32,32]\n",
        "# \"\"\"TODO: IMPORTANT: STRIDE MUST BE KEPT AT 1 TO NOT DEPRECIATE THE SHAPE OF THE INPUT.\"\"\"\n",
        "# x = torch.randn(shape)\n",
        "# h = torch.randn(shape)\n",
        "# c= torch.randn(shape)\n",
        "\n",
        "# # STRIDE ISNT WORKING - IS ONLY 1. \n",
        "# test = LSTMunit(1, 3, 5, 1)\n",
        "# hout, cout = test(x,h,c)\n",
        "\n",
        "\n",
        "# hout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYWXitQGhii7",
        "colab": {}
      },
      "source": [
        "# hout.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3aG2Xl1BElt",
        "colab_type": "code",
        "outputId": "2936d5a9-f5f4-4ebd-d201-c8bc526c10a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "isinstance([], list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqGgsBQMwpVv"
      },
      "source": [
        "# LSTM FULL UNIT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bMDqSonOo7Ze"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67Hc3mTpwr8e",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, debug = False, save_outputs = True, decoder = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        \n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "            if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#             elif copy_in[i] == [0,0]:\n",
        "            elif isinstance(copy_in[i], list):\n",
        "\n",
        "                assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "                # if no copying in in alternate format\n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "            else: # copy in the provided vectors\n",
        "                assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "                \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "                empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \n",
        "            else:\n",
        "                internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rC1P1MgAsLPf",
        "colab": {}
      },
      "source": [
        "# bytes = torch.cuda.memory_allocated()\n",
        "# print(\"amount of memory allocated: \", bytes / 1073741824)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJoBkQ0cp3S9",
        "outputId": "c181991b-1399-4ad6-b22e-eac41f5d7e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "shape = [2,4,1,8,8] # batch size, seq length, 1 layer, 32 by 32 image.\n",
        "# import numpy as np\n",
        "# x = (torch.randn(shape).double()).cuda()\n",
        "\n",
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "printm()\n",
        "# test2 = (LSTMmain(shape, 1, 3, 5, [1], test_input = [1], debug = False)).to(device)\n",
        "# test2.cuda()\n",
        "# # x.cuda()\n",
        "# print(\"IS X CUDA NOW?\")\n",
        "# print(x.is_cuda)\n",
        "\n",
        "\n",
        "# print(\"x_shape:\")\n",
        "\n",
        "# print(x.shape)\n",
        "\n",
        "# ans, _ = test2(x, copy_in = False, copy_out = [False, False, False])\n",
        "\n",
        "# ans.shape\n",
        "# ans = ans.double()\n",
        "# t1 = np.zeros(shape)\n",
        "# t1 = torch.tensor(t1, requires_grad = True).to(device)\n",
        "# t1 = t1.cuda()\n",
        "# t1 = torch.FloatTensor([2,4,1,8,8], dtype = torch.float, requires_grad = True)\n",
        "# print(t1.requires_grad)\n",
        "# # print(ans.requires_grad)\n",
        "# # res = torch.autograd.gradcheck(test2, (t1,), eps=1e-4, raise_exception=True)\n",
        "# print(res)\n",
        "# print(res)\n",
        "# torch.autograd.gradcheck(test2, (ans,))\n",
        "# a = list(x.shape)\n",
        "# print(a[:0] + a[1:])\n",
        "\n",
        "# a = torch.randn([20,19,32,32])\n",
        "# for i in range(19):\n",
        "\n",
        "#     print(a[:,i])\n",
        "# a = test2(x)\n",
        "# b = test2(x)\n",
        "\n",
        "\n",
        "\n",
        "# a - b"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 10.4 GB  | Proc size: 3.5 GB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2jYzORXfpun0",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3EmcUGjKINT",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQn2rMzeIz50",
        "colab": {}
      },
      "source": [
        "# print(test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eC_TKY5fHfT2",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(param.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZ0mkGyCPsPw",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLJmUSsev_DA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gI4KvWRsvOz2",
        "colab": {}
      },
      "source": [
        "# summary(test2, input_size = (1,4,1,8,8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRD3CpXTDrGG"
      },
      "source": [
        "# ENCODER DECODER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YyQn8-wODw9v",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "class LSTMencdec(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape).cuda()\n",
        "        \n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LopG6SRbBEmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# structure = np.array([[2,2,2,0],[0,2,2,1]])\n",
        "# shape = [2,4,1,8,8]\n",
        "\n",
        "# x = torch.randn(shape).double()\n",
        "\n",
        "\n",
        "# print(structure.shape)\n",
        "# # this we will also use an input channel no of 1. \n",
        "# # we then anticipate channels 2, 2 -> decoder : 2, 1 -> output. last channel of decoder is 1 as we then need to narrow down \n",
        "# # to parameters of the size we need for output\n",
        "# test = LSTMencdec(structure, 1)\n",
        "# test(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jgCP0n9TIfA",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# a = np.array([[1,2,3,4],[4,5,6,7]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7CQVfd8TOcs",
        "colab": {}
      },
      "source": [
        "# # b = a[0]\n",
        "# b[b!=1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQwsOj4r6OUS"
      },
      "source": [
        "# Dataset and Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AeW7rJXktINC"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-17XQIt95pP8",
        "colab": {}
      },
      "source": [
        "# train_set.\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)\n",
        "# is of shape list(1000), tuple - start and finish. \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C73qzDLrtGTZ",
        "colab": {}
      },
      "source": [
        "# test_set[1][1].shape\n",
        "# input is batch_size, tuple(prev, after), seq(10,), height, width\n",
        "# need to go to batch, seq, chan, height, width.\n",
        "\n",
        "# print(test_set[:][0].shape) # of size 1000, 10, 64, 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DyY_jsKM6Nx7",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# # test_set[0][0].shape\n",
        "# class SequenceDataset(Dataset):\n",
        "#     \"\"\"simple data set wrapper \n",
        "#     for the moving mnist dataset\n",
        "    \n",
        "#     we use this as need to insert channel dimension in the data\"\"\"\n",
        "#     def __init__(self, data, transform = None):\n",
        "        \n",
        "#         self.input_sequence = data[:][0].unsqueeze(2)\n",
        "        \n",
        "#         self.output_sequence = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "        \n",
        "#         assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "# #         print(self.input_sequence.shape)\n",
        "        \n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.input_sequence)\n",
        "    \n",
        "#     def __getitem__(self, i):\n",
        "#         \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "#         This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "#         i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "#         return self.input_sequence[i], self.output_sequence[i]\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hz3G4TczPorI",
        "colab": {}
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"simple data set wrapper \n",
        "    for the moving mnist dataset\n",
        "    \n",
        "    we use this as need to insert channel dimension in the data\"\"\"\n",
        "    def __init__(self, predictor, ground_truth, transform = None):\n",
        "        \n",
        "        self.input_sequence = predictor\n",
        "        \n",
        "        self.output_sequence = ground_truth\n",
        "        \n",
        "        assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "#         print(self.input_sequence.shape)\n",
        "        \n",
        "       \n",
        "    def __len__(self):\n",
        "        return len(self.input_sequence)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "        This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "        i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "        return self.input_sequence[i], self.output_sequence[i]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAApP8cVeRvm",
        "colab_type": "text"
      },
      "source": [
        "## HDF5 DATASET AND INITIALISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaI2yzIJeU2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYzmYB8IeZW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTOK8ayBZnEY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYd_uHpidAXR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sNDP894hbI3D"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNgX8A8FanJx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ybkVJYQacyZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEDjKHnLZt6y",
        "colab": {}
      },
      "source": [
        "# test[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmnFDE8ktM_l"
      },
      "source": [
        "## Data Loader\n",
        "num_workers parameter is useful for bypassing large data set issues. this is very relevant for future work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0hbwutatPLh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ytcMV0ijN6Dj"
      },
      "source": [
        "#TRAINING FUNCTIONS - ENC DEC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6CfY7VYvKok"
      },
      "source": [
        "## Load in datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1hUMkrgvHdQ",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LfR9IrqGFiOi",
        "outputId": "4bf8b97f-0531-4f71-a14d-e5119109e414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = 10\n",
        "dummy_array = np.zeros(data)\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state = 32)\n",
        "generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "indices = [(a, b) for a, b in generator][0]\n",
        "print(indices)\n",
        "\n",
        "# for a, b in generator:\n",
        "#     print(a, b)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([9, 4, 6, 5, 3, 1, 2, 0, 8]), array([7]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8v6uTVC5v-92"
      },
      "source": [
        "## Function to shuffle the dataset \n",
        "\n",
        "layter include kfold validaiton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEEzT6P4wEU8",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nDowTHWTL-C2"
      },
      "source": [
        "## Unsqueeze data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6d-Tge-wL_vE",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezTixs-hOmpD"
      },
      "source": [
        "## Produce Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q74kom8gPgdK",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SljjESCUcWf4"
      },
      "source": [
        "### Test of produce dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izVjB0kvIDIY",
        "outputId": "c8e003d5-1ec4-4d41-9296-6e910d03ff5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## test \n",
        "printm()\n",
        "train, valid = initialise_dataset_HDF5()\n",
        "\n",
        "printm()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 10.4 GB  | Proc size: 3.5 GB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n",
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n",
            "Gen RAM Free: 10.4 GB  | Proc size: 3.5 GB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HnTh0evpbpOR",
        "outputId": "562573a3-4002-4d16-b3ce-82bd376ab08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# plt.imshow(train[0][0][0][0])\n",
        "train[0][0][0][0][30][30]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.7695, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0vGAsBmUccBp"
      },
      "source": [
        "As we can see below, works as intended. note that shape now has a channel , which must be specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XzkEbuwtZMM4",
        "colab": {}
      },
      "source": [
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][0][i][0].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][1][i][0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mp2UWjXiM_bb"
      },
      "source": [
        "## patch size alteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvak3Av-IpF6",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dX5ny69vOpe"
      },
      "source": [
        "## Define Training Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCMtJpBXXvdi",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        loss = loss_func(prediction, y)\n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "        \n",
        "        ##\n",
        "        # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        tot_loss += loss \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, criterion = nn.MSELoss()):\n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad: # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x.to(device) # send to cuda.\n",
        "            y.to(device)\n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction, y)\n",
        "            \n",
        "            \n",
        "    return validloss, validaccuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "    loss_func = nn.MSELoss()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader)\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_train\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_train_model\"+str(epoch)+\".pth\")\n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z5oT3A-OeF_B"
      },
      "source": [
        "# TEST OF FULL ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Gey5vMBEm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, valid = initialise_dataset(train_set)\n",
        "# print(train[0][0].dtype)\n",
        "\n",
        "# dat = DataLoader(train, batch_size = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhpOWkfoBEnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46af51ba-a5f5-4bdb-97af-1a8fbb77fffa"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3glozZf5uttV",
        "colab_type": "code",
        "outputId": "14bc1572-f932-4112-82f0-30019dae166e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 10.4 GB  | Proc size: 3.5 GB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mmHSWVWjN-TJ",
        "outputId": "0e9628e8-bc6f-4b54-a36f-9b1c3ee3e097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "structure = np.array([[4,8,0],[0,8,1]])\n",
        "\n",
        "test_model = LSTMencdec(structure, 1).to(device)\n",
        "\n",
        "# optim = torch.optim.Adam(test_model.parameters())\n",
        "\n",
        "\n",
        "\n",
        "# train_enc = train_enc_dec(test_model,)\n",
        "\n",
        "\n",
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\n",
        "\n",
        "model, optimizer = train_main(test_model, 1, train, valid, epochs = 20, batch_size = 50)\n",
        "\n",
        "\n",
        "torch.save(optimizer.state_dict(), F\"Finished_opt.pth\")\n",
        "torch.save(model.state_dict(), F\"Finished_mod.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 0\n",
            "8 8\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[4 8]\n",
            "[8 1]\n",
            "[False, True]\n",
            "[True, False]\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(1.0092, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(1.0030, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(1.0093, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(1.0035, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(1.0028, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(1.0102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(1.0084, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(1.0134, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(1.0108, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(1.0138, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(1.0149, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(1.0171, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(1.0102, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(1.0081, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(1.0125, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(1.0143, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(1.0115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(1.0113, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(1.0115, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(1.0080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(1.0063, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(1.0068, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(1.0080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(1.0094, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(1.0088, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(1.0080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(1.0080, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(1.0073, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(1.0070, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(1.0074, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(1.0065, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(1.0077, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(1.0058, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(1.0055, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(1.0041, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(1.0027, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(1.0010, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(1.0008, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.9996, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.9978, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.9974, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.9969, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.9966, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.9961, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.9964, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.9962, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.9954, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.9945, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.9947, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.9934, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.9918, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.9910, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.9917, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.9915, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.9904, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.9902, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.9887, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.9881, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.9878, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.9874, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.9867, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.9863, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.9855, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.9838, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.9831, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.9836, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.9833, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.9823, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.9819, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.9820, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.9818, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.9808, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.9803, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.9794, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.9780, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.9773, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.9779, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.9778, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.9779, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.9780, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.9777, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.9774, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.9765, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.9767, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.9763, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.9764, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.9762, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.9760, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.9757, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.9752, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.9746, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.9743, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.9738, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.9735, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.9733, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.9732, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.9730, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.9723, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.9719, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.9717, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.9716, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.9714, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.9713, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.9709, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.9701, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.9699, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.9703, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.9698, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.9695, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.9693, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.9693, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.9696, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.9695, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.9694, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.9688, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.9679, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.9678, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.9678, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.9676, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.9673, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.9670, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.9668, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.9668, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.9667, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.9664, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.9663, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.9665, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.9669, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.9666, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.9665, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.9665, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.9658, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.9654, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.9654, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.9652, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.9656, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.9655, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.9653, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.9651, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.9650, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.9651, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.9648, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.9650, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.9651, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.9651, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.9648, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.9647, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.9643, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.9725, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.9573, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.9380, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.9263, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.9225, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.9366, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.9370, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.9395, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.9487, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.9549, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.9543, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.9548, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.9535, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.9519, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.9523, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.9495, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.9495, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.9477, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.9443, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.9444, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.9451, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.9475, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.9470, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.9478, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.9475, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.9463, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.9470, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.9458, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.9463, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.9460, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.9462, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.9470, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.9467, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.9458, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.9460, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.9458, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.9454, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.9463, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.9473, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.9475, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.9475, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.9476, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.9479, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.9478, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.9476, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.9469, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.9469, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.9460, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.9453, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.9445, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.9440, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.9439, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.9440, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.9439, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.9423, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.9424, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.9421, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.9421, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.9421, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.9424, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.9424, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.9421, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.9424, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.9433, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.9435, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.9441, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.9442, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.9445, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.9442, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.9442, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.9440, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.9436, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.9436, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.9433, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.9718, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.9708, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.9619, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.9524, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.9374, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.9443, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.9439, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.9433, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.9444, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.9441, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.9436, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.9392, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.9386, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.9380, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.9386, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.9372, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.9376, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.9372, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.9391, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.9380, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.9376, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.9378, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.9387, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.9400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.9423, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.9430, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.9433, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.9433, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.9423, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.9421, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.9414, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.9414, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.9414, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.9164, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.9546, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.9503, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.9527, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.9520, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.9390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.9373, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.9450, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.9486, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.9487, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.9516, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.9522, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.9530, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.9537, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.9515, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.9498, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.9485, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.9473, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.9481, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.9489, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.9490, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.9484, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.9486, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.9490, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.9490, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.9495, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.9491, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.9478, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.9463, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.9473, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.9473, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.9468, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.9475, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.9476, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.9441, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.9443, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.9436, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.9422, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.9423, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.9400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.9396, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.9400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.9397, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: tensor(0.9418, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: tensor(0.9414, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: tensor(0.9400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: tensor(0.9395, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: tensor(0.9392, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: tensor(0.9390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: tensor(0.9388, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: tensor(0.9391, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: tensor(0.9390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: tensor(0.9390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: tensor(0.9391, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: tensor(0.9389, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: tensor(0.9615, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: tensor(0.9506, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: tensor(0.9588, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: tensor(0.9441, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: tensor(0.9462, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: tensor(0.9447, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: tensor(0.9455, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: tensor(0.9439, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: tensor(0.9436, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: tensor(0.9450, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: tensor(0.9424, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: tensor(0.9406, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: tensor(0.9389, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: tensor(0.9364, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: tensor(0.9352, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: tensor(0.9356, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: tensor(0.9361, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: tensor(0.9356, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: tensor(0.9356, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: tensor(0.9361, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: tensor(0.9377, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: tensor(0.9382, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: tensor(0.9377, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: tensor(0.9369, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: tensor(0.9368, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: tensor(0.9371, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: tensor(0.9382, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: tensor(0.9385, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: tensor(0.9390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: tensor(0.9386, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: tensor(0.9384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: tensor(0.9382, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: tensor(0.9379, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: tensor(0.9379, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: tensor(0.9388, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: tensor(0.9392, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: tensor(0.9385, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: tensor(0.9385, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: tensor(0.9397, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: tensor(0.9410, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: tensor(0.9416, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: tensor(0.9409, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: tensor(0.9400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: tensor(0.9397, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: tensor(0.9400, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: tensor(0.9397, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: tensor(0.9417, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: tensor(0.9397, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: tensor(0.9395, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: tensor(0.9389, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: tensor(0.9392, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: tensor(0.9388, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: tensor(0.9390, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: tensor(0.9387, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: tensor(0.9384, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: tensor(0.9380, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: tensor(0.9382, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\n",
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-8a8b51416188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-d06e3d47eefc>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(model, params, train, valid, epochs, batch_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtrain_enc_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-d06e3d47eefc>\u001b[0m in \u001b[0;36mtrain_enc_dec\u001b[0;34m(model, optimizer, dataloader, loss_func)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# steps forward the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# uses loss.backward() to give gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# loss is negative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLKD_Cy6qqD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a81fb3ce-7bd6-4566-9d8a-b906ad8dd052"
      },
      "source": [
        "test_model = LSTMencdec(structure, 1).to(device)\n",
        "test_model.load_state_dict(torch.load(F\"Test_train_model3.pth\"))\n",
        "test_model.eval()\n",
        "\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 0\n",
            "8 8\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[4 8]\n",
            "[8 1]\n",
            "[False, True]\n",
            "[True, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMencdec(\n",
              "  (encoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxf): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxc): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Wxo): Conv2d(8, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (Whi): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whf): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Whc): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (Who): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elrw2cLWztFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab98e266-9fa3-4231-a977-ff09aa5c6dfe"
      },
      "source": [
        "# plt.imshow(train[0][0][0][0])\n",
        "train[0][0].shape\n",
        "train[0][0].shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21XyFadhziml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b58be513-52dc-4b3e-a088-ba9409b98b3a"
      },
      "source": [
        "with torch.no_grad():\n",
        "    x = test_model(torch.unsqueeze(train[0][0], 0).cuda())\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_1o8PO1SAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLHga3C0TVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "3b28a97e-0463-4692-9f01-b2fb53312def"
      },
      "source": [
        "x = x.cpu()\n",
        "plt.imshow(train[0][1][0][0])\n",
        "plt.figure()\n",
        "plt.imshow(x[0][-1][0])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7bd03bee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niR8ENeT/A7RFMyG5pNdeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8vrB_e20nO0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "5bb2bc00-a9f0-4c64-a528-e46e0fa48b24"
      },
      "source": [
        "x.requires_grad = False"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-b1aec2d1b8a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach()."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3XwzBgcC_3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_model.to(device)\n",
        "for params in test_model.parameters():\n",
        "    print(params.is_cuda)\n",
        "    print(params.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnlctWooTK3",
        "colab_type": "text"
      },
      "source": [
        "# MODEL PRINTING FOR ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1apcf2oWJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn([1,10,1,64,64]).double()\n",
        "# y = test_model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo39qN4HpFs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchviz import make_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRE_QxLIpNy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph = make_dot(y, params = dict(test_model.named_parameters()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzQ4YMZrw5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZxUfjhBrZJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph.render(format = 'pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ga1gBkBqiUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-94sHHGnxSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0__frqSBEnF",
        "colab_type": "text"
      },
      "source": [
        "C:\\Users\\Gareth\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([1, 10, 1, 64, 64])) that is different to the input size (torch.Size([1, 10, 2, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
        "  return F.mse_loss(input, target, reduction=self.reduction) \n",
        "  \n",
        "error message from above - why is there an increase in channle size / number?? \n"
      ]
    }
  ]
}