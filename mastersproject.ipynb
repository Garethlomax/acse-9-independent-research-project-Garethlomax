{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mastersproject.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "mUR7Uvu8WwEb"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/copy_in_fix/mastersproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G3b-9exMWkjz"
      },
      "source": [
        "# test implementation of lstm, convlstm and cnn lstm in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mUR7Uvu8WwEb"
      },
      "source": [
        "# IMPORT - TORCH AND MOVING MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMb14uDjWeG7",
        "outputId": "9335b713-4fca-40bf-81f3-a684338d8fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7CqnY3RW_Rb",
        "outputId": "36234e21-13b1-481c-88e0-3bfcabb3a8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# !ls\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvDANand_K3",
        "colab_type": "code",
        "outputId": "e7a8050c-840a-482c-e9f4-c70e43192bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 2.037s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN47F0kLKj7J",
        "colab_type": "text"
      },
      "source": [
        "# Snippet to investigate gpu ram \n",
        "\n",
        "\n",
        "CITE THIS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53M1Gf1DKnUF",
        "colab_type": "code",
        "outputId": "31e48af1-50df-4c06-9a8e-b65f76442a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 309.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwI0bjfLKr-5",
        "colab_type": "text"
      },
      "source": [
        "# OTHER IMPORTS'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AetKjh8KoRU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-1LkLNBEk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchvision import models\n",
        "# alexnet = models.AlexNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-0M6ZfwEIEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.randn([1,10,1,64,64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5jC195UEgfX",
        "colab_type": "code",
        "outputId": "7d23b1cb-2a79-40b1-c1f9-acc4d7583209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyJGZtMHEwM9",
        "colab_type": "code",
        "outputId": "49345b6d-7ad0-4b7c-cf5d-04cc22d7e428",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = [1,2,3,4,5]\n",
        "print(a[-1:])\n",
        "print(a[:1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5]\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxcGZH8NEgob",
        "colab_type": "code",
        "outputId": "1712642b-b7c1-4362-c51c-646c985e7785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x[:,-1:,:,:,:].shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeIo5I79EgmD",
        "colab_type": "code",
        "outputId": "ba1e82a8-68e4-4eec-85da-3949915a190a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jj9K0iqKYuO",
        "colab_type": "code",
        "outputId": "7a8c6baa-4bbf-4917-cce1-dc1e6d68abb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98WsXAiBEk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %pwd\n",
        "# %cd ../MovingMNIST-master/\n",
        "# from MovingMNIST import MovingMNIST\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tExiB9XBEk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# printm()\n",
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=False)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=False)\n",
        "# printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO311itUBEk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxJbjX0BElB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn((1,3,256,256))\n",
        "# alexnet(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-DiqxICNDikN"
      },
      "source": [
        "# CUDA CODE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uD6EFhtQDmc7",
        "outputId": "9d24e1fb-bea1-4472-bf0e-c01649a79e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wA2sMmtzDvj3"
      },
      "source": [
        "# MOVING MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oEqF2KZrYOvW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RfsF_VXAx_Xf",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oWYcd-zmEtgl",
        "colab": {}
      },
      "source": [
        "# len(train_set)\n",
        "# of dimensions, sample, test data / predictordata, frame\n",
        "#train_set[0][2].shape\n",
        "# size = train_set[8999][0].element_size() * train_set[8999][0].nelement() #print(x.element_size() * x.nelement()/ 1000000)\n",
        "# print(size * 9000 * 2/ 1000000)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cw6wDKD-Y7ac",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# examining video sequences.\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][0][i].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train_set[0][1][i].numpy())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mDjblSs5EsvY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkMf7emiZZec",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6vvT7_DcRpo"
      },
      "source": [
        "# LSTM CELL AND MODEL\n",
        "\n",
        "Based on lstm model from weather paper and others. \n",
        "\n",
        "pseudo code for lstm: \n",
        "\n",
        "http://people.idsia.ch/~juergen/lstm/sld024.htm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A9AYovA7cXI-",
        "colab": {}
      },
      "source": [
        "# now we start lstm cell\n",
        "\n",
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 64, 64]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kbiJUARC7QKG",
        "outputId": "d7d76c13-03f0-43bc-d430-3786ef61f412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "9000/ 20 * 0.6\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUFctO7TLDrw",
        "colab_type": "code",
        "outputId": "ae54c57b-5de7-41f5-9c20-47c22157fbb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 315.1 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGqsUxMBJtoJ",
        "colab": {}
      },
      "source": [
        "# test1 = (LSTMunit(20,20,5).double()).cuda()\n",
        "# test1 = (LSTMunit(20,20,5).float()).cuda()\n",
        "# # x = torch.randn((20,30,20,32,32)).double()\n",
        "\n",
        "\n",
        "# x = torch.randn((20, 20, 32,32)).double()\n",
        "# x = x.cuda()\n",
        "# print(x.element_size() * x.nelement()/ 1000000)\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "\n",
        "# for i in range(20):\n",
        "#     x, _ = test1(x,x,x)\n",
        "# \"the code you want to test stays here\"\n",
        "# end = time.time()\n",
        "# print(end - start)\n",
        "# ans, _ = test1(x,x,x)\n",
        "# print(ans.shape)\n",
        "# shape = [1,1,8,8]\n",
        "# summary(test1, [(1,224,224),(3,224,224),(3,224,224)])\n",
        "\n",
        "# from torchvision import models\n",
        "# vgg = models.vgg16().to(device)\n",
        "\n",
        "# summary(vgg, (3, 224, 224))\n",
        "\n",
        "# alexnet = models.AlexNet().to(device)\n",
        "# summary(alexnet, (3,224,224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l6I58R442CHb",
        "colab": {}
      },
      "source": [
        "# from pytorch_modelsize import SizeEstimator\n",
        "# se = SizeEstimator(test1, input_size=(20, 20, 32,32))\n",
        "# print(se.estimate_size())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mip77CMw2D7i",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmoXagUwYwXx",
        "colab": {}
      },
      "source": [
        "# #input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "# shape = [20,1,32,32]\n",
        "# \"\"\"TODO: IMPORTANT: STRIDE MUST BE KEPT AT 1 TO NOT DEPRECIATE THE SHAPE OF THE INPUT.\"\"\"\n",
        "# x = torch.randn(shape)\n",
        "# h = torch.randn(shape)\n",
        "# c= torch.randn(shape)\n",
        "\n",
        "# # STRIDE ISNT WORKING - IS ONLY 1. \n",
        "# test = LSTMunit(1, 3, 5, 1)\n",
        "# hout, cout = test(x,h,c)\n",
        "\n",
        "\n",
        "# hout.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYWXitQGhii7",
        "colab": {}
      },
      "source": [
        "# hout.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3aG2Xl1BElt",
        "colab_type": "code",
        "outputId": "cddd3faf-d1da-45d4-f672-72dc78cac12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "isinstance([], list)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rqGgsBQMwpVv"
      },
      "source": [
        "# LSTM FULL UNIT\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bMDqSonOo7Ze"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67Hc3mTpwr8e",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P03McBH6aDKT",
        "colab_type": "text"
      },
      "source": [
        "## garbage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rC1P1MgAsLPf",
        "colab": {}
      },
      "source": [
        "# bytes = torch.cuda.memory_allocated()\n",
        "# print(\"amount of memory allocated: \", bytes / 1073741824)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TJoBkQ0cp3S9",
        "outputId": "45fab338-3246-4ed5-fe28-67a65c3253e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "shape = [2,4,1,8,8] # batch size, seq length, 1 layer, 32 by 32 image.\n",
        "# import numpy as np\n",
        "# x = (torch.randn(shape).double()).cuda()\n",
        "\n",
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "printm()\n",
        "# test2 = (LSTMmain(shape, 1, 3, 5, [1], test_input = [1], debug = False)).to(device)\n",
        "# test2.cuda()\n",
        "# # x.cuda()\n",
        "# print(\"IS X CUDA NOW?\")\n",
        "# print(x.is_cuda)\n",
        "\n",
        "\n",
        "# print(\"x_shape:\")\n",
        "\n",
        "# print(x.shape)\n",
        "\n",
        "# ans, _ = test2(x, copy_in = False, copy_out = [False, False, False])\n",
        "\n",
        "# ans.shape\n",
        "# ans = ans.double()\n",
        "# t1 = np.zeros(shape)\n",
        "# t1 = torch.tensor(t1, requires_grad = True).to(device)\n",
        "# t1 = t1.cuda()\n",
        "# t1 = torch.FloatTensor([2,4,1,8,8], dtype = torch.float, requires_grad = True)\n",
        "# print(t1.requires_grad)\n",
        "# # print(ans.requires_grad)\n",
        "# # res = torch.autograd.gradcheck(test2, (t1,), eps=1e-4, raise_exception=True)\n",
        "# print(res)\n",
        "# print(res)\n",
        "# torch.autograd.gradcheck(test2, (ans,))\n",
        "# a = list(x.shape)\n",
        "# print(a[:0] + a[1:])\n",
        "\n",
        "# a = torch.randn([20,19,32,32])\n",
        "# for i in range(19):\n",
        "\n",
        "#     print(a[:,i])\n",
        "# a = test2(x)\n",
        "# b = test2(x)\n",
        "\n",
        "\n",
        "\n",
        "# a - b"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.7 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2jYzORXfpun0",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k3EmcUGjKINT",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(type(param.data), param.size(), param.is_cuda, param.name)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TQn2rMzeIz50",
        "colab": {}
      },
      "source": [
        "# print(test2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eC_TKY5fHfT2",
        "colab": {}
      },
      "source": [
        "# for param in test2.parameters():\n",
        "#     print(param.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZ0mkGyCPsPw",
        "colab": {}
      },
      "source": [
        "# res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLJmUSsev_DA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gI4KvWRsvOz2",
        "colab": {}
      },
      "source": [
        "# summary(test2, input_size = (1,4,1,8,8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gRD3CpXTDrGG"
      },
      "source": [
        "# ENCODER DECODER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YyQn8-wODw9v",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "class LSTMencdec(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape).cuda()\n",
        "        \n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzRHb4knBBda",
        "colab_type": "text"
      },
      "source": [
        "# one step conditional lstm \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7_szunBEEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,1,64,64]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv7SWbN1dD-D",
        "colab_type": "code",
        "outputId": "de6fb59c-5e90-4d7b-98eb-ad46936373d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "i = [1,3]\n",
        "type(i)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LopG6SRbBEmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# structure = np.array([[2,2,2,0],[0,2,2,1]])\n",
        "# shape = [2,4,1,8,8]\n",
        "\n",
        "# x = torch.randn(shape).double()\n",
        "\n",
        "\n",
        "# print(structure.shape)\n",
        "# # this we will also use an input channel no of 1. \n",
        "# # we then anticipate channels 2, 2 -> decoder : 2, 1 -> output. last channel of decoder is 1 as we then need to narrow down \n",
        "# # to parameters of the size we need for output\n",
        "# test = LSTMencdec(structure, 1)\n",
        "# test(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jgCP0n9TIfA",
        "colab": {}
      },
      "source": [
        "# import numpy as np\n",
        "# a = np.array([[1,2,3,4],[4,5,6,7]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d7CQVfd8TOcs",
        "colab": {}
      },
      "source": [
        "# # b = a[0]\n",
        "# b[b!=1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uQwsOj4r6OUS"
      },
      "source": [
        "# Dataset and Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AeW7rJXktINC"
      },
      "source": [
        "## DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-17XQIt95pP8",
        "colab": {}
      },
      "source": [
        "# train_set.\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)\n",
        "# is of shape list(1000), tuple - start and finish. \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C73qzDLrtGTZ",
        "colab": {}
      },
      "source": [
        "# test_set[1][1].shape\n",
        "# input is batch_size, tuple(prev, after), seq(10,), height, width\n",
        "# need to go to batch, seq, chan, height, width.\n",
        "\n",
        "# print(test_set[:][0].shape) # of size 1000, 10, 64, 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DyY_jsKM6Nx7",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# # test_set[0][0].shape\n",
        "# class SequenceDataset(Dataset):\n",
        "#     \"\"\"simple data set wrapper \n",
        "#     for the moving mnist dataset\n",
        "    \n",
        "#     we use this as need to insert channel dimension in the data\"\"\"\n",
        "#     def __init__(self, data, transform = None):\n",
        "        \n",
        "#         self.input_sequence = data[:][0].unsqueeze(2)\n",
        "        \n",
        "#         self.output_sequence = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "        \n",
        "#         assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "# #         print(self.input_sequence.shape)\n",
        "        \n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.input_sequence)\n",
        "    \n",
        "#     def __getitem__(self, i):\n",
        "#         \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "#         This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "#         i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "#         return self.input_sequence[i], self.output_sequence[i]\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hz3G4TczPorI",
        "colab": {}
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"simple data set wrapper \n",
        "    for the moving mnist dataset\n",
        "    \n",
        "    we use this as need to insert channel dimension in the data\"\"\"\n",
        "    def __init__(self, predictor, ground_truth, transform = None):\n",
        "        \n",
        "        self.input_sequence = predictor\n",
        "        \n",
        "        self.output_sequence = ground_truth\n",
        "        \n",
        "        assert len(self.input_sequence.shape) == 5, \"internal data should be : batch_size, sequence_length, channels, height, width\"\n",
        "        \n",
        "        \n",
        "#         print(self.input_sequence.shape)\n",
        "        \n",
        "       \n",
        "    def __len__(self):\n",
        "        return len(self.input_sequence)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"returns tuple of predictor and result sequence\n",
        "        \n",
        "        This should later be specified to return a valid number of steps in the future\n",
        "        \n",
        "        i.e can specify whether want input of 10 and to predict 5 ect.\"\"\"\n",
        "                \n",
        "\n",
        "        \n",
        "        return self.input_sequence[i], self.output_sequence[i]\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAApP8cVeRvm",
        "colab_type": "text"
      },
      "source": [
        "## HDF5 DATASET AND INITIALISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaI2yzIJeU2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYzmYB8IeZW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTOK8ayBZnEY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RYd_uHpidAXR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sNDP894hbI3D"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNgX8A8FanJx",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ybkVJYQacyZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NEDjKHnLZt6y",
        "colab": {}
      },
      "source": [
        "# test[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmnFDE8ktM_l"
      },
      "source": [
        "## Data Loader\n",
        "num_workers parameter is useful for bypassing large data set issues. this is very relevant for future work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0hbwutatPLh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ytcMV0ijN6Dj"
      },
      "source": [
        "#TRAINING FUNCTIONS - ENC DEC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6CfY7VYvKok"
      },
      "source": [
        "## Load in datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1hUMkrgvHdQ",
        "colab": {}
      },
      "source": [
        "# train_set = MovingMNIST(root='.data/mnist', train=True, download=True)\n",
        "# test_set = MovingMNIST(root='.data/mnist', train=False, download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LfR9IrqGFiOi",
        "outputId": "109185f1-211d-42ee-b375-c26710f34455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = 10\n",
        "dummy_array = np.zeros(data)\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.1, random_state = 32)\n",
        "generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "indices = [(a, b) for a, b in generator][0]\n",
        "print(indices)\n",
        "\n",
        "# for a, b in generator:\n",
        "#     print(a, b)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([9, 4, 6, 5, 3, 1, 2, 0, 8]), array([7]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8v6uTVC5v-92"
      },
      "source": [
        "## Function to shuffle the dataset \n",
        "\n",
        "layter include kfold validaiton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEEzT6P4wEU8",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nDowTHWTL-C2"
      },
      "source": [
        "## Unsqueeze data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6d-Tge-wL_vE",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezTixs-hOmpD"
      },
      "source": [
        "## Produce Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q74kom8gPgdK",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SljjESCUcWf4"
      },
      "source": [
        "### Test of produce dataset function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "izVjB0kvIDIY",
        "outputId": "b2c7c8bb-dbc8-4aa9-c2a6-1808afb4f7ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## test \n",
        "printm()\n",
        "train, valid = initialise_dataset_HDF5()\n",
        "\n",
        "printm()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.7 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n",
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n",
            "Gen RAM Free: 11.6 GB  | Proc size: 2.0 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HnTh0evpbpOR",
        "outputId": "4e233361-06b9-449f-8472-8e008b23efe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# plt.imshow(train[0][0][0][0])\n",
        "train[0][0][0][0][30][30]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.7695, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0vGAsBmUccBp"
      },
      "source": [
        "As we can see below, works as intended. note that shape now has a channel , which must be specified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XzkEbuwtZMM4",
        "colab": {}
      },
      "source": [
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][0][i][0].numpy())\n",
        "\n",
        "# for i in range(10):\n",
        "#     plt.figure()\n",
        "#     plt.imshow(train[0][1][i][0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mp2UWjXiM_bb"
      },
      "source": [
        "## patch size alteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvak3Av-IpF6",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6dX5ny69vOpe"
      },
      "source": [
        "## Define Training Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCMtJpBXXvdi",
        "outputId": "c20e3d0a-5942-45ce-ff32-c81997433f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "        loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "        # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, criterion = nn.MSELoss()):\n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad: # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x.to(device) # send to cuda.\n",
        "            y.to(device)\n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction, y)\n",
        "            \n",
        "            \n",
        "    return validloss, validaccuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_big_kern3_\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_big_kern3_\"+str(epoch)+\".pth\")\n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6youMS9_Z3CA",
        "colab_type": "text"
      },
      "source": [
        "# TRYING TO UNDERSTAND WHY IVE SOMEHOW BUILT A SHIT AUTOENCODER BY ACCIDENT \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s38IdcKKbLBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 20, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtRC7hZfZ9l-",
        "colab_type": "code",
        "outputId": "b65cb913-6d2f-4441-dbc1-65913a319dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "source": [
        "for x, y in train_loader:\n",
        "    \n",
        "    print(y.shape)\n",
        "    print(y[:,:1,:,:,:].shape)\n",
        "    plt.figure()\n",
        "    \n",
        "    plt.imshow(x[:,-1:,:,:,:][0][0][0])\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.imshow(x[:,:1,:,:,:][0][0][0])\n",
        "    plt.figure()\n",
        "    plt.imshow(y[:,:1,:,:,:][0][0][0])\n",
        "    break\n",
        "        \n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 10, 1, 64, 64])\n",
            "torch.Size([20, 1, 1, 64, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCfp/QShahECIAiIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z5oT3A-OeF_B"
      },
      "source": [
        "# TEST OF FULL ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Gey5vMBEm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, valid = initialise_dataset(train_set)\n",
        "# print(train[0][0].dtype)\n",
        "\n",
        "# dat = DataLoader(train, batch_size = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhpOWkfoBEnA",
        "colab_type": "code",
        "outputId": "271fd746-af4a-4686-b6de-25035aa0257f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/data/models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3glozZf5uttV",
        "colab_type": "code",
        "outputId": "e4b8b702-25f2-4a3a-d299-991e731cc61d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "printm()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 11.2 GB  | Proc size: 1.8 GB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mmHSWVWjN-TJ",
        "outputId": "19266fde-c779-43b3-b8b5-cfaa4ae7074c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# changed \n",
        "structure = np.array([[12,24,0,0],[0,24,12,6,1]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 3).to(device)\n",
        "print(\"seq length\", test_model.decoder.seq_length)\n",
        "# optim = torch.optim.Adam(test_model.parameters())\n",
        "\n",
        "\n",
        "\n",
        "# train_enc = train_enc_dec(test_model,)\n",
        "\n",
        "\n",
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\n",
        "\n",
        "model, optimizer = train_main(test_model, 1, train, valid, epochs = 50, batch_size = 50)\n",
        "\n",
        "\n",
        "# torch.save(optimizer.state_dict(), F\"Finished_opt_bce.pth\")\n",
        "# torch.save(model.state_dict(), F\"Finished_mod_bce.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  1]\n",
            "[False, True, False]\n",
            "[True, False, False]\n",
            "seq length 1\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: 1.0132140415669526\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: 1.021546428774799\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: 0.9936984273816996\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: 0.9928469213819332\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: 0.9888795760647284\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: 0.9791515538455057\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: 0.9735721697253545\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: 0.9578680379035567\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: 0.9450680530665309\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: 0.9310318706647551\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: 0.9198249803365591\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: 0.9100374723044341\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: 0.8997392251640093\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: 0.8879850523759042\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: 0.8781022346135745\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: 0.8722205847773609\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: 0.8623386678683091\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: 0.8572487907533582\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: 0.8527396508238149\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: 0.8470545599947401\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: 0.8420215378037438\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: 0.83781602788037\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: 0.8348434562410914\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: 0.8315258483297145\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: 0.8259334259735369\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: 0.8235149156040903\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: 0.8209336243531443\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: 0.819741126196073\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: 0.8161586418743452\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: 0.8140277951360577\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: 0.8120560585184149\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: 0.8104060343872221\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: 0.8088073928116809\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: 0.8073535011152613\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: 0.8054207432480358\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: 0.8033708217737375\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: 0.802605154637499\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: 0.800702662959392\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: 0.7994724557974282\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: 0.798162274662362\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: 0.7961097768235527\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: 0.7949754032738938\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: 0.7928756267359471\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: 0.7917823659480839\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: 0.7904462869155856\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: 0.7898360911714495\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: 0.788792682445922\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: 0.78783226889013\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: 0.7866629713426809\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: 0.7849852755657811\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: 0.7837698683015063\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: 0.7826969986714604\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: 0.7828065392396079\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: 0.7819641375998775\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: 0.7814338691063458\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: 0.7807270751917567\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: 0.7796213340805911\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: 0.7788882090451931\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: 0.7782132505881781\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: 0.777054475625618\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: 0.7765060119646178\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: 0.7755926595732027\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: 0.7747507430001511\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: 0.7735344765353401\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: 0.7727715172789548\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: 0.7716925151650196\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: 0.7710543235277062\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: 0.7701302562113193\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: 0.7694634631967138\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: 0.7685424646670691\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: 0.76757477510889\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: 0.7668776155529573\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: 0.7672625644653948\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: 0.7668780421130694\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: 0.7663400761986221\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: 0.7654031338723258\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: 0.7650826544180236\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: 0.7644987969644647\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: 0.76356945748556\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: 0.7632323830590585\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: 0.7630706544217513\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: 0.7628979910907012\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: 0.762420648026999\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: 0.7618491401861543\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: 0.7609679279481767\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: 0.7603698528509135\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: 0.7601965285292316\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: 0.7599939826847752\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: 0.7596062463004981\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: 0.7591301815102108\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: 0.7583605782539279\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: 0.7583928808723716\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: 0.7576101671255211\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: 0.7568815066385615\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: 0.7564660433336762\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: 0.7558473983575164\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: 0.7556816176166826\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: 0.7549580069079651\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: 0.7548831470764015\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: 0.754379523437019\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: 0.7539294380244121\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: 0.7535903533374694\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: 0.7532092078394875\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: 0.7525023794478163\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: 0.7517037179080509\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: 0.7516112390111197\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: 0.7515340177565962\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: 0.7509931018241324\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: 0.7507390019553071\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: 0.750876028662646\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: 0.7506900663029564\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: 0.750261222259412\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: 0.749804374394061\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: 0.7495190040682913\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: 0.7489993362932897\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: 0.7487002039415083\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: 0.7484629115748882\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: 0.7482586299994298\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: 0.7478952123213428\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: 0.7478199759334759\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: 0.7472172530640421\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: 0.7468992075906974\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: 0.7467733273263804\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: 0.7465508959095187\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: 0.7463943031488749\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: 0.7461585345029462\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: 0.7462259360564116\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: 0.7458725969087777\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: 0.7458962390045338\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: 0.7454978900257627\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: 0.7452707895699202\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: 0.7453237281063857\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: 0.7447094629114817\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: 0.7443337878175563\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: 0.7442216551642366\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: 0.7442146013866306\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: 0.7440593403781206\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: 0.7435036903239619\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: 0.7431789746074416\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: 0.7431515693249457\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: 0.7430787685614573\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: 0.7427242338495519\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: 0.7426729566532558\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: 0.7425443240830253\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: 0.7422286152492263\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: 0.7420167966955741\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: 0.7419114798839685\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: 0.7415611676775326\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: 0.7415516033399965\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: 0.7411621595664062\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: 0.7409367790395502\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: 0.7406654574386541\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: 0.740118709522913\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: 0.7399687337034234\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: 0.7399105165829937\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: 0.7395456892968314\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: 0.7394153505448198\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: 0.7392325372573668\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: 0.7389013503486642\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: 0.7386751392874699\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: 0.7384402970888756\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: 0.7381127043661034\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: 0.7081172227139299\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: 0.7029769877095713\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: 0.6998514620754605\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: 0.7007893075083381\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: 0.7043101323032858\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: 0.7039431141958117\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: 0.7054529202396626\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: 0.7076496401771797\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: 0.7133261332834999\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: 0.7118670007227456\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: 0.7134408638231705\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: 0.7119282995397795\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: 0.7123452051799095\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: 0.710586417115014\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: 0.7111590221528288\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: 0.7077892379316063\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: 0.7078811610165205\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: 0.7096235625085872\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: 0.709486160338614\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: 0.7083338752534246\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: 0.7081981892283536\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: 0.7076137852584933\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: 0.707722215977595\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: 0.7072696365459231\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: 0.7060609146151036\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: 0.7054988513578214\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: 0.7049790232601822\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: 0.705467058703216\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: 0.705626520372354\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: 0.7071184051502022\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: 0.7060387722630348\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: 0.7053534847740509\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: 0.7055764040779317\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: 0.7067440664930935\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: 0.7065258955648265\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: 0.706972722294188\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: 0.7064139798753509\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: 0.706189084917119\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: 0.7054680317978697\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: 0.7054005762009518\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: 0.7059663967930251\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: 0.7067240102363322\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: 0.7078197752290435\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: 0.7090820055508064\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: 0.7089738597908141\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: 0.7090983710352841\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: 0.7093949834898129\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: 0.7091685400089406\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: 0.7086884977807038\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: 0.708774507274305\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: 0.7095900082990126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: 0.7101104326036588\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: 0.7104742397237397\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: 0.7115779877609953\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: 0.7109631507599762\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: 0.7113489182337978\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: 0.7112440274727306\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: 0.7112850082401428\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: 0.7113232777840744\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: 0.7109680351097982\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: 0.7105821508506242\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: 0.710766833343626\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: 0.7109350330416916\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: 0.7116332845903885\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: 0.7111688488861249\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: 0.7103737228385024\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: 0.7096131410600036\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: 0.7093659803543587\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: 0.709181346511485\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: 0.7094433250356154\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: 0.70946723133419\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: 0.709306195115624\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: 0.7093578260829294\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: 0.7099263085564987\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: 0.7101769424863802\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: 0.7103502628838813\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: 0.7110359988025272\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: 0.7108292457498495\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: 0.7106367703871731\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: 0.7105724600668045\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: 0.7112380728740549\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: 0.7113501757319939\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: 0.7115284644293531\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: 0.7111582843348722\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: 0.7108346356292736\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: 0.7105658574141556\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: 0.710414549319536\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: 0.71081157009597\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: 0.7099699831385251\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: 0.7098634965465632\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: 0.709831384019757\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: 0.7104898798560967\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: 0.7102243963096713\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: 0.710045252330229\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: 0.710279858356107\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: 0.7107085628070716\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: 0.7108912821221879\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: 0.711079006464781\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: 0.7107558014119114\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: 0.7105220729067918\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: 0.7102011208696475\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: 0.7099132678424394\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: 0.7099034102462154\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: 0.7104066924801073\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: 0.7104198997776022\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: 0.7099636215681047\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: 0.7102739329181014\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: 0.7100029994383213\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: 0.7098761210705654\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: 0.7096097191990283\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: 0.70939313831645\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: 0.7091791847144494\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: 0.7089020772027139\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: 0.7091251055062537\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: 0.709245373743611\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: 0.7087546130126838\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: 0.708715123299809\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: 0.7084321621869726\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: 0.708568756813005\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: 0.7084147124888845\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: 0.708255188338257\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: 0.7082987456628739\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: 0.7082393504132425\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: 0.7085650613738743\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: 0.7084667466089801\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: 0.7082207831350997\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: 0.7079964042300876\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: 0.708146363859861\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: 0.7080834178069502\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: 0.7079738706667628\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: 0.7083960427880418\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: 0.7084599905989962\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: 0.7086758487602728\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: 0.7087586770254602\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: 0.7087352078118164\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: 0.7086240944875917\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: 0.708785107823105\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: 0.7088094043470656\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: 0.7089443216156361\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: 0.7087972210835692\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: 0.7088930431000142\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: 0.7089078913785446\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: 0.708929811944544\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: 0.708868874507332\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: 0.7089333772429759\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: 0.709197212352776\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: 0.7092686337819792\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: 0.7093797014931462\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: 0.7092259714818555\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: 0.7093859548285276\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: 0.7094986554686407\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: 0.7093300240826876\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: 0.7092097102445379\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: 0.709223465951788\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: 0.7091217084159142\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: 0.7089758856453736\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: 0.7089810949072387\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: 0.7089264770657987\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: 0.7085623174055353\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: 0.708247583875478\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: 0.7082720438053548\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: 0.7083489829105217\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: 0.6658194933664126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: 0.6775154700754046\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: 0.6825448220038804\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: 0.6846095030766723\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: 0.6913654725757871\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: 0.6970056097452835\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: 0.701388928024221\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: 0.6994790069994526\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: 0.7041500030575377\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: 0.7015256583197876\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: 0.7055211469490051\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: 0.7056284330093026\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: 0.7065054629861126\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: 0.7070064371513913\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: 0.7077730048392332\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "15\n",
            "MSE_LOSS: 0.7089177784506728\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "16\n",
            "MSE_LOSS: 0.7094437437338087\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "17\n",
            "MSE_LOSS: 0.710270058697927\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "18\n",
            "MSE_LOSS: 0.7090503337390638\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "19\n",
            "MSE_LOSS: 0.7070033996525251\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "20\n",
            "MSE_LOSS: 0.7070143127671334\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "21\n",
            "MSE_LOSS: 0.7090163355436299\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "22\n",
            "MSE_LOSS: 0.7092086930933539\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "23\n",
            "MSE_LOSS: 0.7078619054611418\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "24\n",
            "MSE_LOSS: 0.7069288703617452\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "25\n",
            "MSE_LOSS: 0.7067664724367967\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "26\n",
            "MSE_LOSS: 0.7064310370671811\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "27\n",
            "MSE_LOSS: 0.7061691600083303\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "28\n",
            "MSE_LOSS: 0.7049824245316129\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "29\n",
            "MSE_LOSS: 0.705339335057491\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "30\n",
            "MSE_LOSS: 0.7050858657824831\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "31\n",
            "MSE_LOSS: 0.7044846810603224\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "32\n",
            "MSE_LOSS: 0.7040004283631581\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "33\n",
            "MSE_LOSS: 0.7048415925612422\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "34\n",
            "MSE_LOSS: 0.7037108417111351\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "35\n",
            "MSE_LOSS: 0.7046417143059303\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "36\n",
            "MSE_LOSS: 0.7028719804295392\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "37\n",
            "MSE_LOSS: 0.7022605425423607\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "38\n",
            "MSE_LOSS: 0.7021242462272591\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "39\n",
            "MSE_LOSS: 0.7023543828939942\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "40\n",
            "MSE_LOSS: 0.7023434388416694\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "41\n",
            "MSE_LOSS: 0.7015791581124733\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "42\n",
            "MSE_LOSS: 0.7017860471511809\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "43\n",
            "MSE_LOSS: 0.7015683387357792\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "44\n",
            "MSE_LOSS: 0.7015328102018686\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "45\n",
            "MSE_LOSS: 0.7016206804709718\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "46\n",
            "MSE_LOSS: 0.7015695706306956\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "47\n",
            "MSE_LOSS: 0.7024861200130362\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "48\n",
            "MSE_LOSS: 0.7027558408551152\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "49\n",
            "MSE_LOSS: 0.7014578785629196\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "50\n",
            "MSE_LOSS: 0.7013873801114957\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "51\n",
            "MSE_LOSS: 0.702105129791044\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "52\n",
            "MSE_LOSS: 0.7016841487208977\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "53\n",
            "MSE_LOSS: 0.7022328002009175\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "54\n",
            "MSE_LOSS: 0.7028517203590132\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "55\n",
            "MSE_LOSS: 0.7020142588479048\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "56\n",
            "MSE_LOSS: 0.7027749800142818\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "57\n",
            "MSE_LOSS: 0.7027608557084596\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "58\n",
            "MSE_LOSS: 0.7026185577231245\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "59\n",
            "MSE_LOSS: 0.702311241962803\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "60\n",
            "MSE_LOSS: 0.7020990860278499\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "61\n",
            "MSE_LOSS: 0.7022452701053139\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "62\n",
            "MSE_LOSS: 0.7020694050474771\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "63\n",
            "MSE_LOSS: 0.7026505960308849\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "64\n",
            "MSE_LOSS: 0.702617140464065\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "65\n",
            "MSE_LOSS: 0.7030783198555451\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "66\n",
            "MSE_LOSS: 0.7032516367982821\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "67\n",
            "MSE_LOSS: 0.7028204325637769\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "68\n",
            "MSE_LOSS: 0.703226269052054\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "69\n",
            "MSE_LOSS: 0.7028388310328486\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "70\n",
            "MSE_LOSS: 0.7031922196585733\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "71\n",
            "MSE_LOSS: 0.7036177563172565\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "72\n",
            "MSE_LOSS: 0.703836382915569\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "73\n",
            "MSE_LOSS: 0.7035481904631115\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "74\n",
            "MSE_LOSS: 0.7035480354228091\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "75\n",
            "MSE_LOSS: 0.7032693654055354\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "76\n",
            "MSE_LOSS: 0.7033149139867482\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "77\n",
            "MSE_LOSS: 0.702695734765882\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "78\n",
            "MSE_LOSS: 0.7028623072223461\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "79\n",
            "MSE_LOSS: 0.7024431909812979\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "80\n",
            "MSE_LOSS: 0.7021754415831701\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "81\n",
            "MSE_LOSS: 0.7015496451232067\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "82\n",
            "MSE_LOSS: 0.7012738942612282\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "83\n",
            "MSE_LOSS: 0.7020262459638035\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "84\n",
            "MSE_LOSS: 0.7023143698679706\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "85\n",
            "MSE_LOSS: 0.7019151097226421\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "86\n",
            "MSE_LOSS: 0.7017112100003237\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "87\n",
            "MSE_LOSS: 0.70176452602926\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "88\n",
            "MSE_LOSS: 0.7018010372012492\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "89\n",
            "MSE_LOSS: 0.7017098097786248\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "90\n",
            "MSE_LOSS: 0.70172724533129\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "91\n",
            "MSE_LOSS: 0.701729853324027\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "92\n",
            "MSE_LOSS: 0.7019514569162384\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "93\n",
            "MSE_LOSS: 0.7018491869898702\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "94\n",
            "MSE_LOSS: 0.702048303395118\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "95\n",
            "MSE_LOSS: 0.7025607730395814\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "96\n",
            "MSE_LOSS: 0.7028455159559368\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "97\n",
            "MSE_LOSS: 0.702459549000333\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "98\n",
            "MSE_LOSS: 0.7027389396703005\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "99\n",
            "MSE_LOSS: 0.7027236296531619\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "100\n",
            "MSE_LOSS: 0.702925693031887\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "101\n",
            "MSE_LOSS: 0.7031691066406046\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "102\n",
            "MSE_LOSS: 0.7035894310005136\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "103\n",
            "MSE_LOSS: 0.7037099631359178\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "104\n",
            "MSE_LOSS: 0.7039329265348465\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "105\n",
            "MSE_LOSS: 0.7039409815268968\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "106\n",
            "MSE_LOSS: 0.7039057493534455\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "107\n",
            "MSE_LOSS: 0.7040034167676583\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "108\n",
            "MSE_LOSS: 0.7035061353865859\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "109\n",
            "MSE_LOSS: 0.7037282051808841\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "110\n",
            "MSE_LOSS: 0.7035709729462366\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "111\n",
            "MSE_LOSS: 0.7038733840834225\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "112\n",
            "MSE_LOSS: 0.7037963326030375\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "113\n",
            "MSE_LOSS: 0.7035515808732222\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "114\n",
            "MSE_LOSS: 0.7030994184219285\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "115\n",
            "MSE_LOSS: 0.7031992874256926\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "116\n",
            "MSE_LOSS: 0.7034743449303658\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "117\n",
            "MSE_LOSS: 0.703346211505041\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "118\n",
            "MSE_LOSS: 0.7034320769461125\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "119\n",
            "MSE_LOSS: 0.7034055754602951\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "120\n",
            "MSE_LOSS: 0.7034952346039612\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "121\n",
            "MSE_LOSS: 0.7034383402797586\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "122\n",
            "MSE_LOSS: 0.7035052374353497\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "123\n",
            "MSE_LOSS: 0.7037058003739054\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "124\n",
            "MSE_LOSS: 0.7040155432500452\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "125\n",
            "MSE_LOSS: 0.703999142660358\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "126\n",
            "MSE_LOSS: 0.7039387173723121\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "127\n",
            "MSE_LOSS: 0.7040826192561356\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "128\n",
            "MSE_LOSS: 0.7040133053572315\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "129\n",
            "MSE_LOSS: 0.7040741909149005\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "130\n",
            "MSE_LOSS: 0.7039440047574476\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "131\n",
            "MSE_LOSS: 0.7038817095641439\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "132\n",
            "MSE_LOSS: 0.7038766813919138\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "133\n",
            "MSE_LOSS: 0.7040232451012142\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "134\n",
            "MSE_LOSS: 0.7037523282673257\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "135\n",
            "MSE_LOSS: 0.7034138774103696\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "136\n",
            "MSE_LOSS: 0.7031998447791937\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "137\n",
            "MSE_LOSS: 0.7031986269958462\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "138\n",
            "MSE_LOSS: 0.7031857741885178\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "139\n",
            "MSE_LOSS: 0.7031780700821043\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "140\n",
            "MSE_LOSS: 0.7033415029381985\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "141\n",
            "MSE_LOSS: 0.7032161682281667\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "142\n",
            "MSE_LOSS: 0.7029120141034302\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "143\n",
            "MSE_LOSS: 0.7030503741388173\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "144\n",
            "MSE_LOSS: 0.7031173008492989\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "145\n",
            "MSE_LOSS: 0.7031867416408508\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "146\n",
            "MSE_LOSS: 0.702946998556996\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "147\n",
            "MSE_LOSS: 0.7031145724514621\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "148\n",
            "MSE_LOSS: 0.7031524911591301\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "149\n",
            "MSE_LOSS: 0.703236157179034\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "150\n",
            "MSE_LOSS: 0.7033089382385961\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "151\n",
            "MSE_LOSS: 0.7030949574503629\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "152\n",
            "MSE_LOSS: 0.7028357744026164\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "153\n",
            "MSE_LOSS: 0.7029230909621632\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "154\n",
            "MSE_LOSS: 0.7030216927452881\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "155\n",
            "MSE_LOSS: 0.7034188228913241\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "156\n",
            "MSE_LOSS: 0.7034984558168065\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "157\n",
            "MSE_LOSS: 0.7035336733831447\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "158\n",
            "MSE_LOSS: 0.7034702437056648\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "159\n",
            "MSE_LOSS: 0.7037110588767648\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "160\n",
            "MSE_LOSS: 0.7037610014508577\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "161\n",
            "MSE_LOSS: 0.703493389916123\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "0\n",
            "MSE_LOSS: 0.7217049159542585\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "1\n",
            "MSE_LOSS: 0.7025402454061838\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "2\n",
            "MSE_LOSS: 0.6873201113098831\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "3\n",
            "MSE_LOSS: 0.6925198214351893\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "4\n",
            "MSE_LOSS: 0.6993695851306624\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "5\n",
            "MSE_LOSS: 0.6927066206660987\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "6\n",
            "MSE_LOSS: 0.6972188168684561\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "7\n",
            "MSE_LOSS: 0.6973927117579884\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "8\n",
            "MSE_LOSS: 0.6981790780976296\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "9\n",
            "MSE_LOSS: 0.6987944486841011\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "10\n",
            "MSE_LOSS: 0.7000563467880588\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "11\n",
            "MSE_LOSS: 0.6995454554570314\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "12\n",
            "MSE_LOSS: 0.6985723222523423\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "13\n",
            "MSE_LOSS: 0.7023433440032066\n",
            "FINISHING ONE PASS\n",
            "BATCH:\n",
            "14\n",
            "MSE_LOSS: 0.7001917251505003\n",
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-dce3cd6d59b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-9be5daccdcf9>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(model, params, train, valid, epochs, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtrain_enc_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-9be5daccdcf9>\u001b[0m in \u001b[0;36mtrain_enc_dec\u001b[0;34m(model, optimizer, dataloader, loss_func)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# steps forward the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# uses loss.backward() to give gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# loss is negative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-qxydjDYmZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_model.decoder.layers\n",
        "len([True, False, False])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLKD_Cy6qqD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84d42399-5ccd-4d18-d64d-f7de4182a486"
      },
      "source": [
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 3).to(device)\n",
        "test_model.load_state_dict(torch.load(F\"Test_big_kern3_2.pth\"))\n",
        "test_model.eval()\n",
        "\n",
        "\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 1\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  1]\n",
            "[False, True, False]\n",
            "[True, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMencdec_onestep(\n",
              "  (encoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxf): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxc): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxo): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): LSTMmain(\n",
              "    (unit_list): ModuleList(\n",
              "      (0): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (1): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (2): LSTMunit(\n",
              "        (conv_dict): ModuleDict(\n",
              "          (Wxi): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxf): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxc): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Wxo): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (Whi): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whf): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Whc): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (Who): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elrw2cLWztFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "o = 0\n",
        "for a, b in train_loader:\n",
        "#     o += 1\n",
        "# #     print(y.shape)\n",
        "# #     print(y[:,:1,:,:,:].shape)\n",
        "# #     plt.figure()\n",
        "# #     plt.imshow(x[:,-1:,:,:,:][0][0][0])\n",
        "# #     plt.figure()\n",
        "# #     plt.imshow(y[:,:1,:,:,:][0][0][0])\n",
        "#     if o == 3:\n",
        "#         break\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UbcsLGPefcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21XyFadhziml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30252999-1c74-4037-9862-f8f15e2582fe"
      },
      "source": [
        "with torch.no_grad():\n",
        "#     x = test_model(torch.unsqueeze(train[0][0], 0).cuda())\n",
        "    x = test_model(a.cuda())"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8jPr5uKUpO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape\n",
        "x = x.cpu()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_1o8PO1SAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "outputId": "c0b83d33-46ba-46b7-e608-a4c01777541d"
      },
      "source": [
        "fig, axes = plt.subplots(10, 2, figsize = (2,10))\n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(b[0][i][0], aspect = \"auto\")\n",
        "    axes[i,1].imshow(x[0][i][0], aspect = \"auto\")\n",
        "  "
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-a3fdba69ff11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJgAAAJCCAYAAAA8+A5BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmMUo1mAWo1iDWYxiDWYxijWYxSj/P2LYxhI6UObYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 144x720 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OqeeAjeRTPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "684d054c-b258-406c-f865-ea42ab261c2f"
      },
      "source": [
        "train[0][0].shape"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV5QpmlwFmGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "760ec686-9502-4352-8c86-c4ef5ca6f7ff"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 1, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoLHga3C0TVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "76d49bf4-2d65-49d1-c0f4-9eba5a33d0df"
      },
      "source": [
        "x = x.cpu()\n",
        "\n",
        "plt.imshow(train[0][1][0][0])\n",
        "plt.figure()\n",
        "pic = plt.imshow(x[0][0][0])\n",
        "\n",
        "k = nn.MSELoss()(train[0][0][-1][0], x[0][0][0])\n",
        "ssim_loss = pytorch_ssim.SSIM()\n",
        "\n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "\n",
        "ssim_value = - ssim_out.data.item()\n",
        "print(ssim_value)\n",
        "print(k)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.664939014303155\n",
            "tensor(0.8742, dtype=torch.float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNSwfoU_Ce5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "9ba00a2b-6461-4915-85b4-b6eb2ce33473"
      },
      "source": [
        "ax = plt.axes()\n",
        "ax.imshow(x[0][0][0])\n",
        "plt.savefig(\"37_612121_6_epochs.pdf\")"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu3NydKqB-cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62N-g91UfiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = train[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBgcIbETUhw-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f118b8dd-2f20-49c2-a888-9ea29fe5d9f0"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1, 64, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8vrB_e20nO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3XwzBgcC_3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15d872fa-65fd-45f9-d597-e126d170b953"
      },
      "source": [
        "test_model.to(device)\n",
        "for params in test_model.parameters():\n",
        "    print(params.is_cuda)\n",
        "    print(params.data)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor([[[[ 5.9365e-03,  1.2856e-03,  4.2731e-03,  ..., -7.3714e-03,\n",
            "            3.4127e-04,  1.7845e-02],\n",
            "          [ 2.5782e-02,  1.3066e-02,  1.5287e-02,  ...,  1.2863e-02,\n",
            "            1.5761e-02,  2.8109e-03],\n",
            "          [ 3.4904e-02,  1.5992e-02,  1.9430e-02,  ...,  1.7807e-02,\n",
            "            2.1793e-02,  3.7980e-03],\n",
            "          ...,\n",
            "          [ 3.2841e-02,  1.4269e-02,  1.8686e-02,  ...,  1.5319e-02,\n",
            "            1.7830e-02,  3.8705e-03],\n",
            "          [ 2.6861e-02,  1.7868e-02,  2.5976e-02,  ...,  1.5018e-02,\n",
            "            1.6740e-02,  8.1114e-03],\n",
            "          [ 2.2089e-02,  1.3076e-02,  2.2031e-02,  ...,  1.0724e-02,\n",
            "            1.9011e-02,  2.0336e-03]],\n",
            "\n",
            "         [[-9.6345e-03, -1.8232e-02, -1.4240e-02,  ..., -1.1891e-02,\n",
            "           -1.5410e-02, -2.5687e-02],\n",
            "          [-8.3373e-02, -6.4466e-02, -5.0526e-02,  ..., -3.5665e-02,\n",
            "           -4.0650e-02, -3.8417e-02],\n",
            "          [-5.3419e-02, -4.0663e-02, -3.4600e-02,  ..., -2.8812e-02,\n",
            "           -2.8234e-02, -3.0679e-02],\n",
            "          ...,\n",
            "          [-4.9765e-02, -3.3095e-02, -3.1578e-02,  ..., -2.2126e-02,\n",
            "           -2.2834e-02, -2.7332e-02],\n",
            "          [-6.4858e-02, -4.9549e-02, -4.7260e-02,  ..., -3.5087e-02,\n",
            "           -3.8071e-02, -5.1144e-02],\n",
            "          [-7.1530e-02, -4.9230e-02, -4.5376e-02,  ..., -3.9320e-02,\n",
            "           -4.6921e-02, -3.0649e-02]],\n",
            "\n",
            "         [[-1.6955e-02, -5.6335e-03, -6.7513e-03,  ..., -3.6372e-03,\n",
            "           -6.5894e-03, -1.0074e-02],\n",
            "          [-7.8458e-02, -1.1655e-01, -9.7675e-02,  ..., -6.3955e-02,\n",
            "           -9.7894e-02, -8.4919e-02],\n",
            "          [-5.8182e-02, -9.3386e-02, -6.0578e-02,  ..., -4.9707e-02,\n",
            "           -6.9609e-02, -7.6289e-02],\n",
            "          ...,\n",
            "          [-4.4635e-02, -6.2866e-02, -5.0816e-02,  ..., -3.5972e-02,\n",
            "           -5.2294e-02, -6.5461e-02],\n",
            "          [-8.1836e-02, -1.2112e-01, -9.5689e-02,  ..., -8.4412e-02,\n",
            "           -9.6437e-02, -9.8355e-02],\n",
            "          [-9.2884e-02, -1.9493e-01, -1.8158e-01,  ..., -1.6286e-01,\n",
            "           -1.7224e-01, -1.3034e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1365e-02, -1.0074e-01, -7.5425e-02,  ..., -8.4243e-02,\n",
            "           -1.0400e-01, -1.3458e-01],\n",
            "          [-9.6870e-02, -1.0972e-01, -6.8745e-02,  ..., -1.0055e-01,\n",
            "           -1.2370e-01, -1.2245e-01],\n",
            "          [-6.5256e-02, -6.6678e-02, -4.3863e-02,  ..., -7.2703e-02,\n",
            "           -8.3432e-02, -8.8267e-02],\n",
            "          ...,\n",
            "          [-8.0244e-02, -1.0476e-01, -7.9757e-02,  ..., -1.0364e-01,\n",
            "           -1.3095e-01, -1.6367e-01],\n",
            "          [-1.2158e-01, -1.4796e-01, -1.0649e-01,  ..., -1.4926e-01,\n",
            "           -1.9097e-01, -2.0958e-01],\n",
            "          [-6.5187e-02, -7.0096e-02, -4.6059e-02,  ..., -8.2923e-02,\n",
            "           -1.0801e-01, -7.8042e-02]],\n",
            "\n",
            "         [[-1.0315e-01, -2.9152e-01, -2.1962e-01,  ..., -2.2264e-01,\n",
            "           -2.5086e-01, -5.8764e-02],\n",
            "          [-1.4475e-01, -3.8504e-01, -2.0979e-01,  ..., -2.0871e-01,\n",
            "           -2.6413e-01, -1.3456e-01],\n",
            "          [-8.1300e-02, -2.3305e-01, -1.2993e-01,  ..., -1.5869e-01,\n",
            "           -1.9721e-01, -1.1696e-01],\n",
            "          ...,\n",
            "          [-5.8162e-02, -1.4792e-01, -1.0987e-01,  ..., -1.3720e-01,\n",
            "           -1.6999e-01, -1.0166e-01],\n",
            "          [-1.2296e-01, -2.6341e-01, -1.4619e-01,  ..., -2.3877e-01,\n",
            "           -2.9038e-01, -1.2982e-01],\n",
            "          [-5.9626e-03, -6.2193e-02, -2.4532e-02,  ..., -3.6717e-02,\n",
            "           -5.4810e-02, -2.1170e-02]],\n",
            "\n",
            "         [[-7.4894e-02, -1.6440e-01, -1.2401e-01,  ..., -1.4111e-01,\n",
            "           -1.5695e-01, -4.0603e-02],\n",
            "          [-1.1644e-01, -2.5108e-01, -1.9469e-01,  ..., -9.5516e-02,\n",
            "           -5.0529e-02,  7.8381e-03],\n",
            "          [-6.2548e-02, -1.1883e-01, -7.3034e-02,  ..., -1.0672e-01,\n",
            "           -8.1345e-02, -1.6844e-02],\n",
            "          ...,\n",
            "          [-3.7657e-02, -4.0055e-02, -2.7186e-03,  ..., -4.6984e-02,\n",
            "           -1.0398e-01, -2.1540e-02],\n",
            "          [-7.1177e-02, -1.0313e-01, -1.2128e-01,  ..., -1.2151e-01,\n",
            "           -1.0852e-01, -1.7824e-02],\n",
            "          [-2.5325e-02, -4.5725e-02, -2.2730e-02,  ..., -4.1990e-02,\n",
            "           -2.1563e-02,  9.1726e-04]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0057,  0.0021,  0.0050,  ..., -0.0046,  0.0011,  0.0153],\n",
            "          [ 0.0232,  0.0117,  0.0139,  ...,  0.0118,  0.0136,  0.0024],\n",
            "          [ 0.0316,  0.0150,  0.0183,  ...,  0.0174,  0.0200,  0.0035],\n",
            "          ...,\n",
            "          [ 0.0295,  0.0124,  0.0170,  ...,  0.0142,  0.0152,  0.0035],\n",
            "          [ 0.0244,  0.0156,  0.0229,  ...,  0.0130,  0.0148,  0.0075],\n",
            "          [ 0.0206,  0.0118,  0.0194,  ...,  0.0101,  0.0170,  0.0018]],\n",
            "\n",
            "         [[-0.0085, -0.0141, -0.0108,  ..., -0.0090, -0.0120, -0.0206],\n",
            "          [-0.0711, -0.0507, -0.0400,  ..., -0.0285, -0.0334, -0.0317],\n",
            "          [-0.0463, -0.0330, -0.0284,  ..., -0.0230, -0.0235, -0.0258],\n",
            "          ...,\n",
            "          [-0.0432, -0.0264, -0.0249,  ..., -0.0175, -0.0187, -0.0227],\n",
            "          [-0.0565, -0.0397, -0.0377,  ..., -0.0280, -0.0310, -0.0426],\n",
            "          [-0.0639, -0.0437, -0.0404,  ..., -0.0352, -0.0424, -0.0269]],\n",
            "\n",
            "         [[-0.0151, -0.0051, -0.0069,  ..., -0.0030, -0.0057, -0.0098],\n",
            "          [-0.0777, -0.1207, -0.1062,  ..., -0.0680, -0.0994, -0.0841],\n",
            "          [-0.0612, -0.1038, -0.0720,  ..., -0.0558, -0.0739, -0.0759],\n",
            "          ...,\n",
            "          [-0.0468, -0.0720, -0.0579,  ..., -0.0394, -0.0550, -0.0653],\n",
            "          [-0.0817, -0.1284, -0.1018,  ..., -0.0858, -0.0981, -0.0988],\n",
            "          [-0.0982, -0.2061, -0.1895,  ..., -0.1665, -0.1788, -0.1282]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0530, -0.0899, -0.0674,  ..., -0.0771, -0.0951, -0.1261],\n",
            "          [-0.0799, -0.0960, -0.0604,  ..., -0.0885, -0.1111, -0.1153],\n",
            "          [-0.0537, -0.0569, -0.0371,  ..., -0.0647, -0.0746, -0.0810],\n",
            "          ...,\n",
            "          [-0.0660, -0.0895, -0.0666,  ..., -0.0898, -0.1155, -0.1517],\n",
            "          [-0.1004, -0.1296, -0.0934,  ..., -0.1327, -0.1743, -0.2004],\n",
            "          [-0.0524, -0.0577, -0.0379,  ..., -0.0697, -0.0926, -0.0696]],\n",
            "\n",
            "         [[-0.0995, -0.2984, -0.2209,  ..., -0.2183, -0.2517, -0.0595],\n",
            "          [-0.1448, -0.3902, -0.2089,  ..., -0.2031, -0.2645, -0.1370],\n",
            "          [-0.0827, -0.2341, -0.1275,  ..., -0.1471, -0.1932, -0.1179],\n",
            "          ...,\n",
            "          [-0.0592, -0.1561, -0.1087,  ..., -0.1307, -0.1673, -0.1010],\n",
            "          [-0.1257, -0.2761, -0.1532,  ..., -0.2312, -0.2890, -0.1319],\n",
            "          [-0.0058, -0.0678, -0.0282,  ..., -0.0402, -0.0592, -0.0220]],\n",
            "\n",
            "         [[-0.0727, -0.1726, -0.1312,  ..., -0.1516, -0.1666, -0.0403],\n",
            "          [-0.1196, -0.2912, -0.2292,  ..., -0.1178, -0.0751,  0.0079],\n",
            "          [-0.0660, -0.1450, -0.0892,  ..., -0.1253, -0.0971, -0.0194],\n",
            "          ...,\n",
            "          [-0.0424, -0.0652, -0.0169,  ..., -0.0633, -0.1260, -0.0228],\n",
            "          [-0.0728, -0.1203, -0.1322,  ..., -0.1415, -0.1220, -0.0183],\n",
            "          [-0.0252, -0.0518, -0.0234,  ..., -0.0463, -0.0256,  0.0011]]]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0059,  0.0017,  0.0047,  ..., -0.0061,  0.0010,  0.0151],\n",
            "          [ 0.0259,  0.0134,  0.0154,  ...,  0.0132,  0.0157,  0.0026],\n",
            "          [ 0.0351,  0.0161,  0.0192,  ...,  0.0181,  0.0217,  0.0035],\n",
            "          ...,\n",
            "          [ 0.0329,  0.0142,  0.0181,  ...,  0.0148,  0.0171,  0.0036],\n",
            "          [ 0.0268,  0.0181,  0.0258,  ...,  0.0146,  0.0163,  0.0077],\n",
            "          [ 0.0215,  0.0126,  0.0207,  ...,  0.0101,  0.0177,  0.0019]],\n",
            "\n",
            "         [[-0.0093, -0.0169, -0.0130,  ..., -0.0107, -0.0145, -0.0245],\n",
            "          [-0.0764, -0.0595, -0.0468,  ..., -0.0333, -0.0394, -0.0379],\n",
            "          [-0.0491, -0.0375, -0.0320,  ..., -0.0259, -0.0271, -0.0301],\n",
            "          ...,\n",
            "          [-0.0461, -0.0308, -0.0283,  ..., -0.0198, -0.0221, -0.0267],\n",
            "          [-0.0598, -0.0457, -0.0430,  ..., -0.0321, -0.0361, -0.0496],\n",
            "          [-0.0672, -0.0466, -0.0432,  ..., -0.0373, -0.0452, -0.0294]],\n",
            "\n",
            "         [[-0.0160, -0.0055, -0.0073,  ..., -0.0032, -0.0062, -0.0100],\n",
            "          [-0.0797, -0.1217, -0.1068,  ..., -0.0670, -0.0995, -0.0817],\n",
            "          [-0.0611, -0.1012, -0.0676,  ..., -0.0522, -0.0721, -0.0735],\n",
            "          ...,\n",
            "          [-0.0464, -0.0691, -0.0553,  ..., -0.0362, -0.0519, -0.0629],\n",
            "          [-0.0808, -0.1232, -0.0988,  ..., -0.0836, -0.0947, -0.0942],\n",
            "          [-0.0961, -0.1994, -0.1849,  ..., -0.1637, -0.1749, -0.1267]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0575, -0.0971, -0.0728,  ..., -0.0828, -0.1025, -0.1258],\n",
            "          [-0.0903, -0.1081, -0.0672,  ..., -0.0996, -0.1251, -0.1174],\n",
            "          [-0.0599, -0.0634, -0.0407,  ..., -0.0713, -0.0830, -0.0828],\n",
            "          ...,\n",
            "          [-0.0743, -0.1015, -0.0747,  ..., -0.1007, -0.1306, -0.1550],\n",
            "          [-0.1124, -0.1453, -0.1045,  ..., -0.1486, -0.1941, -0.2026],\n",
            "          [-0.0584, -0.0657, -0.0425,  ..., -0.0789, -0.1044, -0.0722]],\n",
            "\n",
            "         [[-0.0917, -0.2598, -0.1976,  ..., -0.1973, -0.2233, -0.0509],\n",
            "          [-0.1358, -0.3508, -0.1967,  ..., -0.1947, -0.2477, -0.1300],\n",
            "          [-0.0781, -0.2211, -0.1250,  ..., -0.1457, -0.1834, -0.1106],\n",
            "          ...,\n",
            "          [-0.0575, -0.1533, -0.1117,  ..., -0.1303, -0.1624, -0.0972],\n",
            "          [-0.1157, -0.2565, -0.1501,  ..., -0.2201, -0.2719, -0.1249],\n",
            "          [-0.0057, -0.0607, -0.0252,  ..., -0.0363, -0.0533, -0.0203]],\n",
            "\n",
            "         [[-0.0650, -0.1377, -0.1025,  ..., -0.1240, -0.1359, -0.0369],\n",
            "          [-0.0958, -0.1810, -0.1365,  ..., -0.0861, -0.0629,  0.0040],\n",
            "          [-0.0523, -0.0932, -0.0562,  ..., -0.0828, -0.0711, -0.0142],\n",
            "          ...,\n",
            "          [-0.0361, -0.0512, -0.0233,  ..., -0.0515, -0.0933, -0.0181],\n",
            "          [-0.0595, -0.0820, -0.0822,  ..., -0.0957, -0.0892, -0.0145],\n",
            "          [-0.0222, -0.0394, -0.0191,  ..., -0.0359, -0.0222,  0.0006]]]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0598,  0.4481,  0.1642],\n",
            "          [-0.0104, -0.0584,  0.3337],\n",
            "          [ 0.0007,  0.1873,  0.5105]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2704,  0.0757,  0.0704],\n",
            "          [ 0.0170,  0.1755,  0.3315],\n",
            "          [ 0.2822,  0.1022, -0.1369]]],\n",
            "\n",
            "\n",
            "        [[[-0.0227,  0.3857, -0.1596],\n",
            "          [ 0.3298,  0.2553,  0.2020],\n",
            "          [-0.0125, -0.1153,  0.3114]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1597,  0.3213,  0.4234],\n",
            "          [ 0.3110, -0.2170,  0.3046],\n",
            "          [-0.0549,  0.3092, -0.0294]]],\n",
            "\n",
            "\n",
            "        [[[-0.1085,  0.4137,  0.1828],\n",
            "          [ 0.3036,  0.1372, -0.0796],\n",
            "          [ 0.1505,  0.3827,  0.0575]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4595,  0.2053,  0.3505],\n",
            "          [ 0.2167,  0.0756,  0.2467],\n",
            "          [ 0.3142,  0.0173, -0.0291]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4784,  0.4326,  0.1918],\n",
            "          [-0.1441, -0.1167,  0.2172],\n",
            "          [-0.0446, -0.2048,  0.3460]]],\n",
            "\n",
            "\n",
            "        [[[-0.0654, -0.2212, -0.0932],\n",
            "          [ 0.2131, -0.0079,  0.1832],\n",
            "          [ 0.3367, -0.0828, -0.0029]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4782,  0.2927, -0.0727],\n",
            "          [ 0.1093, -0.1492, -0.0787],\n",
            "          [ 0.3723, -0.1272, -0.1560]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4354,  0.1522,  0.2379],\n",
            "          [ 0.3188, -0.2531,  0.1803],\n",
            "          [ 0.2613, -0.3333, -0.2696]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3747,  0.2947,  0.1369],\n",
            "          [-0.2862, -0.1235,  0.0866],\n",
            "          [-0.1960,  0.2371,  0.2629]]],\n",
            "\n",
            "\n",
            "        [[[-0.2707, -0.0683,  0.2304],\n",
            "          [-0.1453,  0.0015, -0.0850],\n",
            "          [-0.3091,  0.1349, -0.2946]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-1.4330e-01,  2.1504e-01,  4.7075e-01],\n",
            "          [ 9.9051e-02,  3.3060e-01,  1.3331e-01],\n",
            "          [ 9.2999e-02,  2.4027e-01,  3.6454e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.6197e-01, -1.3150e-02, -2.5478e-01],\n",
            "          [ 5.8366e-02,  2.2740e-01, -2.4760e-01],\n",
            "          [ 2.8752e-02, -2.2667e-01,  1.2925e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.4412e-03, -3.0848e-01,  7.4786e-02],\n",
            "          [-2.7852e-01,  1.6425e-01, -3.6812e-02],\n",
            "          [-4.0594e-02, -4.1211e-02,  1.3527e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5873e-01, -2.3202e-01, -2.3465e-01],\n",
            "          [-2.4484e-01,  5.8189e-02, -2.4304e-01],\n",
            "          [-1.4961e-01,  2.1977e-02, -4.2462e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2454e-01,  6.4148e-02,  1.6029e-01],\n",
            "          [-2.4914e-01, -2.1956e-01,  1.3264e-02],\n",
            "          [-2.0973e-01,  7.7604e-02,  2.1524e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.3822e-01,  9.1611e-02, -1.7282e-01],\n",
            "          [ 2.0198e-01,  3.6459e-01,  2.7459e-02],\n",
            "          [-1.1472e-01,  5.6821e-01,  7.1328e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0659e-01, -1.4196e-01, -2.0795e-01],\n",
            "          [ 2.4204e-01, -5.9237e-02,  1.0053e-01],\n",
            "          [-3.3728e-01,  2.0414e-01,  1.7013e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1405e-01,  1.3006e-01, -2.3707e-01],\n",
            "          [ 4.6486e-01, -3.2340e-02, -8.5819e-02],\n",
            "          [ 7.0812e-03, -3.6081e-01, -2.5663e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.2718e-01,  7.1644e-03,  4.9393e-01],\n",
            "          [-3.5247e-01, -1.7133e-01,  2.4081e-01],\n",
            "          [ 1.9106e-01, -1.1322e-01, -1.9362e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 6.3923e-02, -5.5029e-02,  1.5944e-01],\n",
            "          [-3.0595e-01,  8.5403e-02,  2.2476e-01],\n",
            "          [-2.9082e-01, -4.8302e-02,  3.6607e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2066e-01, -2.2635e-02, -2.0897e-01],\n",
            "          [-3.6527e-01, -2.6088e-01, -1.6717e-01],\n",
            "          [-3.5660e-01, -9.2098e-02, -2.7661e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.7440e-01, -3.0971e-01, -3.2548e-01],\n",
            "          [-2.3994e-01,  2.6124e-04, -1.2689e-01],\n",
            "          [ 1.5717e-01, -1.1958e-01, -1.5167e-01]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 9.8853e-02, -2.1947e-01, -3.8684e-01],\n",
            "          [ 2.0993e-01, -2.5813e-01, -3.8221e-01],\n",
            "          [ 3.1926e-02, -2.0584e-01, -4.0901e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2252e-01,  3.7154e-01,  3.5839e-01],\n",
            "          [ 1.7192e-01,  2.7947e-01,  1.4324e-01],\n",
            "          [ 1.0760e-01,  1.2318e-01, -6.1126e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2436e-01,  4.5226e-01,  5.0170e-02],\n",
            "          [ 9.5036e-03,  2.7824e-01,  4.6155e-01],\n",
            "          [ 4.8135e-02,  1.6101e-01,  1.3179e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8954e-01,  3.8899e-01, -2.9505e-01],\n",
            "          [ 4.3636e-02,  2.2660e-01, -1.9662e-02],\n",
            "          [-2.9784e-01, -2.5587e-01, -3.1601e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8262e-01,  2.2279e-01,  3.9052e-01],\n",
            "          [ 2.6989e-01,  1.4453e-01, -1.4865e-01],\n",
            "          [ 3.4335e-01,  3.1920e-01,  3.0576e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 9.0644e-02,  8.3305e-02,  2.1558e-01],\n",
            "          [ 1.4802e-01,  2.2464e-01,  3.2383e-01],\n",
            "          [ 1.4665e-01,  3.4962e-02,  4.9941e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2663e-01,  3.7700e-01,  4.3093e-02],\n",
            "          [ 1.7171e-01,  1.7964e-01,  1.6580e-02],\n",
            "          [-1.1019e-01,  1.5105e-01,  4.3129e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7115e-01,  2.6965e-01,  4.1665e-02],\n",
            "          [ 2.8917e-01,  2.7689e-01,  1.5959e-01],\n",
            "          [ 2.8077e-01,  3.5024e-01,  7.3608e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.5447e-02, -1.0670e-01, -3.9058e-01],\n",
            "          [-2.3015e-01, -3.6041e-01, -4.9203e-04],\n",
            "          [-3.8735e-01, -2.6473e-01,  2.0659e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4825e-01,  2.6293e-01,  1.1214e-01],\n",
            "          [ 2.0736e-01,  1.9340e-01,  8.3312e-02],\n",
            "          [ 7.8343e-02,  1.4516e-01,  3.5420e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1981e-01,  1.9392e-01, -1.3528e-01],\n",
            "          [ 3.0100e-01, -1.9301e-01,  3.1453e-01],\n",
            "          [ 4.9631e-01,  3.0069e-01,  3.9882e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7281e-02, -6.9871e-02,  3.9593e-01],\n",
            "          [ 2.7759e-01,  1.2925e-01,  3.4001e-01],\n",
            "          [ 1.7227e-01,  9.8365e-02,  3.1313e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.1192, -0.0092,  0.1389],\n",
            "          [ 0.1798,  0.0830,  0.4284],\n",
            "          [ 0.1800, -0.1308,  0.2145]]],\n",
            "\n",
            "\n",
            "        [[[-0.0166,  0.2802,  0.0830],\n",
            "          [ 0.3914,  0.0797,  0.1510],\n",
            "          [ 0.4605, -0.1876,  0.2639]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0706,  0.0670, -0.2831],\n",
            "          [-0.0573, -0.2994, -0.1179],\n",
            "          [-0.2604,  0.0251,  0.2084]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1819,  0.3358,  0.1798],\n",
            "          [ 0.3872, -0.0045, -0.1099],\n",
            "          [ 0.5224,  0.3609,  0.3461]]],\n",
            "\n",
            "\n",
            "        [[[-0.1337, -0.0435, -0.2708],\n",
            "          [ 0.2728,  0.2846, -0.1306],\n",
            "          [ 0.1801,  0.2280, -0.0855]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2150, -0.1140,  0.4372],\n",
            "          [-0.3006, -0.3174,  0.2429],\n",
            "          [-0.1738, -0.3345,  0.3104]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3768,  0.3000,  0.4060],\n",
            "          [ 0.3958,  0.2930, -0.1895],\n",
            "          [ 0.4051,  0.0692,  0.4186]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3040,  0.0579,  0.0369],\n",
            "          [-0.1790, -0.4025,  0.1408],\n",
            "          [-0.0848,  0.2556,  0.4396]]],\n",
            "\n",
            "\n",
            "        [[[-0.0767,  0.0745, -0.1064],\n",
            "          [ 0.2182,  0.0840, -0.0998],\n",
            "          [ 0.4015,  0.1297,  0.2292]]],\n",
            "\n",
            "\n",
            "        [[[ 0.4082,  0.3793,  0.0343],\n",
            "          [ 0.3387,  0.1875,  0.1775],\n",
            "          [ 0.2517, -0.1535, -0.4802]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2182, -0.1188, -0.0758],\n",
            "          [-0.1374, -0.1346,  0.0704],\n",
            "          [ 0.1284,  0.1009,  0.2352]]],\n",
            "\n",
            "\n",
            "        [[[-0.2644,  0.0722,  0.3594],\n",
            "          [-0.1279, -0.2665,  0.2627],\n",
            "          [-0.4618,  0.0126, -0.3303]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0025, -0.0980, -0.3396],\n",
            "          [ 0.0181, -0.0907, -0.2216],\n",
            "          [ 0.1147,  0.0957, -0.1907]],\n",
            "\n",
            "         [[-0.0607,  0.0326,  0.0796],\n",
            "          [-0.0485,  0.0622,  0.1735],\n",
            "          [-0.0625, -0.0148,  0.0951]],\n",
            "\n",
            "         [[-0.1518, -0.0430,  0.0574],\n",
            "          [-0.1083, -0.0239, -0.0021],\n",
            "          [-0.1270,  0.0580, -0.0843]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1528, -0.0436, -0.0211],\n",
            "          [-0.0234, -0.0185,  0.0831],\n",
            "          [-0.1066, -0.0818,  0.2237]],\n",
            "\n",
            "         [[-0.1549,  0.0536,  0.0727],\n",
            "          [-0.1076, -0.0237,  0.0666],\n",
            "          [-0.1413,  0.0372, -0.2345]],\n",
            "\n",
            "         [[-0.1963, -0.0023, -0.1020],\n",
            "          [-0.2285, -0.0386, -0.0606],\n",
            "          [-0.2382, -0.1282, -0.0995]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0211, -0.0252, -0.1632],\n",
            "          [ 0.0020, -0.0490, -0.0996],\n",
            "          [-0.0997, -0.1156, -0.0983]],\n",
            "\n",
            "         [[ 0.1025,  0.2178, -0.0131],\n",
            "          [ 0.1457,  0.0910,  0.1198],\n",
            "          [ 0.0213,  0.0028,  0.0102]],\n",
            "\n",
            "         [[-0.1416, -0.1509, -0.1174],\n",
            "          [ 0.0182, -0.0173, -0.0635],\n",
            "          [ 0.0202, -0.0734, -0.0778]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0369,  0.1563,  0.2053],\n",
            "          [ 0.0445,  0.1332,  0.2264],\n",
            "          [-0.0511,  0.0901,  0.0622]],\n",
            "\n",
            "         [[-0.0301, -0.0579, -0.0272],\n",
            "          [-0.0261,  0.1003, -0.0049],\n",
            "          [ 0.0193, -0.1219, -0.1359]],\n",
            "\n",
            "         [[ 0.0895,  0.0115,  0.0606],\n",
            "          [-0.0008,  0.0442,  0.0203],\n",
            "          [-0.1384, -0.0469,  0.0037]]],\n",
            "\n",
            "\n",
            "        [[[-0.0843, -0.0352,  0.0058],\n",
            "          [ 0.0425, -0.1499, -0.0798],\n",
            "          [-0.1730, -0.0890, -0.2168]],\n",
            "\n",
            "         [[ 0.1067,  0.1690, -0.0522],\n",
            "          [-0.0156,  0.0321,  0.0577],\n",
            "          [-0.0324,  0.0305,  0.1809]],\n",
            "\n",
            "         [[ 0.1049,  0.1179, -0.1120],\n",
            "          [ 0.0567,  0.0250, -0.1076],\n",
            "          [ 0.1893,  0.0199, -0.0334]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1421,  0.1874,  0.1969],\n",
            "          [ 0.2401,  0.2706,  0.1105],\n",
            "          [ 0.1081,  0.0154,  0.1199]],\n",
            "\n",
            "         [[ 0.1693,  0.0996,  0.0846],\n",
            "          [ 0.1624, -0.0047,  0.1382],\n",
            "          [ 0.1678,  0.1651,  0.0650]],\n",
            "\n",
            "         [[-0.0454, -0.0310,  0.0207],\n",
            "          [-0.0100,  0.0298, -0.0536],\n",
            "          [-0.0239, -0.0467,  0.0979]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0348,  0.0621, -0.0933],\n",
            "          [-0.0902,  0.0672,  0.0041],\n",
            "          [-0.0314, -0.0526, -0.0451]],\n",
            "\n",
            "         [[ 0.0888,  0.0432,  0.0349],\n",
            "          [-0.0041,  0.0666, -0.0113],\n",
            "          [ 0.0993,  0.0642,  0.1212]],\n",
            "\n",
            "         [[-0.0399, -0.0640, -0.0363],\n",
            "          [ 0.0103, -0.0073, -0.0611],\n",
            "          [ 0.0083, -0.0401,  0.0295]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1191, -0.0962,  0.0360],\n",
            "          [ 0.0928,  0.0866,  0.0799],\n",
            "          [ 0.0242,  0.0997,  0.0728]],\n",
            "\n",
            "         [[ 0.0169,  0.0500, -0.1208],\n",
            "          [ 0.1391,  0.0795,  0.0818],\n",
            "          [ 0.0797,  0.0958,  0.1480]],\n",
            "\n",
            "         [[-0.0073,  0.0624, -0.0113],\n",
            "          [-0.0300, -0.0228,  0.0008],\n",
            "          [ 0.1124,  0.0083, -0.0592]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0141,  0.0113,  0.0171],\n",
            "          [ 0.0307,  0.0458,  0.1032],\n",
            "          [ 0.1059,  0.0729,  0.1182]],\n",
            "\n",
            "         [[ 0.1987,  0.1215, -0.1269],\n",
            "          [ 0.1370,  0.1726, -0.0029],\n",
            "          [-0.0057,  0.0691, -0.1993]],\n",
            "\n",
            "         [[-0.0692, -0.0095, -0.1352],\n",
            "          [-0.0886, -0.0847, -0.0724],\n",
            "          [-0.0240, -0.0657, -0.1603]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1192,  0.0369,  0.0027],\n",
            "          [ 0.0377,  0.2083, -0.0183],\n",
            "          [ 0.0931,  0.1691,  0.0176]],\n",
            "\n",
            "         [[ 0.0260,  0.0489, -0.0453],\n",
            "          [ 0.0426,  0.0388, -0.0999],\n",
            "          [ 0.0247, -0.1551, -0.2094]],\n",
            "\n",
            "         [[-0.1491, -0.1077, -0.1219],\n",
            "          [-0.0175, -0.0912, -0.1413],\n",
            "          [-0.0154, -0.1543, -0.1290]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1236,  0.0019, -0.0568],\n",
            "          [ 0.0954,  0.0710,  0.0267],\n",
            "          [ 0.0535, -0.0051,  0.0784]],\n",
            "\n",
            "         [[-0.1555, -0.1250, -0.0346],\n",
            "          [-0.1281, -0.0769,  0.0469],\n",
            "          [-0.1733, -0.0649, -0.0714]],\n",
            "\n",
            "         [[-0.0869, -0.0218, -0.1139],\n",
            "          [-0.1687,  0.0120, -0.1537],\n",
            "          [-0.0709, -0.0986, -0.1880]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1153, -0.1368, -0.0314],\n",
            "          [ 0.0264, -0.0253, -0.0380],\n",
            "          [ 0.0617,  0.0880,  0.0316]],\n",
            "\n",
            "         [[-0.1580, -0.0310, -0.1213],\n",
            "          [-0.1254, -0.1142, -0.0722],\n",
            "          [-0.0436, -0.1269, -0.2343]],\n",
            "\n",
            "         [[-0.1048, -0.0534, -0.0370],\n",
            "          [-0.1283,  0.0165, -0.1320],\n",
            "          [-0.0081,  0.0110, -0.1441]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([0.1444, 0.1672, 0.2042, 0.1514, 0.0054, 0.2368, 0.0784, 0.2327, 0.2219,\n",
            "        0.1058, 0.2137, 0.0547], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0533,  0.0500,  0.0510],\n",
            "          [ 0.1006, -0.0481,  0.0552],\n",
            "          [ 0.0573,  0.0462,  0.1355]],\n",
            "\n",
            "         [[-0.0774,  0.0501,  0.0451],\n",
            "          [ 0.0979,  0.0426, -0.0014],\n",
            "          [-0.0591,  0.0538, -0.0063]],\n",
            "\n",
            "         [[ 0.0172, -0.0598, -0.0722],\n",
            "          [ 0.1525,  0.0905,  0.0575],\n",
            "          [-0.0008, -0.0828, -0.0087]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1817, -0.0205,  0.0086],\n",
            "          [-0.1000, -0.0023, -0.0689],\n",
            "          [-0.0396,  0.0128, -0.1380]],\n",
            "\n",
            "         [[ 0.1508,  0.0976, -0.0109],\n",
            "          [ 0.0326, -0.0830, -0.1010],\n",
            "          [-0.0559, -0.1208, -0.0822]],\n",
            "\n",
            "         [[ 0.0576,  0.0990,  0.1876],\n",
            "          [ 0.1379,  0.1283,  0.2311],\n",
            "          [ 0.3060,  0.1142,  0.1134]]],\n",
            "\n",
            "\n",
            "        [[[-0.0020,  0.0662,  0.1230],\n",
            "          [ 0.0808,  0.0352,  0.1005],\n",
            "          [-0.0605, -0.0364,  0.1910]],\n",
            "\n",
            "         [[-0.0585, -0.1074, -0.0887],\n",
            "          [-0.1016,  0.0858, -0.1221],\n",
            "          [ 0.0763,  0.0027, -0.0361]],\n",
            "\n",
            "         [[ 0.0484,  0.1137, -0.0045],\n",
            "          [ 0.1289, -0.0296,  0.0220],\n",
            "          [ 0.1431,  0.1847,  0.0849]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2154, -0.1527, -0.1183],\n",
            "          [-0.1257, -0.0061, -0.0602],\n",
            "          [-0.0332,  0.0347, -0.0802]],\n",
            "\n",
            "         [[-0.0062, -0.0922, -0.0308],\n",
            "          [ 0.0889,  0.0574,  0.0709],\n",
            "          [ 0.1782,  0.1605,  0.1532]],\n",
            "\n",
            "         [[-0.0313,  0.0360,  0.0356],\n",
            "          [ 0.2186,  0.1514,  0.1463],\n",
            "          [ 0.2185,  0.1723,  0.1238]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0366,  0.0064,  0.0796],\n",
            "          [ 0.1101, -0.0597,  0.0499],\n",
            "          [-0.0143, -0.0762,  0.1627]],\n",
            "\n",
            "         [[-0.2713, -0.0685, -0.0767],\n",
            "          [-0.1067,  0.0344, -0.1159],\n",
            "          [ 0.0377,  0.0656,  0.0480]],\n",
            "\n",
            "         [[-0.1109, -0.0316, -0.0911],\n",
            "          [ 0.0623, -0.0215, -0.0905],\n",
            "          [-0.0967, -0.0054,  0.0254]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1863, -0.1784, -0.0930],\n",
            "          [-0.1501, -0.0532, -0.0927],\n",
            "          [-0.1495, -0.1000, -0.0428]],\n",
            "\n",
            "         [[-0.0556, -0.0878, -0.0543],\n",
            "          [-0.0680,  0.0042, -0.0715],\n",
            "          [ 0.1231,  0.1687,  0.1940]],\n",
            "\n",
            "         [[ 0.0668,  0.1557,  0.1096],\n",
            "          [ 0.0980,  0.2080,  0.2174],\n",
            "          [ 0.1702,  0.2382,  0.1411]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0925,  0.0176,  0.0513],\n",
            "          [-0.0268, -0.0797,  0.0833],\n",
            "          [-0.0302, -0.0188,  0.0605]],\n",
            "\n",
            "         [[ 0.1075,  0.0770, -0.0600],\n",
            "          [-0.0236,  0.0625,  0.1187],\n",
            "          [ 0.0663,  0.0338,  0.0013]],\n",
            "\n",
            "         [[ 0.1082,  0.0331,  0.0426],\n",
            "          [ 0.0717,  0.0648,  0.0937],\n",
            "          [ 0.2111,  0.1035, -0.0016]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0138, -0.0352, -0.0296],\n",
            "          [-0.0497,  0.0432,  0.0306],\n",
            "          [-0.0636,  0.0914,  0.0851]],\n",
            "\n",
            "         [[ 0.0049, -0.0116,  0.1068],\n",
            "          [ 0.1072,  0.0937,  0.1305],\n",
            "          [ 0.0445,  0.0932, -0.0939]],\n",
            "\n",
            "         [[ 0.0058,  0.0807,  0.0365],\n",
            "          [-0.0506, -0.0464,  0.1030],\n",
            "          [-0.0448, -0.0350, -0.0359]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0436,  0.1154,  0.0268],\n",
            "          [ 0.0978,  0.0346,  0.0186],\n",
            "          [ 0.0564, -0.0402,  0.0585]],\n",
            "\n",
            "         [[-0.0981, -0.0190, -0.1536],\n",
            "          [-0.0280, -0.0501, -0.1219],\n",
            "          [-0.0663,  0.0154, -0.0441]],\n",
            "\n",
            "         [[-0.0329,  0.0481, -0.0180],\n",
            "          [-0.0846,  0.0255, -0.1557],\n",
            "          [-0.0846, -0.0176, -0.1045]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0911, -0.1283, -0.1646],\n",
            "          [-0.0990, -0.0201, -0.0480],\n",
            "          [-0.0954, -0.1221, -0.0845]],\n",
            "\n",
            "         [[ 0.0522,  0.0339,  0.0271],\n",
            "          [-0.0688,  0.0453,  0.0365],\n",
            "          [-0.0326, -0.0721,  0.0978]],\n",
            "\n",
            "         [[ 0.0724, -0.0334, -0.0196],\n",
            "          [ 0.0133, -0.0582,  0.0910],\n",
            "          [-0.0300,  0.1259,  0.0892]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0262,  0.0233,  0.0356],\n",
            "          [-0.0495,  0.0035, -0.0254],\n",
            "          [ 0.0233,  0.1062,  0.2368]],\n",
            "\n",
            "         [[-0.0788,  0.0607,  0.0351],\n",
            "          [-0.0106, -0.0166, -0.0635],\n",
            "          [-0.0414,  0.0378, -0.1000]],\n",
            "\n",
            "         [[ 0.0113,  0.0816, -0.0549],\n",
            "          [ 0.0263, -0.0088,  0.0959],\n",
            "          [-0.0020, -0.0517,  0.0045]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1178, -0.1155, -0.0337],\n",
            "          [-0.1707, -0.0278,  0.0040],\n",
            "          [-0.0894,  0.0081, -0.0584]],\n",
            "\n",
            "         [[-0.0402,  0.1247,  0.0835],\n",
            "          [ 0.0307,  0.0124,  0.1136],\n",
            "          [-0.0427, -0.0546,  0.0746]],\n",
            "\n",
            "         [[-0.0053,  0.0584,  0.1566],\n",
            "          [-0.0427,  0.0220, -0.0126],\n",
            "          [ 0.0436,  0.1178,  0.0245]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.0654, -0.1429,  0.0011, -0.1779, -0.0993, -0.0895, -0.0766, -0.1091,\n",
            "        -0.0045, -0.1255,  0.0536, -0.0201], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0467,  0.0350, -0.1013],\n",
            "          [-0.0557, -0.1541, -0.0214],\n",
            "          [-0.0204,  0.0352, -0.1094]],\n",
            "\n",
            "         [[ 0.0725, -0.0043,  0.0094],\n",
            "          [-0.0043, -0.0378,  0.0246],\n",
            "          [ 0.0736,  0.0300,  0.1122]],\n",
            "\n",
            "         [[-0.1289,  0.0585,  0.0843],\n",
            "          [ 0.0339, -0.0585,  0.0016],\n",
            "          [-0.0459,  0.0006,  0.0790]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0981, -0.0508,  0.0723],\n",
            "          [-0.0350, -0.0762,  0.0279],\n",
            "          [ 0.0101, -0.0199,  0.0403]],\n",
            "\n",
            "         [[-0.0657,  0.0790, -0.0113],\n",
            "          [ 0.0080,  0.0243,  0.1022],\n",
            "          [-0.0274, -0.0210,  0.0886]],\n",
            "\n",
            "         [[-0.1118,  0.0093,  0.0436],\n",
            "          [ 0.0237,  0.0542,  0.0606],\n",
            "          [ 0.0662,  0.0149,  0.0746]]],\n",
            "\n",
            "\n",
            "        [[[-0.0783, -0.1109,  0.0402],\n",
            "          [-0.0919, -0.0299,  0.2063],\n",
            "          [ 0.0306,  0.0777,  0.1022]],\n",
            "\n",
            "         [[-0.1092,  0.0318, -0.0382],\n",
            "          [-0.0642, -0.0330, -0.0082],\n",
            "          [-0.0116, -0.0741, -0.1031]],\n",
            "\n",
            "         [[ 0.0808, -0.0506, -0.0242],\n",
            "          [ 0.0089,  0.0616, -0.0680],\n",
            "          [ 0.0222, -0.0208,  0.0049]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2352, -0.2166, -0.0490],\n",
            "          [ 0.0136,  0.0504, -0.0965],\n",
            "          [ 0.0421, -0.0476,  0.0498]],\n",
            "\n",
            "         [[-0.0636, -0.1249, -0.0370],\n",
            "          [ 0.0527, -0.0026, -0.0508],\n",
            "          [ 0.0228, -0.0114,  0.0169]],\n",
            "\n",
            "         [[-0.0900, -0.0271,  0.0348],\n",
            "          [ 0.0041, -0.0691, -0.1228],\n",
            "          [-0.1200, -0.0186, -0.0871]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0167, -0.0155,  0.1363],\n",
            "          [-0.0397,  0.0878, -0.0120],\n",
            "          [-0.1001, -0.0009,  0.1263]],\n",
            "\n",
            "         [[-0.1198, -0.0095, -0.0881],\n",
            "          [-0.1092,  0.0897,  0.0947],\n",
            "          [-0.0239,  0.0005,  0.0366]],\n",
            "\n",
            "         [[-0.0225,  0.0802, -0.1241],\n",
            "          [ 0.0505,  0.0992,  0.0053],\n",
            "          [ 0.0042, -0.0294,  0.0055]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1117, -0.2388, -0.1178],\n",
            "          [-0.1298, -0.0855,  0.0145],\n",
            "          [-0.1051, -0.0891,  0.0485]],\n",
            "\n",
            "         [[-0.1017,  0.0471, -0.1361],\n",
            "          [-0.0246, -0.1015,  0.0174],\n",
            "          [-0.0667, -0.0200, -0.0962]],\n",
            "\n",
            "         [[-0.0567, -0.0544,  0.0042],\n",
            "          [ 0.0149,  0.0252, -0.0745],\n",
            "          [ 0.0786,  0.0077, -0.0782]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0830, -0.0568,  0.0850],\n",
            "          [-0.0487,  0.0180, -0.0005],\n",
            "          [-0.1156, -0.0765, -0.1041]],\n",
            "\n",
            "         [[-0.1096,  0.0504,  0.0675],\n",
            "          [ 0.0715,  0.1334, -0.0361],\n",
            "          [ 0.1826,  0.0536,  0.0506]],\n",
            "\n",
            "         [[-0.1094,  0.0160, -0.1111],\n",
            "          [-0.0886, -0.0022, -0.0685],\n",
            "          [ 0.0945,  0.0873,  0.0381]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0040, -0.0348, -0.1179],\n",
            "          [ 0.0107,  0.0626,  0.0696],\n",
            "          [ 0.0422,  0.0556, -0.0652]],\n",
            "\n",
            "         [[-0.0558,  0.0261,  0.0131],\n",
            "          [ 0.0925, -0.0676,  0.0511],\n",
            "          [ 0.0593,  0.0325, -0.0763]],\n",
            "\n",
            "         [[-0.0158, -0.1150, -0.0865],\n",
            "          [ 0.0075, -0.0754, -0.0942],\n",
            "          [-0.0277, -0.0926,  0.0026]]],\n",
            "\n",
            "\n",
            "        [[[-0.0774,  0.1098,  0.1950],\n",
            "          [ 0.0773,  0.0143, -0.0188],\n",
            "          [ 0.1275, -0.1037,  0.0034]],\n",
            "\n",
            "         [[-0.0337, -0.0389, -0.0932],\n",
            "          [-0.0874, -0.0803, -0.0052],\n",
            "          [-0.0474,  0.0317, -0.0058]],\n",
            "\n",
            "         [[-0.0254, -0.0359, -0.0604],\n",
            "          [ 0.0654,  0.1090,  0.0671],\n",
            "          [ 0.0460,  0.0773,  0.0601]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0361, -0.0505, -0.1165],\n",
            "          [-0.1220, -0.1108, -0.0583],\n",
            "          [-0.1016, -0.0054, -0.0780]],\n",
            "\n",
            "         [[-0.0329,  0.0775,  0.0843],\n",
            "          [-0.0447,  0.0325,  0.0237],\n",
            "          [-0.0440, -0.0493, -0.0413]],\n",
            "\n",
            "         [[-0.0131, -0.0026, -0.0683],\n",
            "          [ 0.0856,  0.0947,  0.0365],\n",
            "          [-0.0382,  0.0465,  0.0716]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0042, -0.1190,  0.1192],\n",
            "          [-0.1057,  0.0616,  0.0772],\n",
            "          [ 0.0455,  0.0274,  0.0172]],\n",
            "\n",
            "         [[-0.0106,  0.0139, -0.0450],\n",
            "          [ 0.1306,  0.0609, -0.0319],\n",
            "          [ 0.1174, -0.0442,  0.0334]],\n",
            "\n",
            "         [[-0.0339, -0.0376,  0.0486],\n",
            "          [ 0.0252,  0.0118,  0.0262],\n",
            "          [-0.0110,  0.0166, -0.0178]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0444, -0.0964, -0.0634],\n",
            "          [-0.0480,  0.0438,  0.0387],\n",
            "          [ 0.1097, -0.0492,  0.0695]],\n",
            "\n",
            "         [[-0.0005,  0.0557,  0.0065],\n",
            "          [ 0.0684,  0.0532,  0.0724],\n",
            "          [ 0.1268,  0.1035,  0.0509]],\n",
            "\n",
            "         [[-0.0873, -0.0543, -0.0724],\n",
            "          [-0.0132, -0.0062, -0.0191],\n",
            "          [-0.0440,  0.0059,  0.0783]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.0701, -0.0174, -0.0220,  0.0825, -0.0309,  0.0152,  0.0915,  0.1362,\n",
            "        -0.1026, -0.0387, -0.0406, -0.0009], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0202, -0.0183, -0.2152],\n",
            "          [-0.0419,  0.0096, -0.0340],\n",
            "          [ 0.0079,  0.0447,  0.1176]],\n",
            "\n",
            "         [[ 0.1262,  0.1679,  0.1787],\n",
            "          [-0.0558,  0.0643,  0.1596],\n",
            "          [-0.1392,  0.1552,  0.1462]],\n",
            "\n",
            "         [[-0.1079,  0.0680,  0.0397],\n",
            "          [-0.0931, -0.0200,  0.0326],\n",
            "          [-0.1005, -0.0360, -0.1878]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0757,  0.1075,  0.1544],\n",
            "          [ 0.0970,  0.0523,  0.2041],\n",
            "          [ 0.0295,  0.1680,  0.1983]],\n",
            "\n",
            "         [[-0.0313,  0.0246, -0.0057],\n",
            "          [-0.0624,  0.1016, -0.0578],\n",
            "          [ 0.0504,  0.0260, -0.2070]],\n",
            "\n",
            "         [[-0.1989, -0.0230, -0.0938],\n",
            "          [-0.2155, -0.0493, -0.0977],\n",
            "          [-0.1089, -0.2064, -0.2395]]],\n",
            "\n",
            "\n",
            "        [[[-0.0737, -0.1690, -0.1057],\n",
            "          [-0.0424,  0.0171, -0.0218],\n",
            "          [-0.0840,  0.0045,  0.0693]],\n",
            "\n",
            "         [[ 0.1355,  0.1709,  0.1454],\n",
            "          [ 0.0010, -0.0172,  0.0835],\n",
            "          [-0.0068, -0.0761,  0.0313]],\n",
            "\n",
            "         [[-0.0152, -0.0197,  0.0335],\n",
            "          [-0.0783, -0.0645,  0.1113],\n",
            "          [ 0.0163,  0.0689, -0.0412]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0259,  0.0741,  0.1531],\n",
            "          [ 0.1435,  0.2572,  0.1307],\n",
            "          [ 0.0600,  0.0973,  0.1497]],\n",
            "\n",
            "         [[-0.0327,  0.1195,  0.0308],\n",
            "          [-0.0411,  0.0645,  0.0954],\n",
            "          [ 0.0594, -0.0893, -0.0565]],\n",
            "\n",
            "         [[ 0.0207,  0.0774,  0.1103],\n",
            "          [ 0.0876,  0.1222,  0.1452],\n",
            "          [-0.0325, -0.0103,  0.0294]]],\n",
            "\n",
            "\n",
            "        [[[-0.1242, -0.0477, -0.0694],\n",
            "          [ 0.0722, -0.0500, -0.1716],\n",
            "          [-0.0691, -0.2178, -0.1949]],\n",
            "\n",
            "         [[ 0.1916,  0.0379, -0.0645],\n",
            "          [ 0.0049,  0.0300, -0.0180],\n",
            "          [-0.1881, -0.0674,  0.0740]],\n",
            "\n",
            "         [[ 0.2139, -0.0736, -0.1512],\n",
            "          [ 0.1185,  0.0410,  0.0775],\n",
            "          [ 0.0383,  0.0522,  0.1009]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.5034,  0.3724,  0.1310],\n",
            "          [ 0.2589,  0.3100,  0.1043],\n",
            "          [ 0.1018,  0.0330,  0.0170]],\n",
            "\n",
            "         [[ 0.2680,  0.1347, -0.0276],\n",
            "          [ 0.1257,  0.0188,  0.0119],\n",
            "          [-0.0374, -0.0416,  0.1132]],\n",
            "\n",
            "         [[ 0.1282,  0.0161,  0.1360],\n",
            "          [ 0.0043, -0.1376, -0.0096],\n",
            "          [-0.0969, -0.0794,  0.1861]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0507,  0.0363, -0.0074],\n",
            "          [ 0.0897,  0.0284, -0.0686],\n",
            "          [-0.0168,  0.0627,  0.0262]],\n",
            "\n",
            "         [[-0.1485, -0.1108, -0.1367],\n",
            "          [-0.1049,  0.0090,  0.0258],\n",
            "          [-0.0649,  0.0700,  0.0367]],\n",
            "\n",
            "         [[ 0.0161, -0.0300,  0.0515],\n",
            "          [ 0.1354, -0.0409,  0.0628],\n",
            "          [ 0.0902, -0.1084,  0.0430]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2638, -0.1891, -0.1839],\n",
            "          [-0.2026, -0.0105,  0.0404],\n",
            "          [-0.0444,  0.0069, -0.0111]],\n",
            "\n",
            "         [[-0.0862, -0.0181,  0.0485],\n",
            "          [-0.0763,  0.0362, -0.0990],\n",
            "          [-0.0265, -0.0336,  0.0190]],\n",
            "\n",
            "         [[-0.0662, -0.0588,  0.0319],\n",
            "          [ 0.0277, -0.0711, -0.0228],\n",
            "          [-0.0482,  0.0806, -0.0414]]],\n",
            "\n",
            "\n",
            "        [[[-0.0945,  0.1393,  0.1250],\n",
            "          [-0.1293, -0.0605,  0.0289],\n",
            "          [ 0.0277, -0.0350, -0.0670]],\n",
            "\n",
            "         [[ 0.0872, -0.0996, -0.1920],\n",
            "          [ 0.0106, -0.0341, -0.2088],\n",
            "          [-0.0507,  0.0720,  0.0348]],\n",
            "\n",
            "         [[-0.0645, -0.1307, -0.1117],\n",
            "          [ 0.0225, -0.0913, -0.0479],\n",
            "          [-0.0752,  0.0336, -0.0866]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0106,  0.1485, -0.0249],\n",
            "          [ 0.1969,  0.0928,  0.0435],\n",
            "          [ 0.2520,  0.1038,  0.1809]],\n",
            "\n",
            "         [[ 0.0308,  0.0146, -0.1397],\n",
            "          [-0.0185, -0.0582, -0.0152],\n",
            "          [ 0.0504,  0.0255, -0.0007]],\n",
            "\n",
            "         [[-0.1296, -0.1467, -0.1290],\n",
            "          [-0.0399,  0.0096, -0.0545],\n",
            "          [ 0.0202, -0.0201,  0.0666]]],\n",
            "\n",
            "\n",
            "        [[[-0.0241,  0.0581, -0.0144],\n",
            "          [-0.0539,  0.1059, -0.0834],\n",
            "          [ 0.0940,  0.1330,  0.1479]],\n",
            "\n",
            "         [[-0.1363, -0.0138, -0.0374],\n",
            "          [-0.0345, -0.1107, -0.0396],\n",
            "          [-0.0968, -0.0872, -0.1060]],\n",
            "\n",
            "         [[-0.1389, -0.1040, -0.0624],\n",
            "          [ 0.0161, -0.0205, -0.0388],\n",
            "          [-0.0751, -0.0493, -0.0291]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1379,  0.0373, -0.0481],\n",
            "          [-0.1317, -0.0884, -0.0249],\n",
            "          [ 0.0247, -0.0076,  0.0539]],\n",
            "\n",
            "         [[-0.0367,  0.0785, -0.0859],\n",
            "          [-0.0400,  0.0628, -0.0894],\n",
            "          [-0.0359,  0.0388, -0.1488]],\n",
            "\n",
            "         [[ 0.0264,  0.0609,  0.1348],\n",
            "          [ 0.0725, -0.0626, -0.0155],\n",
            "          [ 0.0235, -0.0341,  0.0956]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.1979,  0.1399,  0.1500,  0.1670,  0.1445,  0.1611, -0.0125,  0.0503,\n",
            "         0.0664, -0.0542,  0.0171, -0.0114], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-4.4065e-02, -3.1484e-01, -3.0084e-01,  ..., -1.8927e-01,\n",
            "           -3.6223e-01, -1.9987e-01],\n",
            "          [-3.8237e-01, -6.3819e-01, -6.9147e-01,  ..., -4.9799e-01,\n",
            "           -1.0048e+00, -7.7330e-01],\n",
            "          [-4.5231e-01, -9.2990e-01, -4.7063e-01,  ..., -1.3305e-01,\n",
            "           -3.1536e-01, -3.9519e-01],\n",
            "          ...,\n",
            "          [-5.2831e-01, -5.3157e-01, -2.6776e-01,  ..., -3.4551e-02,\n",
            "           -3.6118e-01, -3.2708e-01],\n",
            "          [-4.6413e-01, -9.1614e-01, -4.4808e-01,  ..., -6.8327e-01,\n",
            "           -9.8031e-01, -7.9416e-01],\n",
            "          [-2.5756e-01, -9.6135e-01, -5.9864e-01,  ..., -5.8158e-01,\n",
            "           -8.5021e-01, -3.4035e-01]],\n",
            "\n",
            "         [[ 1.3429e-01,  2.0323e-01,  1.0114e-01,  ...,  9.8933e-02,\n",
            "            1.8046e-01,  8.1834e-02],\n",
            "          [ 1.3777e-01,  1.4292e-01,  7.5107e-02,  ...,  7.2696e-02,\n",
            "            1.4538e-01,  7.6328e-02],\n",
            "          [ 8.3317e-02,  7.1234e-02,  3.3850e-02,  ...,  3.3148e-02,\n",
            "            4.8783e-02,  1.9369e-02],\n",
            "          ...,\n",
            "          [ 1.1798e-01,  1.0074e-01,  6.6271e-02,  ...,  4.8253e-02,\n",
            "            8.3712e-02,  4.3696e-02],\n",
            "          [ 2.0286e-01,  1.4898e-01,  7.9099e-02,  ...,  8.7099e-02,\n",
            "            1.4073e-01,  9.2088e-02],\n",
            "          [ 6.2271e-02,  4.6038e-02,  2.4136e-02,  ...,  2.9700e-02,\n",
            "            5.9247e-02,  1.6244e-02]],\n",
            "\n",
            "         [[-1.0987e-02, -1.3926e-03, -1.8439e-03,  ..., -1.7998e-03,\n",
            "           -4.3291e-03,  2.8253e-04],\n",
            "          [-1.3044e-02, -8.7671e-03, -5.2931e-03,  ..., -1.0680e-02,\n",
            "           -3.8086e-02, -6.4195e-03],\n",
            "          [-1.1030e-02, -8.2555e-03, -3.6191e-03,  ..., -8.3117e-03,\n",
            "           -9.7538e-03, -3.3539e-03],\n",
            "          ...,\n",
            "          [-1.9165e-02, -6.6165e-03,  1.7457e-03,  ..., -2.5992e-03,\n",
            "           -9.0729e-03, -1.2678e-03],\n",
            "          [-3.1542e-02, -2.2318e-02,  8.1636e-05,  ..., -7.7056e-03,\n",
            "           -2.2321e-02, -2.6527e-03],\n",
            "          [-7.0937e-02, -6.9651e-02, -8.8680e-03,  ..., -5.9171e-03,\n",
            "           -8.3381e-02, -2.6396e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.3572e-03, -1.0837e-02, -9.9673e-03,  ..., -1.4245e-02,\n",
            "           -1.4915e-02, -1.4034e-02],\n",
            "          [-9.5944e-03, -5.1764e-03, -8.8047e-03,  ..., -1.0455e-02,\n",
            "            5.4249e-03, -5.4344e-03],\n",
            "          [-6.7569e-03, -5.6823e-03, -5.3168e-03,  ..., -1.0290e-02,\n",
            "           -7.3735e-04,  1.5510e-02],\n",
            "          ...,\n",
            "          [ 1.5011e-03,  1.1828e-02,  4.0355e-03,  ..., -4.1968e-03,\n",
            "           -4.6005e-03, -4.5545e-03],\n",
            "          [ 5.9604e-02,  8.8325e-02,  5.5965e-02,  ...,  9.3309e-03,\n",
            "            5.5015e-02, -1.1864e-02],\n",
            "          [-4.4492e-03, -3.5987e-03,  1.2377e-02,  ...,  3.3415e-03,\n",
            "            7.0740e-03,  3.4002e-03]],\n",
            "\n",
            "         [[-1.1171e-03, -4.7661e-03, -7.0686e-03,  ..., -9.6639e-03,\n",
            "           -5.9077e-03,  9.4187e-04],\n",
            "          [ 1.8224e-03, -8.1751e-03, -1.4998e-02,  ..., -1.6234e-02,\n",
            "           -9.5558e-03, -3.8343e-04],\n",
            "          [-1.7196e-03, -1.4469e-02, -2.0052e-02,  ..., -2.1052e-02,\n",
            "           -1.7423e-02, -8.6600e-03],\n",
            "          ...,\n",
            "          [-4.1887e-03, -1.5942e-02, -2.0756e-02,  ..., -2.2517e-02,\n",
            "           -1.9546e-02, -1.0940e-02],\n",
            "          [-1.5304e-04, -7.7668e-03, -1.4897e-02,  ..., -1.7744e-02,\n",
            "           -1.1991e-02, -1.0167e-03],\n",
            "          [ 1.1430e-03,  1.7948e-03, -5.8856e-03,  ..., -6.8549e-03,\n",
            "           -1.6688e-03,  1.5733e-03]],\n",
            "\n",
            "         [[-4.5169e-03, -3.8953e-03, -4.6923e-03,  ..., -1.3083e-02,\n",
            "           -6.4658e-03,  6.7723e-03],\n",
            "          [-9.4916e-03, -1.5101e-02, -1.7267e-02,  ..., -2.1807e-02,\n",
            "           -1.3821e-02,  9.0818e-03],\n",
            "          [-1.2771e-02, -1.8616e-02, -2.1014e-02,  ..., -2.5451e-02,\n",
            "           -2.0683e-02, -7.3631e-04],\n",
            "          ...,\n",
            "          [-1.5454e-02, -1.9559e-02, -2.1141e-02,  ..., -2.5845e-02,\n",
            "           -2.0976e-02,  4.5686e-04],\n",
            "          [-8.5334e-03, -1.0645e-02, -1.5145e-02,  ..., -2.1389e-02,\n",
            "           -9.8878e-03,  1.8352e-02],\n",
            "          [-7.6100e-04, -2.2142e-03, -6.7722e-03,  ..., -8.3877e-03,\n",
            "            4.2632e-04,  8.1518e-03]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 2.3345e-01,  1.6195e-01,  5.7302e-02,  ...,  3.3042e-02,\n",
            "            6.9340e-02,  2.0728e-01],\n",
            "          [ 8.2612e-02, -9.3022e-02, -2.0200e-01,  ..., -5.8637e-02,\n",
            "           -4.7279e-01, -2.2274e-01],\n",
            "          [-7.0641e-02, -4.6504e-01, -8.3169e-02,  ...,  4.4433e-02,\n",
            "            2.4861e-02, -5.2127e-02],\n",
            "          ...,\n",
            "          [-1.2057e-01, -2.8176e-02,  2.8647e-02,  ...,  8.8983e-02,\n",
            "            4.1962e-02,  8.6395e-02],\n",
            "          [ 1.3059e-02, -3.5993e-01, -4.3233e-02,  ..., -1.6849e-01,\n",
            "           -3.6707e-01, -2.1336e-01],\n",
            "          [ 1.2925e-01, -5.4128e-01, -1.3842e-01,  ..., -1.5548e-01,\n",
            "           -3.8039e-01,  1.1907e-01]],\n",
            "\n",
            "         [[ 5.4591e-02,  1.8027e-01,  1.0223e-01,  ...,  1.0516e-01,\n",
            "            1.9034e-01,  7.8683e-02],\n",
            "          [ 1.0778e-01,  1.9515e-01,  8.1887e-02,  ...,  7.7683e-02,\n",
            "            1.5354e-01,  1.0414e-01],\n",
            "          [ 7.2665e-02,  8.3489e-02,  3.8588e-02,  ...,  3.2705e-02,\n",
            "            5.1335e-02,  3.0772e-02],\n",
            "          ...,\n",
            "          [ 1.1963e-01,  1.1896e-01,  7.6401e-02,  ...,  5.4355e-02,\n",
            "            8.6921e-02,  5.4152e-02],\n",
            "          [ 1.5827e-01,  1.8428e-01,  9.1132e-02,  ...,  9.8714e-02,\n",
            "            1.4278e-01,  1.0489e-01],\n",
            "          [ 3.0387e-02,  4.1836e-02,  2.9363e-02,  ...,  3.3134e-02,\n",
            "            5.8316e-02,  1.5525e-02]],\n",
            "\n",
            "         [[-9.9552e-03, -4.2944e-03, -3.4726e-03,  ..., -4.6558e-03,\n",
            "           -6.6163e-03, -2.6273e-03],\n",
            "          [-1.7560e-02, -2.7978e-02, -1.3251e-02,  ..., -1.8742e-02,\n",
            "           -3.9933e-02, -9.7250e-03],\n",
            "          [-1.0239e-02, -1.5107e-02, -9.0481e-03,  ..., -1.1154e-02,\n",
            "           -1.6759e-02, -7.5200e-03],\n",
            "          ...,\n",
            "          [-2.8763e-02, -1.3858e-02, -4.5602e-03,  ..., -1.0146e-02,\n",
            "           -1.5341e-02, -6.1943e-03],\n",
            "          [-8.9445e-02, -5.1269e-02, -1.1381e-02,  ..., -2.4944e-02,\n",
            "           -5.4102e-02, -1.8242e-02],\n",
            "          [-4.1501e-02, -4.8280e-02, -9.2788e-03,  ..., -1.2396e-02,\n",
            "           -6.9429e-02, -1.9525e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4580e-02, -1.4576e-02, -1.3399e-02,  ..., -1.7378e-02,\n",
            "           -1.9712e-02, -1.3538e-02],\n",
            "          [-1.3333e-02, -8.7106e-03, -1.0762e-02,  ..., -1.4229e-02,\n",
            "           -1.1549e-02, -6.9244e-03],\n",
            "          [-9.3847e-03, -7.4031e-03, -7.1030e-03,  ..., -6.4618e-03,\n",
            "           -8.3492e-03,  3.4039e-03],\n",
            "          ...,\n",
            "          [-5.7603e-03, -1.3090e-03, -3.1518e-03,  ..., -8.8808e-03,\n",
            "           -8.4422e-03, -3.2007e-03],\n",
            "          [ 1.8003e-02,  1.8036e-02,  8.3927e-03,  ..., -4.5981e-03,\n",
            "            6.5776e-03, -4.9533e-03],\n",
            "          [-2.8780e-02, -8.7196e-03,  2.0954e-03,  ..., -6.7206e-04,\n",
            "           -1.3510e-04, -2.6922e-03]],\n",
            "\n",
            "         [[ 6.6539e-07, -2.8263e-03, -5.8133e-03,  ..., -8.4474e-03,\n",
            "           -4.1621e-03,  2.6127e-03],\n",
            "          [ 2.6813e-03, -5.9233e-03, -1.3757e-02,  ..., -1.5298e-02,\n",
            "           -8.5015e-03,  1.5153e-03],\n",
            "          [-9.5340e-04, -1.2696e-02, -1.8680e-02,  ..., -1.9834e-02,\n",
            "           -1.5969e-02, -6.4946e-03],\n",
            "          ...,\n",
            "          [-3.1484e-03, -1.3688e-02, -1.9093e-02,  ..., -2.1257e-02,\n",
            "           -1.8103e-02, -8.7358e-03],\n",
            "          [ 7.8590e-04, -5.3069e-03, -1.2826e-02,  ..., -1.6248e-02,\n",
            "           -1.0593e-02,  1.9859e-03],\n",
            "          [ 1.8325e-03,  4.1645e-03, -3.3681e-03,  ..., -4.7537e-03,\n",
            "            1.0370e-03,  3.9197e-03]],\n",
            "\n",
            "         [[ 1.8583e-03,  5.9766e-03,  1.9030e-03,  ..., -5.8232e-03,\n",
            "            6.4074e-03,  3.3420e-02],\n",
            "          [ 9.1112e-04,  3.0423e-03, -7.8069e-03,  ..., -1.4490e-02,\n",
            "            5.7060e-04,  3.8587e-02],\n",
            "          [-5.2121e-03, -1.0407e-02, -1.7245e-02,  ..., -2.1667e-02,\n",
            "           -1.2954e-02,  1.3107e-02],\n",
            "          ...,\n",
            "          [-7.1200e-03, -1.1254e-02, -1.7415e-02,  ..., -2.2132e-02,\n",
            "           -1.3024e-02,  1.5424e-02],\n",
            "          [ 6.6621e-03,  8.7637e-03, -7.0931e-03,  ..., -1.3297e-02,\n",
            "            9.9978e-03,  5.9671e-02],\n",
            "          [ 4.5592e-03,  8.7013e-03,  6.5393e-04,  ..., -1.5821e-03,\n",
            "            1.1296e-02,  2.1270e-02]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 2.1400e-01,  1.3606e-01,  6.0965e-02,  ...,  3.6155e-02,\n",
            "            5.1318e-02,  1.6009e-01],\n",
            "          [ 3.1851e-02, -7.0381e-02, -1.3964e-01,  ..., -4.1246e-02,\n",
            "           -3.6241e-01, -1.3919e-01],\n",
            "          [-5.5182e-02, -3.4772e-01, -4.3073e-02,  ...,  4.6916e-02,\n",
            "            1.9621e-02, -3.1748e-02],\n",
            "          ...,\n",
            "          [-7.6865e-02,  2.2057e-03,  5.0023e-02,  ...,  9.6730e-02,\n",
            "            4.9362e-02,  7.7634e-02],\n",
            "          [-7.3152e-03, -2.7657e-01, -1.2930e-02,  ..., -1.2406e-01,\n",
            "           -3.0300e-01, -1.3758e-01],\n",
            "          [ 8.5777e-02, -3.7835e-01, -6.9045e-02,  ..., -1.0798e-01,\n",
            "           -2.6596e-01,  8.1776e-02]],\n",
            "\n",
            "         [[ 4.7202e-02,  1.7649e-01,  1.0770e-01,  ...,  1.0684e-01,\n",
            "            1.9074e-01,  7.3171e-02],\n",
            "          [ 1.0447e-01,  2.1018e-01,  8.5170e-02,  ...,  7.7796e-02,\n",
            "            1.5978e-01,  1.0884e-01],\n",
            "          [ 7.2958e-02,  8.6787e-02,  4.0166e-02,  ...,  3.6685e-02,\n",
            "            5.0902e-02,  3.0711e-02],\n",
            "          ...,\n",
            "          [ 1.2232e-01,  1.1974e-01,  8.2149e-02,  ...,  5.5018e-02,\n",
            "            8.7254e-02,  5.6269e-02],\n",
            "          [ 1.5859e-01,  1.9443e-01,  9.7857e-02,  ...,  1.0038e-01,\n",
            "            1.4175e-01,  1.1024e-01],\n",
            "          [ 2.8500e-02,  4.4232e-02,  3.2656e-02,  ...,  3.5695e-02,\n",
            "            6.5575e-02,  1.4600e-02]],\n",
            "\n",
            "         [[-1.2858e-02, -5.6511e-03, -5.0825e-03,  ..., -5.9986e-03,\n",
            "           -1.0104e-02, -3.0615e-03],\n",
            "          [-2.2142e-02, -2.9227e-02, -1.3812e-02,  ..., -2.1043e-02,\n",
            "           -6.4054e-02, -1.3639e-02],\n",
            "          [-1.2893e-02, -1.6135e-02, -1.1084e-02,  ..., -1.5763e-02,\n",
            "           -2.0837e-02, -9.1486e-03],\n",
            "          ...,\n",
            "          [-3.9583e-02, -1.6035e-02, -6.9616e-03,  ..., -1.2692e-02,\n",
            "           -2.0445e-02, -7.5601e-03],\n",
            "          [-1.1539e-01, -6.2619e-02, -9.4203e-03,  ..., -2.5277e-02,\n",
            "           -7.5496e-02, -2.6298e-02],\n",
            "          [-4.5656e-02, -5.7758e-02, -1.0370e-02,  ..., -1.1421e-02,\n",
            "           -8.0947e-02, -2.2412e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3756e-02, -1.3569e-02, -1.2418e-02,  ..., -1.5806e-02,\n",
            "           -1.7946e-02, -1.1284e-02],\n",
            "          [-1.1855e-02, -9.4647e-03, -1.0623e-02,  ..., -1.2691e-02,\n",
            "           -1.3018e-02, -6.9504e-03],\n",
            "          [-8.7324e-03, -8.6771e-03, -7.6944e-03,  ..., -8.5657e-03,\n",
            "           -8.8481e-03,  4.0375e-03],\n",
            "          ...,\n",
            "          [-8.3284e-03, -6.9995e-03, -6.2212e-03,  ..., -8.6240e-03,\n",
            "           -9.7865e-03, -2.9949e-03],\n",
            "          [ 4.6158e-03,  2.5348e-03, -1.2967e-03,  ..., -8.5875e-03,\n",
            "           -4.9992e-03, -4.9077e-03],\n",
            "          [-2.6168e-02, -8.1535e-03,  6.4346e-04,  ..., -1.7803e-03,\n",
            "           -1.0418e-03, -4.0076e-03]],\n",
            "\n",
            "         [[ 3.4519e-04, -2.4211e-03, -5.2544e-03,  ..., -8.4044e-03,\n",
            "           -4.4453e-03,  3.1722e-03],\n",
            "          [ 3.6831e-03, -5.2071e-03, -1.3002e-02,  ..., -1.4513e-02,\n",
            "           -6.8430e-03,  2.8583e-03],\n",
            "          [-2.8188e-04, -1.1924e-02, -1.8503e-02,  ..., -1.9955e-02,\n",
            "           -1.5863e-02, -6.4101e-03],\n",
            "          ...,\n",
            "          [-2.9068e-03, -1.3064e-02, -1.9025e-02,  ..., -2.1602e-02,\n",
            "           -1.8486e-02, -8.9572e-03],\n",
            "          [ 1.2838e-03, -3.6145e-03, -1.1773e-02,  ..., -1.5874e-02,\n",
            "           -9.6535e-03,  3.5222e-03],\n",
            "          [ 1.7550e-03,  4.9324e-03, -3.4159e-03,  ..., -4.8451e-03,\n",
            "            1.2760e-03,  4.0102e-03]],\n",
            "\n",
            "         [[ 2.3947e-03,  7.6985e-03,  3.8964e-03,  ..., -4.9088e-03,\n",
            "            8.0598e-03,  3.6351e-02],\n",
            "          [ 2.4833e-03,  8.3507e-03, -4.1733e-03,  ..., -1.2461e-02,\n",
            "            5.1352e-03,  4.4096e-02],\n",
            "          [-3.9342e-03, -7.4331e-03, -1.5481e-02,  ..., -2.1133e-02,\n",
            "           -1.0743e-02,  1.6073e-02],\n",
            "          ...,\n",
            "          [-6.2046e-03, -8.6594e-03, -1.5629e-02,  ..., -2.1415e-02,\n",
            "           -1.0817e-02,  1.8425e-02],\n",
            "          [ 8.4617e-03,  1.5332e-02, -2.8454e-03,  ..., -1.0715e-02,\n",
            "            1.7141e-02,  6.7830e-02],\n",
            "          [ 4.9531e-03,  1.0568e-02,  2.2898e-03,  ..., -1.2904e-04,\n",
            "            1.4063e-02,  2.3255e-02]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0352, -0.1254, -0.0141],\n",
            "          [-0.0070,  0.0353, -0.1151],\n",
            "          [-0.0996,  0.0201, -0.0272]],\n",
            "\n",
            "         [[ 0.1369,  0.0492, -0.0385],\n",
            "          [-0.0505, -0.0383,  0.0201],\n",
            "          [-0.1118, -0.0086, -0.0614]],\n",
            "\n",
            "         [[-0.0585,  0.1319,  0.0534],\n",
            "          [-0.0728, -0.0092,  0.0062],\n",
            "          [-0.0427, -0.0995,  0.0193]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0996,  0.0095,  0.1128],\n",
            "          [ 0.0173, -0.0172, -0.0110],\n",
            "          [-0.0186,  0.0032,  0.0064]],\n",
            "\n",
            "         [[ 0.0525, -0.0117,  0.0251],\n",
            "          [ 0.0128,  0.1101,  0.1137],\n",
            "          [ 0.1409,  0.0739,  0.0532]],\n",
            "\n",
            "         [[ 0.0968,  0.1416, -0.1365],\n",
            "          [-0.0175,  0.0761, -0.0085],\n",
            "          [-0.0592, -0.0816,  0.0664]]],\n",
            "\n",
            "\n",
            "        [[[-0.0771, -0.1109, -0.2208],\n",
            "          [-0.1218,  0.0112, -0.1585],\n",
            "          [-0.1663, -0.0755, -0.3225]],\n",
            "\n",
            "         [[ 0.2954,  0.0867,  0.1391],\n",
            "          [ 0.2316,  0.1037,  0.2353],\n",
            "          [ 0.2607,  0.1945,  0.3854]],\n",
            "\n",
            "         [[ 0.3181,  0.2411,  0.2450],\n",
            "          [ 0.2451,  0.2645,  0.1568],\n",
            "          [ 0.3264,  0.0362,  0.1498]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1820,  0.2802,  0.1346],\n",
            "          [ 0.2082,  0.1985,  0.0553],\n",
            "          [ 0.2518,  0.1300,  0.0610]],\n",
            "\n",
            "         [[ 0.2908,  0.1981,  0.3896],\n",
            "          [ 0.1697,  0.1642,  0.1724],\n",
            "          [ 0.3206,  0.1686,  0.1053]],\n",
            "\n",
            "         [[ 0.1151,  0.0840, -0.0869],\n",
            "          [ 0.1312, -0.0320,  0.1564],\n",
            "          [-0.0447,  0.1185,  0.1804]]],\n",
            "\n",
            "\n",
            "        [[[-0.1722, -0.2449, -0.2758],\n",
            "          [-0.2324, -0.1681, -0.1521],\n",
            "          [-0.1551, -0.0744, -0.0767]],\n",
            "\n",
            "         [[ 0.2462,  0.2521,  0.2271],\n",
            "          [ 0.2108,  0.0713,  0.0910],\n",
            "          [ 0.3778,  0.1154,  0.1177]],\n",
            "\n",
            "         [[ 0.1395,  0.1641,  0.1988],\n",
            "          [ 0.1376,  0.1746,  0.0305],\n",
            "          [ 0.2923,  0.2686,  0.1150]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.3121,  0.2115,  0.3092],\n",
            "          [ 0.2709,  0.1236,  0.1552],\n",
            "          [ 0.3462,  0.2251,  0.0581]],\n",
            "\n",
            "         [[ 0.1853,  0.0626,  0.1741],\n",
            "          [ 0.1888,  0.1735,  0.2835],\n",
            "          [ 0.2573,  0.1738,  0.2386]],\n",
            "\n",
            "         [[ 0.2663,  0.0682,  0.1924],\n",
            "          [ 0.1672,  0.0303,  0.2500],\n",
            "          [ 0.1890,  0.0685,  0.0923]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.1166, -0.0642, -0.0586],\n",
            "          [-0.0817,  0.0944,  0.0725],\n",
            "          [-0.0390, -0.0023,  0.0965]],\n",
            "\n",
            "         [[ 0.1672,  0.3017,  0.1123],\n",
            "          [ 0.2004,  0.0211,  0.1129],\n",
            "          [ 0.2245,  0.0521, -0.0152]],\n",
            "\n",
            "         [[ 0.1590,  0.1091,  0.1488],\n",
            "          [ 0.0128, -0.0308,  0.0142],\n",
            "          [-0.0364, -0.1356, -0.2041]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0554,  0.1174,  0.2361],\n",
            "          [ 0.0237,  0.1844,  0.0695],\n",
            "          [ 0.1149,  0.0902,  0.1074]],\n",
            "\n",
            "         [[ 0.1406,  0.1428, -0.0379],\n",
            "          [ 0.1285, -0.0208, -0.0049],\n",
            "          [ 0.1290, -0.0278,  0.0370]],\n",
            "\n",
            "         [[-0.1567, -0.1731, -0.2062],\n",
            "          [-0.1649, -0.0016, -0.0470],\n",
            "          [ 0.0040, -0.0331,  0.0857]]],\n",
            "\n",
            "\n",
            "        [[[-0.0059, -0.0970, -0.0184],\n",
            "          [-0.0970, -0.0530, -0.0336],\n",
            "          [ 0.0150,  0.0103,  0.0578]],\n",
            "\n",
            "         [[ 0.1602,  0.0665, -0.0990],\n",
            "          [ 0.0577,  0.1261,  0.0580],\n",
            "          [-0.0006,  0.0219, -0.0289]],\n",
            "\n",
            "         [[ 0.0458, -0.0403,  0.0618],\n",
            "          [ 0.0769,  0.0056, -0.0591],\n",
            "          [ 0.0228,  0.0935,  0.0983]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0062, -0.0106,  0.0021],\n",
            "          [ 0.0676, -0.0199,  0.0054],\n",
            "          [-0.0399,  0.0035,  0.1480]],\n",
            "\n",
            "         [[ 0.1385,  0.0292,  0.0426],\n",
            "          [ 0.0585,  0.0654,  0.0221],\n",
            "          [ 0.1200,  0.0149,  0.0678]],\n",
            "\n",
            "         [[ 0.0988, -0.0488,  0.0955],\n",
            "          [ 0.0844,  0.0842, -0.0158],\n",
            "          [-0.0254,  0.0242, -0.0200]]],\n",
            "\n",
            "\n",
            "        [[[-0.0096, -0.0048, -0.0990],\n",
            "          [-0.0879, -0.1210, -0.1358],\n",
            "          [-0.0196, -0.0678, -0.1458]],\n",
            "\n",
            "         [[ 0.1501, -0.0258,  0.0544],\n",
            "          [ 0.0630,  0.0505, -0.0016],\n",
            "          [ 0.1195,  0.1597,  0.0027]],\n",
            "\n",
            "         [[ 0.0261,  0.0075,  0.0380],\n",
            "          [-0.0085, -0.0236, -0.0338],\n",
            "          [-0.0018,  0.0253, -0.0229]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0162,  0.0190,  0.0461],\n",
            "          [ 0.0988,  0.0938,  0.0626],\n",
            "          [ 0.1009,  0.0625,  0.0037]],\n",
            "\n",
            "         [[ 0.0018,  0.0870, -0.0443],\n",
            "          [ 0.0874, -0.0287,  0.0039],\n",
            "          [ 0.1288,  0.0935,  0.0112]],\n",
            "\n",
            "         [[ 0.0547, -0.0389, -0.0705],\n",
            "          [ 0.1002, -0.0022, -0.0444],\n",
            "          [-0.0777,  0.0306,  0.0574]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 8.5950e-02,  2.1176e-03,  1.1476e-01],\n",
            "          [ 3.2819e-01,  2.3540e-01,  1.4370e-01],\n",
            "          [ 1.6867e-01,  1.9630e-01,  1.5708e-01]],\n",
            "\n",
            "         [[-1.8979e-01, -8.3407e-02, -7.4844e-02],\n",
            "          [-1.0264e-01, -1.3807e-01, -9.3270e-02],\n",
            "          [-1.7645e-01, -5.3074e-02, -5.3638e-02]],\n",
            "\n",
            "         [[ 9.4251e-02, -3.8082e-02, -5.0724e-03],\n",
            "          [-8.2129e-02, -7.0530e-02, -1.3850e-01],\n",
            "          [-4.4343e-02, -3.1990e-02, -4.1476e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4928e-02, -2.1202e-01, -1.8171e-01],\n",
            "          [-1.7417e-01, -2.7833e-01, -2.0432e-01],\n",
            "          [-4.0885e-02, -1.5778e-01, -1.8617e-01]],\n",
            "\n",
            "         [[-7.4427e-02, -1.8057e-01, -2.8289e-01],\n",
            "          [-1.7788e-01, -1.9798e-01, -2.6347e-01],\n",
            "          [ 1.2187e-01, -1.5121e-01, -2.1372e-02]],\n",
            "\n",
            "         [[-1.4100e-01, -2.0493e-01, -2.2920e-01],\n",
            "          [-1.1482e-02, -7.0624e-02, -5.8701e-02],\n",
            "          [-4.7923e-02, -9.8460e-02, -1.2108e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3488e-02, -3.8484e-02,  1.2101e-01],\n",
            "          [-1.3442e-02,  6.2245e-02,  4.5993e-02],\n",
            "          [-9.1509e-03,  2.0113e-02,  7.7965e-02]],\n",
            "\n",
            "         [[ 1.2091e-01,  2.6111e-01,  4.7025e-02],\n",
            "          [ 4.9141e-02,  2.5490e-01,  7.0922e-02],\n",
            "          [ 1.2750e-01,  8.4566e-02,  2.7826e-02]],\n",
            "\n",
            "         [[ 3.0074e-01,  1.5300e-01,  2.1002e-01],\n",
            "          [ 2.4634e-01,  1.4453e-01,  1.7158e-01],\n",
            "          [ 3.6646e-01,  1.6347e-01,  1.5739e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.0417e-02, -3.3054e-02, -2.8002e-02],\n",
            "          [-1.5042e-02, -9.5953e-02, -4.3928e-02],\n",
            "          [ 5.7462e-02, -2.7764e-02, -5.0186e-02]],\n",
            "\n",
            "         [[ 2.0104e-01,  5.2971e-02,  1.9250e-02],\n",
            "          [ 1.4247e-01, -4.1707e-02,  8.3747e-03],\n",
            "          [ 1.9522e-01, -3.1287e-02,  1.5522e-03]],\n",
            "\n",
            "         [[ 4.1044e-02,  1.3460e-01,  8.8579e-02],\n",
            "          [ 1.3960e-01,  1.8998e-01,  6.9531e-02],\n",
            "          [ 8.7850e-02,  1.4500e-01,  2.1784e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3301e-01,  9.5095e-02,  1.3561e-01],\n",
            "          [ 1.9737e-01,  6.6898e-02,  1.3227e-01],\n",
            "          [ 1.5955e-01,  1.3676e-01,  1.0173e-01]],\n",
            "\n",
            "         [[ 2.3450e-02, -3.1148e-02,  4.6656e-02],\n",
            "          [-1.4437e-01, -2.2829e-02,  5.0815e-02],\n",
            "          [-4.1164e-02, -3.0224e-02,  6.8048e-02]],\n",
            "\n",
            "         [[ 2.3215e-01,  2.6360e-01,  2.5939e-01],\n",
            "          [ 1.6679e-01,  8.3161e-02,  5.5814e-02],\n",
            "          [ 5.6327e-02,  1.8961e-02,  1.9581e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5319e-01, -1.5245e-01, -9.8550e-02],\n",
            "          [-2.0838e-01, -2.7107e-01, -1.2332e-01],\n",
            "          [-1.8915e-01, -8.5086e-02, -7.5776e-02]],\n",
            "\n",
            "         [[ 1.5477e-02, -8.8649e-02, -4.3831e-02],\n",
            "          [-5.8633e-02,  9.2625e-02,  7.1743e-02],\n",
            "          [ 9.6891e-02,  2.2579e-02,  4.1130e-02]],\n",
            "\n",
            "         [[ 2.0392e-01,  1.7876e-01,  1.5806e-01],\n",
            "          [ 8.4059e-02,  2.4221e-01,  6.9562e-02],\n",
            "          [ 1.1271e-01,  2.7599e-01,  1.4951e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.3466e-03,  1.1310e-01,  8.6889e-02],\n",
            "          [ 1.4335e-02,  5.6803e-02,  1.6697e-01],\n",
            "          [ 4.0753e-02,  1.8017e-01,  2.2222e-01]],\n",
            "\n",
            "         [[-2.8082e-02,  6.8996e-02, -1.1226e-01],\n",
            "          [-2.3980e-02, -9.7991e-03, -1.8594e-01],\n",
            "          [ 3.5702e-02, -1.2364e-01, -1.4716e-01]],\n",
            "\n",
            "         [[ 4.3343e-03, -2.5870e-04, -5.7476e-03],\n",
            "          [ 1.5558e-02,  1.1410e-01, -1.1729e-01],\n",
            "          [-1.9294e-03,  5.2864e-02, -6.0964e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2280e-01, -8.9715e-02, -8.6558e-02],\n",
            "          [-1.8252e-01, -5.2531e-02, -1.9444e-02],\n",
            "          [-1.2191e-01, -6.0654e-02, -5.8712e-02]],\n",
            "\n",
            "         [[-5.5960e-02,  9.4130e-02,  2.5924e-02],\n",
            "          [-9.3053e-02,  7.3820e-02, -8.5624e-02],\n",
            "          [-2.4768e-02, -4.4819e-02, -5.1216e-03]],\n",
            "\n",
            "         [[ 1.8072e-01,  1.4057e-01,  1.4271e-01],\n",
            "          [ 3.3261e-02,  4.5830e-02,  2.1307e-01],\n",
            "          [ 5.1518e-02,  9.1293e-02,  2.2521e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.0193e-02,  7.2333e-02,  6.6005e-02],\n",
            "          [-1.1368e-01, -4.9065e-02, -2.6727e-02],\n",
            "          [-7.3717e-02, -2.2591e-02,  7.3243e-02]],\n",
            "\n",
            "         [[ 8.0060e-02,  8.9831e-02,  1.2136e-01],\n",
            "          [ 9.3240e-03,  5.3622e-02,  1.5973e-01],\n",
            "          [ 4.0146e-02,  1.7332e-01,  1.1563e-01]],\n",
            "\n",
            "         [[-1.0917e-02,  1.1455e-01, -4.2595e-02],\n",
            "          [-1.7868e-02,  1.8873e-03,  7.6316e-02],\n",
            "          [ 8.1589e-02,  1.7198e-01,  1.0537e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0043e-01,  1.2378e-01,  1.5504e-02],\n",
            "          [-7.0323e-02, -3.9783e-02,  1.0834e-01],\n",
            "          [-2.7622e-02,  9.9276e-02,  5.6516e-02]],\n",
            "\n",
            "         [[-3.8571e-02,  9.1676e-02,  3.7708e-02],\n",
            "          [ 2.0633e-02,  1.4109e-01,  1.0513e-02],\n",
            "          [ 5.3954e-02,  5.9279e-03, -5.4562e-03]],\n",
            "\n",
            "         [[ 1.7464e-02,  9.3674e-02, -3.7057e-03],\n",
            "          [-2.3425e-02,  1.3420e-02,  8.8986e-02],\n",
            "          [-6.5366e-02, -5.3960e-02,  2.7634e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.9744e-02,  3.8120e-02, -1.3254e-03],\n",
            "          [ 2.5126e-02,  4.0843e-02,  9.4737e-03],\n",
            "          [ 3.8759e-02,  1.1141e-01,  7.7442e-02]],\n",
            "\n",
            "         [[ 6.3627e-02, -9.7754e-02,  1.2273e-02],\n",
            "          [-9.4671e-02,  6.7932e-02,  4.8557e-02],\n",
            "          [-4.5491e-02, -7.6461e-02,  2.0903e-02]],\n",
            "\n",
            "         [[ 9.1748e-02, -6.7371e-02,  4.4008e-02],\n",
            "          [ 2.0771e-02, -2.3304e-02, -4.0451e-02],\n",
            "          [-3.3025e-02,  8.0647e-02, -1.9994e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.7076e-02,  8.9024e-02, -5.2431e-02],\n",
            "          [-6.8675e-02, -3.2588e-02,  5.8751e-05],\n",
            "          [-4.4756e-02,  4.0102e-03, -9.2020e-02]],\n",
            "\n",
            "         [[ 5.0372e-02,  1.9120e-02,  9.3342e-02],\n",
            "          [ 4.9038e-02, -4.4041e-02,  9.8345e-02],\n",
            "          [ 1.5616e-02, -1.2309e-02,  2.2824e-02]],\n",
            "\n",
            "         [[ 8.5339e-02,  4.7554e-02, -4.4523e-02],\n",
            "          [-2.0033e-02,  6.7947e-02, -6.3436e-02],\n",
            "          [-2.8846e-02,  1.0011e-01, -3.8745e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 2.4928e-02, -1.0792e-01, -2.3846e-01],\n",
            "          [ 1.3487e-01,  1.0331e-01, -1.1496e-01],\n",
            "          [-2.3495e-01, -6.4430e-02, -1.9153e-01]],\n",
            "\n",
            "         [[ 1.5702e-01,  3.9249e-02,  4.0734e-01],\n",
            "          [ 1.1603e-01, -1.5290e-01,  1.4590e-01],\n",
            "          [ 2.9350e-01, -9.9347e-03,  2.1442e-01]],\n",
            "\n",
            "         [[ 4.5114e-01,  3.0705e-01,  3.1661e-01],\n",
            "          [ 2.1682e-01, -1.7152e-01,  7.6471e-02],\n",
            "          [ 1.5625e-01, -1.0363e-01,  1.2910e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.0522e-01,  2.9404e-02,  1.1501e-01],\n",
            "          [ 8.1401e-02, -7.0609e-02,  8.5693e-02],\n",
            "          [ 1.8143e-01,  7.7087e-02,  8.8000e-02]],\n",
            "\n",
            "         [[ 1.0860e-01,  2.1828e-02,  1.3840e-01],\n",
            "          [ 2.2272e-01,  6.1843e-02,  8.2873e-02],\n",
            "          [ 1.8691e-01,  2.2427e-01,  1.7677e-01]],\n",
            "\n",
            "         [[ 1.8115e-01,  2.8674e-01,  2.5947e-01],\n",
            "          [ 1.5191e-01,  2.9141e-02,  1.7038e-01],\n",
            "          [-1.7189e-02,  2.6245e-03,  1.0330e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.0171e-01, -8.9447e-02, -7.9288e-03],\n",
            "          [-3.7188e-02,  1.0573e-01,  9.4698e-02],\n",
            "          [ 1.1612e-01,  2.0669e-01,  2.0508e-01]],\n",
            "\n",
            "         [[-6.8507e-03, -8.2567e-02, -5.9836e-03],\n",
            "          [-1.3206e-01, -1.2172e-01, -4.2446e-02],\n",
            "          [-2.8863e-01, -2.2715e-01, -2.3932e-01]],\n",
            "\n",
            "         [[ 5.9410e-02, -2.9474e-02,  2.2303e-02],\n",
            "          [-8.7985e-02, -1.0722e-01, -7.0665e-02],\n",
            "          [-1.1831e-01, -1.6259e-01, -1.3585e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7709e-01, -1.3750e-01, -1.9516e-02],\n",
            "          [-1.5030e-01, -1.0464e-01,  4.2373e-02],\n",
            "          [-1.3776e-01, -1.2583e-01, -2.7245e-03]],\n",
            "\n",
            "         [[-1.1072e-01, -3.1444e-02, -1.0936e-01],\n",
            "          [-9.6805e-02, -1.7346e-01, -2.4732e-01],\n",
            "          [-6.0664e-02, -1.4454e-01, -5.8209e-02]],\n",
            "\n",
            "         [[-5.9517e-02,  6.7179e-02, -1.0337e-01],\n",
            "          [-9.0810e-02,  5.2173e-02,  1.0702e-02],\n",
            "          [ 7.9113e-05, -1.2545e-01, -5.8846e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7362e-02, -1.2084e-01, -2.2743e-01],\n",
            "          [ 1.8709e-01,  1.0659e-01, -1.9559e-01],\n",
            "          [-1.3330e-02, -8.5095e-02, -3.0446e-01]],\n",
            "\n",
            "         [[ 1.8787e-01,  1.3543e-01,  2.2294e-01],\n",
            "          [ 2.5886e-02, -9.2559e-02,  1.2471e-01],\n",
            "          [ 1.1520e-01, -6.2750e-02,  2.8394e-01]],\n",
            "\n",
            "         [[ 1.5513e-01,  1.4402e-01,  1.0494e-01],\n",
            "          [-4.4532e-02, -6.0486e-02,  7.2963e-02],\n",
            "          [-4.7635e-03, -1.5464e-01, -3.1629e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3366e-01,  1.0375e-01,  1.3384e-01],\n",
            "          [ 2.0371e-01,  1.2181e-02,  2.2827e-02],\n",
            "          [ 1.0561e-01, -3.9893e-02, -1.0715e-01]],\n",
            "\n",
            "         [[ 1.3380e-01, -5.0928e-02,  1.2479e-01],\n",
            "          [ 1.2964e-01, -5.1187e-02,  1.0597e-01],\n",
            "          [ 2.0217e-01,  4.0154e-02,  2.0869e-01]],\n",
            "\n",
            "         [[ 9.2872e-02,  4.9710e-02, -9.1062e-03],\n",
            "          [ 5.6937e-02,  2.9270e-02,  4.6040e-02],\n",
            "          [ 1.2700e-01, -1.5699e-02,  1.2699e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.8336e-03, -1.8690e-02, -1.1834e-02],\n",
            "          [-3.3024e-02, -1.4509e-01,  1.9942e-02],\n",
            "          [ 6.6489e-02, -8.0445e-02,  1.4246e-01]],\n",
            "\n",
            "         [[-1.2530e-01, -4.3230e-02, -3.4977e-03],\n",
            "          [ 1.4891e-02,  8.2094e-02,  2.0238e-02],\n",
            "          [ 1.1426e-01,  6.3763e-02,  1.2191e-01]],\n",
            "\n",
            "         [[-2.5089e-01, -6.9633e-02, -5.0695e-02],\n",
            "          [-1.7039e-01, -3.3881e-02,  9.7421e-02],\n",
            "          [ 4.7260e-04,  1.0660e-01,  1.6588e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2952e-01,  9.6856e-03, -9.8753e-02],\n",
            "          [-1.6768e-01, -5.7798e-02,  5.3194e-02],\n",
            "          [ 3.6644e-02,  8.2291e-02,  2.0448e-02]],\n",
            "\n",
            "         [[ 1.9640e-02,  1.9564e-02,  6.2229e-02],\n",
            "          [ 1.0617e-01,  7.5422e-02,  1.0932e-01],\n",
            "          [-1.8443e-02,  1.0497e-01, -1.4029e-02]],\n",
            "\n",
            "         [[ 1.0057e-01,  1.3096e-01,  1.8677e-03],\n",
            "          [ 1.7206e-01, -8.3289e-02, -1.7427e-03],\n",
            "          [ 9.0253e-02,  2.0205e-02, -1.7198e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3694e-02, -3.4595e-02, -5.0558e-02],\n",
            "          [ 4.1892e-02,  7.9700e-02, -5.5798e-03],\n",
            "          [-1.7726e-03,  1.0289e-01,  7.5933e-02]],\n",
            "\n",
            "         [[ 6.6696e-02,  4.0292e-02, -1.0939e-01],\n",
            "          [-1.4932e-01, -6.9534e-02, -1.2611e-01],\n",
            "          [-1.4111e-01, -9.3187e-02, -1.2358e-01]],\n",
            "\n",
            "         [[-4.0242e-02,  2.2339e-02,  8.3930e-02],\n",
            "          [-7.7999e-03, -6.9710e-02,  4.9334e-02],\n",
            "          [-2.4689e-02, -6.5226e-02,  2.2084e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2763e-02, -2.2497e-02,  5.5047e-02],\n",
            "          [ 6.7852e-03, -5.9826e-02, -2.7897e-03],\n",
            "          [-7.2061e-02, -1.0755e-01, -4.9107e-02]],\n",
            "\n",
            "         [[-5.0655e-02, -1.5501e-02, -6.0973e-04],\n",
            "          [ 4.2041e-02, -6.9502e-02,  6.5967e-02],\n",
            "          [-4.3545e-02, -7.1744e-02,  2.2912e-02]],\n",
            "\n",
            "         [[-7.2330e-02, -6.3010e-02,  6.3563e-02],\n",
            "          [ 9.2238e-03,  2.8642e-02, -2.6330e-03],\n",
            "          [ 2.1086e-02,  9.0327e-02, -3.8899e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.9041e-02,  7.6331e-02, -4.4670e-02],\n",
            "          [ 3.0675e-02,  9.8801e-02,  1.6407e-02],\n",
            "          [ 2.8393e-02,  8.2891e-02,  8.9452e-02]],\n",
            "\n",
            "         [[-8.4204e-02,  1.5678e-02, -8.0163e-02],\n",
            "          [ 7.9602e-02,  5.9203e-02,  4.4789e-02],\n",
            "          [ 6.4905e-02,  8.6797e-02, -2.8582e-02]],\n",
            "\n",
            "         [[-8.0054e-02,  3.3593e-02, -3.5693e-02],\n",
            "          [-3.1419e-02,  4.1501e-02, -2.6674e-02],\n",
            "          [-6.3724e-02,  2.5465e-02,  4.9331e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.2238e-02,  1.3512e-02, -1.7803e-02],\n",
            "          [ 3.4901e-02,  3.2908e-02,  7.3285e-02],\n",
            "          [ 1.0366e-01,  5.6796e-02, -9.1402e-03]],\n",
            "\n",
            "         [[-8.7387e-02, -3.7085e-03, -6.5950e-03],\n",
            "          [-9.7800e-02,  4.4012e-02, -2.3825e-02],\n",
            "          [-5.8737e-02,  7.4227e-02, -4.0492e-02]],\n",
            "\n",
            "         [[ 2.6185e-02,  5.6616e-02, -4.4246e-02],\n",
            "          [ 1.8789e-02,  9.4381e-02,  6.5170e-02],\n",
            "          [-8.6544e-02,  5.6972e-02, -5.0269e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-3.2286e-02, -1.8855e-01,  1.5020e-01],\n",
            "          [-5.9825e-02, -1.9823e-01, -1.6050e-01],\n",
            "          [-2.7659e-01, -2.0849e-01, -1.6901e-01]],\n",
            "\n",
            "         [[ 1.6768e-01,  1.3901e-01,  7.2829e-02],\n",
            "          [ 1.6271e-01,  9.2394e-02,  3.2282e-01],\n",
            "          [ 1.4400e-01,  1.7363e-01,  3.0839e-01]],\n",
            "\n",
            "         [[-5.0153e-02,  2.0492e-01,  3.3127e-01],\n",
            "          [ 7.1462e-02,  2.8140e-01,  4.6470e-01],\n",
            "          [ 3.0407e-01,  3.7413e-01,  3.4068e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.7401e-03,  6.9220e-03,  1.0751e-01],\n",
            "          [ 8.5336e-02,  2.3893e-01,  2.3318e-01],\n",
            "          [ 2.1910e-01,  1.8949e-01,  2.1649e-01]],\n",
            "\n",
            "         [[ 1.0781e-01,  3.0263e-01,  2.3956e-01],\n",
            "          [ 2.3322e-01,  2.8627e-01,  1.4811e-01],\n",
            "          [ 3.0564e-01,  2.1985e-01,  8.8399e-02]],\n",
            "\n",
            "         [[ 2.9041e-02,  7.3812e-02, -1.5139e-04],\n",
            "          [ 1.2916e-01,  2.6656e-01,  2.2515e-01],\n",
            "          [ 1.5706e-01,  2.3298e-01,  2.2775e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 5.8440e-02, -1.5582e-01, -1.1142e-01],\n",
            "          [-6.2462e-02, -4.6304e-02, -3.0200e-02],\n",
            "          [-6.9434e-02, -4.4935e-02, -4.8523e-02]],\n",
            "\n",
            "         [[ 1.9841e-01,  9.3519e-02,  2.3520e-01],\n",
            "          [ 9.5928e-02, -3.9642e-02,  2.2613e-01],\n",
            "          [ 1.4487e-01,  1.7343e-01,  2.5667e-01]],\n",
            "\n",
            "         [[ 9.5885e-02,  4.6629e-03,  1.6392e-01],\n",
            "          [ 5.8504e-03, -1.1868e-03,  9.5113e-02],\n",
            "          [ 1.6075e-01, -1.0638e-02, -8.2514e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7636e-01,  3.4589e-02,  1.3509e-01],\n",
            "          [ 1.6257e-01,  8.1596e-02,  3.5920e-02],\n",
            "          [ 1.8738e-01,  1.2821e-01,  1.9054e-02]],\n",
            "\n",
            "         [[ 9.8939e-02,  1.7331e-01,  3.5659e-01],\n",
            "          [ 1.1905e-01, -5.9919e-02,  4.2999e-02],\n",
            "          [ 2.8278e-02, -1.0130e-01, -7.6108e-03]],\n",
            "\n",
            "         [[ 3.1764e-03, -5.2296e-02, -1.7424e-01],\n",
            "          [-6.0225e-02, -1.3030e-01,  5.2865e-02],\n",
            "          [-3.2617e-02, -4.4772e-02,  9.5839e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.2238e-01, -6.9466e-02,  1.4385e-02],\n",
            "          [-1.9776e-01,  4.6885e-02, -1.3629e-03],\n",
            "          [-2.0193e-01,  1.3984e-01,  3.4855e-02]],\n",
            "\n",
            "         [[ 8.4347e-03,  3.6428e-02,  4.4819e-02],\n",
            "          [ 2.4096e-01,  9.3576e-02,  9.3980e-02],\n",
            "          [ 2.4852e-01,  1.3812e-01,  1.2335e-01]],\n",
            "\n",
            "         [[-3.5796e-02, -7.7878e-02, -1.2060e-02],\n",
            "          [-8.4499e-03, -5.5906e-02,  7.2056e-02],\n",
            "          [ 1.4743e-01,  1.4167e-01, -1.1791e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4743e-01,  1.2725e-01,  1.9336e-01],\n",
            "          [ 6.6800e-02,  1.8500e-01,  1.4612e-01],\n",
            "          [ 1.1714e-01,  4.2612e-02,  1.1089e-01]],\n",
            "\n",
            "         [[-5.9523e-03,  1.3591e-01,  1.0763e-01],\n",
            "          [ 5.2721e-02,  7.0956e-02,  1.7670e-01],\n",
            "          [ 1.7308e-01,  2.0221e-01,  1.1081e-01]],\n",
            "\n",
            "         [[ 1.6637e-01,  9.0165e-02,  1.2910e-01],\n",
            "          [ 2.2120e-02, -9.2718e-02,  3.4569e-02],\n",
            "          [ 1.0872e-01, -1.1574e-01, -9.7916e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.6215e-01, -2.2336e-01, -1.2367e-01],\n",
            "          [-1.3237e-01, -7.7846e-02, -1.9623e-01],\n",
            "          [-1.3987e-01, -5.5087e-02, -5.5522e-02]],\n",
            "\n",
            "         [[-7.9970e-02,  6.3347e-02,  1.9223e-01],\n",
            "          [ 5.3698e-02,  7.2145e-02,  2.5294e-01],\n",
            "          [ 1.9655e-01,  1.4594e-01,  1.8827e-01]],\n",
            "\n",
            "         [[ 6.8005e-03,  1.1545e-02,  1.6225e-02],\n",
            "          [-3.1133e-02,  1.4668e-02, -1.9820e-01],\n",
            "          [-1.9626e-01, -5.8946e-02, -1.6154e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1696e-01, -5.6042e-02,  1.2254e-01],\n",
            "          [-1.1913e-01, -3.4698e-02,  1.6738e-01],\n",
            "          [-2.1118e-02,  5.7257e-02,  1.1928e-01]],\n",
            "\n",
            "         [[-1.7774e-02,  2.9736e-02,  1.3023e-01],\n",
            "          [ 2.4270e-02,  1.6674e-01,  1.0517e-01],\n",
            "          [ 9.4717e-02,  1.3730e-01,  8.5835e-02]],\n",
            "\n",
            "         [[ 5.2344e-02, -5.6161e-02, -4.7906e-02],\n",
            "          [ 5.1891e-02,  2.2673e-02,  4.5137e-02],\n",
            "          [ 4.6915e-02, -1.7896e-02,  1.4040e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.5645e-02, -6.0035e-02,  1.2232e-01],\n",
            "          [-1.3314e-01, -1.0452e-01,  2.9359e-02],\n",
            "          [ 1.7646e-02,  6.8996e-02, -5.1389e-02]],\n",
            "\n",
            "         [[ 1.3342e-01,  8.0090e-02, -5.9328e-02],\n",
            "          [ 1.0371e-01,  8.2780e-03, -6.2928e-02],\n",
            "          [ 5.1162e-02,  6.1618e-02,  8.6198e-02]],\n",
            "\n",
            "         [[ 9.2543e-02, -5.5630e-02, -6.1159e-02],\n",
            "          [ 4.8885e-03, -6.6308e-02, -3.2363e-02],\n",
            "          [-6.8636e-02, -6.4616e-03,  9.6108e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5538e-02, -2.9149e-02, -9.6881e-03],\n",
            "          [ 4.5108e-02,  5.9020e-02, -4.0143e-02],\n",
            "          [ 6.7264e-02, -4.8988e-02,  2.0317e-02]],\n",
            "\n",
            "         [[ 1.9720e-02,  1.2245e-01,  5.3948e-02],\n",
            "          [ 1.0603e-01,  1.4168e-02,  5.8141e-02],\n",
            "          [ 7.1680e-02,  6.2312e-02, -4.5222e-02]],\n",
            "\n",
            "         [[-7.7675e-02,  9.8228e-02, -7.9376e-02],\n",
            "          [ 3.6272e-02, -8.0306e-02,  6.2702e-02],\n",
            "          [-3.4069e-02, -6.7231e-03,  8.2045e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.2723e-03,  3.2818e-02,  4.7517e-02],\n",
            "          [-1.1124e-01, -3.8925e-02, -1.6506e-01],\n",
            "          [-9.1044e-02, -7.4382e-02, -6.8993e-02]],\n",
            "\n",
            "         [[-5.3926e-02, -7.8616e-02, -1.3822e-01],\n",
            "          [ 7.0642e-02,  1.0134e-01, -9.7544e-03],\n",
            "          [ 1.2465e-01,  1.7819e-01,  1.3475e-01]],\n",
            "\n",
            "         [[-9.6046e-02, -5.5223e-02,  9.0300e-03],\n",
            "          [-5.8397e-02,  7.8379e-02,  1.0014e-01],\n",
            "          [ 3.4258e-02,  9.1498e-02, -3.2651e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.1416e-03,  2.0198e-02, -7.3553e-02],\n",
            "          [ 1.8395e-02,  1.4519e-01,  7.9895e-02],\n",
            "          [-3.4871e-02,  6.6611e-02,  6.1138e-02]],\n",
            "\n",
            "         [[ 1.3514e-02,  3.5964e-02, -5.9367e-02],\n",
            "          [ 7.3444e-03, -5.2621e-03, -3.3222e-02],\n",
            "          [ 1.0805e-01,  3.7945e-02, -1.4369e-02]],\n",
            "\n",
            "         [[-5.7246e-02,  9.2046e-02,  7.5634e-02],\n",
            "          [ 6.8535e-02, -2.3922e-02, -2.9414e-02],\n",
            "          [ 4.6106e-03,  1.3696e-02,  6.6959e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.1325, -0.0567,  0.0284],\n",
            "          [-0.0595, -0.1292, -0.0171],\n",
            "          [ 0.0691, -0.0087,  0.1031]],\n",
            "\n",
            "         [[-0.0108, -0.0744, -0.1825],\n",
            "          [-0.1128, -0.0333, -0.0817],\n",
            "          [-0.2242, -0.1185, -0.1004]],\n",
            "\n",
            "         [[ 0.0202,  0.0264,  0.0434],\n",
            "          [ 0.0832,  0.1142,  0.0723],\n",
            "          [ 0.1773,  0.1284,  0.1625]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1769,  0.2252,  0.1689],\n",
            "          [ 0.0937,  0.1125,  0.1273],\n",
            "          [ 0.1095,  0.2016,  0.2309]],\n",
            "\n",
            "         [[ 0.0347,  0.0463,  0.0546],\n",
            "          [ 0.0382,  0.0596,  0.0162],\n",
            "          [ 0.0223,  0.0422,  0.1292]],\n",
            "\n",
            "         [[ 0.0265, -0.0637,  0.0073],\n",
            "          [-0.0894, -0.0121, -0.0221],\n",
            "          [-0.0209,  0.0079,  0.0152]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0096, -0.0147,  0.0300],\n",
            "          [ 0.0172,  0.0270,  0.0758],\n",
            "          [ 0.0847,  0.0836, -0.0197]],\n",
            "\n",
            "         [[-0.0402, -0.0695, -0.0407],\n",
            "          [ 0.0202, -0.0944,  0.0292],\n",
            "          [-0.0521,  0.0113,  0.0300]],\n",
            "\n",
            "         [[ 0.0351,  0.0985, -0.0509],\n",
            "          [ 0.0108,  0.0466,  0.0267],\n",
            "          [ 0.0810, -0.0068, -0.0024]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0263, -0.0187, -0.0527],\n",
            "          [-0.1060, -0.0150, -0.1696],\n",
            "          [ 0.0465,  0.0184, -0.0819]],\n",
            "\n",
            "         [[ 0.0746,  0.0070,  0.0854],\n",
            "          [ 0.0553,  0.1180,  0.0774],\n",
            "          [ 0.0917, -0.0300,  0.0534]],\n",
            "\n",
            "         [[ 0.0551,  0.0191, -0.0211],\n",
            "          [ 0.0921,  0.1075,  0.0075],\n",
            "          [-0.0134, -0.0277,  0.0884]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0203, -0.0152, -0.1097],\n",
            "          [-0.1032,  0.0059, -0.1356],\n",
            "          [-0.0553, -0.0203, -0.0579]],\n",
            "\n",
            "         [[-0.1146, -0.0589,  0.0683],\n",
            "          [ 0.0700, -0.0370, -0.0645],\n",
            "          [-0.0264, -0.0064,  0.0380]],\n",
            "\n",
            "         [[ 0.0195,  0.0394, -0.0577],\n",
            "          [ 0.0378, -0.0053, -0.0154],\n",
            "          [-0.0186, -0.0338, -0.0911]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0245, -0.1054, -0.0531],\n",
            "          [ 0.0119, -0.0868, -0.0267],\n",
            "          [ 0.1173, -0.0458, -0.0931]],\n",
            "\n",
            "         [[ 0.0933,  0.0438,  0.2060],\n",
            "          [ 0.1025,  0.0942,  0.1438],\n",
            "          [ 0.1206,  0.1353,  0.1368]],\n",
            "\n",
            "         [[ 0.1378,  0.0898,  0.0895],\n",
            "          [ 0.1413,  0.1583,  0.1155],\n",
            "          [ 0.2075,  0.0376,  0.0486]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0737, -0.0156, -0.1537],\n",
            "          [-0.1229, -0.1292, -0.1396],\n",
            "          [-0.2030, -0.1914, -0.2462]],\n",
            "\n",
            "         [[-0.1202, -0.0356,  0.0956],\n",
            "          [-0.0495, -0.0632,  0.1246],\n",
            "          [ 0.0610,  0.0737,  0.2295]],\n",
            "\n",
            "         [[ 0.1079,  0.0290, -0.1778],\n",
            "          [ 0.0513, -0.0408, -0.1580],\n",
            "          [-0.0508, -0.0262, -0.1382]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.3271, -0.1927, -0.1932],\n",
            "          [-0.2889, -0.3138, -0.1878],\n",
            "          [-0.2836, -0.1960, -0.1940]],\n",
            "\n",
            "         [[-0.0603,  0.0864,  0.1721],\n",
            "          [ 0.0361,  0.0333,  0.0371],\n",
            "          [ 0.0394,  0.0604,  0.0129]],\n",
            "\n",
            "         [[ 0.0459, -0.0121,  0.0244],\n",
            "          [ 0.0753,  0.0639,  0.0492],\n",
            "          [ 0.1349,  0.1349,  0.0650]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1387,  0.1121,  0.1002],\n",
            "          [ 0.1328,  0.0186,  0.0543],\n",
            "          [ 0.0660,  0.0231,  0.1042]],\n",
            "\n",
            "         [[-0.0039, -0.0444, -0.0908],\n",
            "          [-0.0337, -0.0633, -0.0031],\n",
            "          [ 0.0010, -0.0015, -0.0420]],\n",
            "\n",
            "         [[-0.0161, -0.0132, -0.0893],\n",
            "          [ 0.0384, -0.0742,  0.0045],\n",
            "          [-0.0555, -0.0399, -0.0194]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0806,  0.0150,  0.0200],\n",
            "          [ 0.0671, -0.0249,  0.0558],\n",
            "          [-0.0465, -0.0030,  0.0140]],\n",
            "\n",
            "         [[-0.0815, -0.0556, -0.0564],\n",
            "          [ 0.0420, -0.0220,  0.0254],\n",
            "          [ 0.0145,  0.0225,  0.0189]],\n",
            "\n",
            "         [[-0.0765,  0.0219, -0.0742],\n",
            "          [-0.0703,  0.0267,  0.0302],\n",
            "          [ 0.0074, -0.0484,  0.0643]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0614,  0.0439,  0.0331],\n",
            "          [-0.0402,  0.0715,  0.0586],\n",
            "          [ 0.0016,  0.0275,  0.0499]],\n",
            "\n",
            "         [[-0.0728, -0.1012, -0.0659],\n",
            "          [-0.0786, -0.1014, -0.0384],\n",
            "          [ 0.0370, -0.0920, -0.0118]],\n",
            "\n",
            "         [[-0.0253,  0.0311,  0.0718],\n",
            "          [-0.0066,  0.1012,  0.0384],\n",
            "          [ 0.0486,  0.1079,  0.1058]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0057,  0.0275, -0.0218],\n",
            "          [-0.0064, -0.0523,  0.0106],\n",
            "          [ 0.0129, -0.0375,  0.0536]],\n",
            "\n",
            "         [[-0.0052,  0.0461,  0.0426],\n",
            "          [ 0.0449,  0.0114, -0.0089],\n",
            "          [ 0.0562,  0.0596,  0.0407]],\n",
            "\n",
            "         [[-0.0416,  0.0369, -0.0696],\n",
            "          [-0.0575, -0.0383,  0.0389],\n",
            "          [-0.0202, -0.0714, -0.0207]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.1562,  0.1427,  0.1632, -0.0177,  0.0506,  0.0634,  0.0935,  0.0684,\n",
            "        -0.0679,  0.1144,  0.0722,  0.0636,  0.0131,  0.2065, -0.0163,  0.1078,\n",
            "         0.1067,  0.0013, -0.0095,  0.1032,  0.1142,  0.0368, -0.0335, -0.0087],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0512,  0.0471, -0.0413],\n",
            "          [-0.0046, -0.0304,  0.0088],\n",
            "          [-0.1292, -0.0889, -0.1063]],\n",
            "\n",
            "         [[ 0.0342,  0.1109,  0.0525],\n",
            "          [ 0.1134,  0.1139,  0.1335],\n",
            "          [ 0.1097,  0.0450,  0.0081]],\n",
            "\n",
            "         [[-0.0061, -0.1428, -0.1392],\n",
            "          [ 0.0518, -0.1454, -0.0291],\n",
            "          [-0.0052, -0.0465, -0.0976]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0138, -0.0497,  0.0202],\n",
            "          [ 0.0013,  0.0244, -0.0100],\n",
            "          [-0.0913, -0.1195, -0.0305]],\n",
            "\n",
            "         [[ 0.0730,  0.0777,  0.0384],\n",
            "          [ 0.0979,  0.0066, -0.0328],\n",
            "          [ 0.0008, -0.0092, -0.1103]],\n",
            "\n",
            "         [[ 0.0713,  0.0842,  0.1123],\n",
            "          [ 0.1859,  0.1330,  0.0496],\n",
            "          [ 0.0714,  0.0100, -0.0630]]],\n",
            "\n",
            "\n",
            "        [[[-0.0532, -0.0391, -0.1635],\n",
            "          [-0.0363,  0.0119, -0.1273],\n",
            "          [-0.1205, -0.1123, -0.2846]],\n",
            "\n",
            "         [[ 0.1022,  0.0690,  0.1016],\n",
            "          [ 0.0071,  0.0682,  0.0335],\n",
            "          [-0.0164,  0.0605,  0.0593]],\n",
            "\n",
            "         [[ 0.0611, -0.0189, -0.0949],\n",
            "          [-0.0517,  0.0056, -0.0849],\n",
            "          [ 0.0494, -0.0699, -0.0588]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1032, -0.0465, -0.0766],\n",
            "          [-0.0537, -0.1124, -0.0684],\n",
            "          [-0.0868, -0.0769, -0.0681]],\n",
            "\n",
            "         [[-0.0058, -0.0520, -0.0517],\n",
            "          [ 0.0492,  0.0446, -0.0650],\n",
            "          [ 0.0469, -0.0605, -0.0422]],\n",
            "\n",
            "         [[-0.0210,  0.0280,  0.0064],\n",
            "          [ 0.0557, -0.0351,  0.0361],\n",
            "          [-0.0468,  0.0630,  0.0597]]],\n",
            "\n",
            "\n",
            "        [[[-0.0890, -0.0913, -0.0944],\n",
            "          [ 0.0099, -0.0659, -0.0460],\n",
            "          [ 0.0223, -0.0712, -0.1169]],\n",
            "\n",
            "         [[ 0.0720,  0.0533,  0.0609],\n",
            "          [-0.0060, -0.0421,  0.0070],\n",
            "          [ 0.0003, -0.0434,  0.0404]],\n",
            "\n",
            "         [[-0.0465, -0.0286, -0.0680],\n",
            "          [-0.1100, -0.1256, -0.0094],\n",
            "          [-0.0982,  0.0080, -0.1280]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0642,  0.0818,  0.0787],\n",
            "          [-0.0463,  0.0448,  0.0922],\n",
            "          [ 0.0780,  0.0441,  0.0431]],\n",
            "\n",
            "         [[-0.1497, -0.0674, -0.0976],\n",
            "          [-0.0825, -0.0860, -0.0688],\n",
            "          [-0.0622,  0.0037,  0.0201]],\n",
            "\n",
            "         [[ 0.1180,  0.0214,  0.0366],\n",
            "          [-0.0055,  0.0014,  0.1303],\n",
            "          [ 0.1164,  0.0194,  0.0555]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1757,  0.1504,  0.0678],\n",
            "          [ 0.1182,  0.0984,  0.0883],\n",
            "          [ 0.1141,  0.0154,  0.0788]],\n",
            "\n",
            "         [[-0.0097,  0.0378,  0.0579],\n",
            "          [ 0.0099,  0.0581,  0.0113],\n",
            "          [ 0.0276,  0.0364,  0.1039]],\n",
            "\n",
            "         [[-0.0440, -0.1327, -0.1161],\n",
            "          [-0.0862, -0.1018, -0.0731],\n",
            "          [-0.0386, -0.0872, -0.0910]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0840,  0.0195,  0.0025],\n",
            "          [ 0.0389, -0.0078, -0.0817],\n",
            "          [ 0.0066, -0.0962, -0.0117]],\n",
            "\n",
            "         [[-0.1393, -0.1274, -0.1215],\n",
            "          [-0.1111, -0.0837, -0.0335],\n",
            "          [-0.0543, -0.0135,  0.0469]],\n",
            "\n",
            "         [[ 0.0849,  0.0554, -0.0306],\n",
            "          [ 0.0967,  0.0906,  0.0887],\n",
            "          [ 0.1636,  0.0802, -0.0174]]],\n",
            "\n",
            "\n",
            "        [[[-0.0229,  0.0311,  0.1088],\n",
            "          [ 0.0125,  0.1050,  0.0565],\n",
            "          [-0.0259,  0.1028,  0.1169]],\n",
            "\n",
            "         [[ 0.1079,  0.0305,  0.0245],\n",
            "          [ 0.0807,  0.0595,  0.0308],\n",
            "          [-0.0116,  0.0339,  0.0239]],\n",
            "\n",
            "         [[-0.0935, -0.1251, -0.0112],\n",
            "          [-0.1872, -0.0893, -0.0722],\n",
            "          [-0.1387, -0.0967, -0.1314]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0395, -0.0389, -0.0264],\n",
            "          [ 0.0328, -0.0526,  0.0381],\n",
            "          [-0.0604, -0.0044,  0.0302]],\n",
            "\n",
            "         [[-0.0015, -0.0655, -0.0006],\n",
            "          [-0.0663, -0.0820, -0.0531],\n",
            "          [-0.0284, -0.0670,  0.0232]],\n",
            "\n",
            "         [[ 0.0414, -0.0514, -0.0165],\n",
            "          [ 0.0582, -0.0584, -0.0210],\n",
            "          [ 0.0651, -0.0358, -0.0182]]],\n",
            "\n",
            "\n",
            "        [[[-0.0088,  0.0004, -0.0630],\n",
            "          [ 0.0514, -0.0235, -0.0444],\n",
            "          [ 0.0132,  0.0012, -0.0660]],\n",
            "\n",
            "         [[ 0.0775, -0.0019,  0.0589],\n",
            "          [ 0.0202,  0.0014,  0.0108],\n",
            "          [-0.0101,  0.0531,  0.0527]],\n",
            "\n",
            "         [[-0.1626, -0.0504, -0.1157],\n",
            "          [-0.1069, -0.1384, -0.0785],\n",
            "          [-0.0713, -0.1092, -0.0984]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0075,  0.0251,  0.0055],\n",
            "          [-0.0811,  0.0130,  0.0102],\n",
            "          [-0.0334, -0.0345, -0.0150]],\n",
            "\n",
            "         [[-0.0494, -0.0432,  0.0099],\n",
            "          [-0.0054,  0.0380, -0.0348],\n",
            "          [-0.0394,  0.0332,  0.0352]],\n",
            "\n",
            "         [[-0.0738,  0.0119, -0.0050],\n",
            "          [-0.0405, -0.0506,  0.0256],\n",
            "          [-0.0309,  0.0402, -0.0447]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.1299, -0.1087, -0.1172, -0.0071, -0.0446, -0.1139, -0.0653, -0.0968,\n",
            "        -0.1065, -0.0681,  0.0328, -0.0889,  0.0063, -0.0211, -0.0830, -0.1190,\n",
            "        -0.1099, -0.1051,  0.0255, -0.1563, -0.0406, -0.1182, -0.0114, -0.0282],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-1.2716e-01, -4.9651e-02, -1.4511e-01],\n",
            "          [-4.4461e-02, -6.4620e-02, -2.0217e-02],\n",
            "          [-9.0510e-02, -6.6700e-02, -5.6761e-02]],\n",
            "\n",
            "         [[ 7.4670e-02, -2.3176e-02,  2.1446e-02],\n",
            "          [-5.3484e-02,  5.1959e-02,  3.0570e-02],\n",
            "          [ 5.2613e-02,  3.5252e-02, -2.3769e-02]],\n",
            "\n",
            "         [[-4.5203e-02, -1.9178e-02, -9.7871e-02],\n",
            "          [-7.5800e-02, -2.0723e-02, -1.1363e-01],\n",
            "          [-1.3218e-01, -1.1352e-01, -1.3248e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2951e-01, -2.1622e-02, -5.6865e-02],\n",
            "          [-4.4640e-02,  3.9435e-02, -4.9420e-02],\n",
            "          [ 6.6267e-02,  6.7358e-03, -4.4204e-02]],\n",
            "\n",
            "         [[ 8.1114e-02,  4.1954e-02, -2.6186e-02],\n",
            "          [ 2.2394e-02, -1.0038e-01, -6.6595e-02],\n",
            "          [-5.2240e-02, -2.7827e-02,  8.2446e-02]],\n",
            "\n",
            "         [[-3.1444e-02,  1.0728e-02, -5.9758e-04],\n",
            "          [ 5.2995e-02,  6.1292e-03,  1.0820e-02],\n",
            "          [ 6.9065e-02, -1.4372e-02, -2.6829e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2504e-02,  3.5817e-02, -2.2975e-02],\n",
            "          [-9.0842e-02, -2.2855e-02, -2.4666e-02],\n",
            "          [-3.7480e-02, -4.9152e-02, -5.0497e-02]],\n",
            "\n",
            "         [[-1.0110e-01, -9.0574e-02, -4.8456e-02],\n",
            "          [ 1.7677e-02, -3.0941e-02,  7.5936e-02],\n",
            "          [-1.9389e-02, -7.8559e-02,  1.9464e-02]],\n",
            "\n",
            "         [[ 1.5057e-01,  1.4612e-01,  1.1369e-01],\n",
            "          [ 1.7374e-01,  1.0717e-01,  1.1649e-01],\n",
            "          [ 1.6166e-01,  8.5846e-02,  2.2721e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1669e-02,  4.4331e-02,  2.9704e-02],\n",
            "          [ 3.3729e-02,  6.0968e-03, -3.1151e-02],\n",
            "          [-9.6981e-03,  1.0290e-01,  2.3777e-02]],\n",
            "\n",
            "         [[-1.9173e-02, -4.7451e-02, -9.0242e-03],\n",
            "          [ 5.2716e-02,  4.9979e-02,  4.8601e-02],\n",
            "          [-2.1748e-02,  1.9463e-02,  1.3423e-02]],\n",
            "\n",
            "         [[-8.5296e-02, -5.0514e-02,  1.4491e-02],\n",
            "          [-7.6549e-02,  7.9219e-02,  7.0409e-02],\n",
            "          [-1.2483e-02,  7.0253e-02, -2.5005e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.3946e-02, -1.4351e-02, -1.3439e-01],\n",
            "          [ 2.9857e-02, -9.9761e-02, -1.4500e-01],\n",
            "          [ 4.3234e-02, -2.7500e-02, -1.4761e-01]],\n",
            "\n",
            "         [[-2.6586e-02,  5.3699e-02,  4.2433e-02],\n",
            "          [-4.2661e-02,  1.5257e-02,  7.4423e-02],\n",
            "          [ 9.3397e-03,  2.3577e-02,  1.2274e-01]],\n",
            "\n",
            "         [[ 5.0240e-02, -2.1692e-02, -1.2618e-01],\n",
            "          [ 3.7972e-02, -4.9495e-02, -1.0294e-01],\n",
            "          [ 6.2523e-02, -5.9301e-02, -3.6120e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.7639e-02, -1.1169e-01, -6.3615e-02],\n",
            "          [-4.7492e-02,  2.8086e-04, -5.2281e-02],\n",
            "          [-2.7552e-02, -2.0727e-03, -7.9042e-02]],\n",
            "\n",
            "         [[ 7.4239e-02,  6.4802e-02, -1.1672e-02],\n",
            "          [ 5.7825e-02, -2.8099e-03,  5.2605e-02],\n",
            "          [ 7.0338e-02, -1.1609e-03, -1.3359e-02]],\n",
            "\n",
            "         [[ 6.6795e-02,  6.7050e-02,  7.4657e-02],\n",
            "          [ 6.0817e-02, -5.3777e-02,  3.9618e-02],\n",
            "          [ 5.6453e-02, -3.1702e-02,  1.4919e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-9.4815e-05, -3.1184e-02, -4.2034e-02],\n",
            "          [ 3.1870e-02,  1.2434e-01,  1.0896e-01],\n",
            "          [ 1.2709e-01,  2.4677e-01,  1.1394e-01]],\n",
            "\n",
            "         [[-4.8052e-03, -2.2601e-02, -7.3717e-04],\n",
            "          [ 1.1529e-02,  4.9958e-02,  2.5829e-02],\n",
            "          [ 6.2036e-02, -5.2170e-02,  1.2966e-02]],\n",
            "\n",
            "         [[-1.6647e-01, -1.8214e-01, -2.5340e-01],\n",
            "          [-1.5446e-01, -8.6245e-02, -1.7131e-01],\n",
            "          [-1.6903e-01, -1.4715e-01, -1.7343e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.2568e-02, -1.5844e-02, -8.1594e-02],\n",
            "          [-9.1435e-02, -1.8236e-01, -1.2456e-01],\n",
            "          [-1.5998e-01, -1.0232e-01, -1.5689e-01]],\n",
            "\n",
            "         [[ 4.6099e-02,  7.3233e-02,  8.5941e-02],\n",
            "          [ 1.0071e-01,  2.6859e-04,  6.5234e-02],\n",
            "          [ 7.5846e-05,  2.7203e-02,  8.0063e-02]],\n",
            "\n",
            "         [[-6.1599e-03,  9.2025e-02,  1.4216e-02],\n",
            "          [ 4.5322e-02,  1.7180e-03,  1.1001e-01],\n",
            "          [ 2.8285e-02,  1.2754e-01,  7.5197e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9233e-02, -1.4087e-02, -4.5777e-02],\n",
            "          [-1.0492e-02, -5.2724e-02, -3.7366e-02],\n",
            "          [ 5.4604e-02, -5.0239e-02, -8.1065e-02]],\n",
            "\n",
            "         [[-4.2250e-02,  2.0263e-02, -4.9851e-03],\n",
            "          [ 3.8531e-02,  4.0089e-02, -6.7342e-03],\n",
            "          [ 4.7563e-02,  5.8901e-02,  3.7625e-03]],\n",
            "\n",
            "         [[-1.4785e-02, -1.0514e-01, -5.9740e-02],\n",
            "          [-2.1056e-02, -3.2467e-02, -1.5487e-01],\n",
            "          [-1.0232e-01, -4.1580e-02, -6.1547e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2657e-02, -4.6807e-02, -4.3423e-02],\n",
            "          [ 8.0142e-02,  7.4005e-02, -4.7759e-02],\n",
            "          [-4.3331e-02,  4.4530e-02, -4.8333e-02]],\n",
            "\n",
            "         [[-5.9409e-02, -5.0872e-02, -8.8618e-03],\n",
            "          [-1.6870e-02, -5.8223e-02, -6.4537e-02],\n",
            "          [-3.0680e-02, -4.1597e-03, -4.1097e-02]],\n",
            "\n",
            "         [[ 5.3530e-02,  5.3407e-02, -1.2116e-02],\n",
            "          [ 1.8707e-02, -5.9752e-02, -4.2490e-02],\n",
            "          [-1.8602e-02,  2.7370e-02,  6.2257e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.0164e-02, -6.7833e-02, -1.2967e-02],\n",
            "          [-1.5058e-02, -1.1139e-01, -6.8562e-02],\n",
            "          [-7.9790e-02, -4.4695e-02, -1.0957e-01]],\n",
            "\n",
            "         [[ 6.0608e-02, -2.2225e-02,  5.5970e-02],\n",
            "          [-1.4147e-02,  4.1208e-02, -1.9623e-02],\n",
            "          [ 3.0406e-02,  1.8134e-02,  2.0633e-03]],\n",
            "\n",
            "         [[-7.7393e-02, -1.4015e-01, -5.0876e-02],\n",
            "          [-9.0770e-02, -5.7427e-03, -9.8083e-02],\n",
            "          [-1.5054e-01, -1.6986e-01, -4.4233e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4524e-02,  1.8048e-02,  3.2312e-02],\n",
            "          [ 4.7665e-02,  2.0340e-02, -1.6286e-02],\n",
            "          [-6.7028e-02,  1.5281e-02,  2.4652e-02]],\n",
            "\n",
            "         [[-2.2444e-02, -2.0121e-02,  3.2052e-02],\n",
            "          [ 5.9892e-02, -5.0735e-02,  1.7739e-02],\n",
            "          [ 5.7663e-02, -2.2164e-02,  5.9389e-03]],\n",
            "\n",
            "         [[-4.1604e-02, -1.1835e-02,  1.3915e-02],\n",
            "          [-6.9214e-02,  2.1633e-02,  3.3261e-02],\n",
            "          [-6.8774e-02, -6.3281e-02,  2.8638e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.0912,  0.0699,  0.0772, -0.0264,  0.0638, -0.0882,  0.0471, -0.0369,\n",
            "        -0.0195,  0.0437, -0.0116, -0.0508, -0.0642, -0.1198, -0.0391,  0.1055,\n",
            "         0.0690,  0.0040,  0.0528,  0.0377,  0.0075, -0.0577,  0.0303,  0.0396],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.1073, -0.0363, -0.0986],\n",
            "          [-0.1597, -0.0254, -0.0515],\n",
            "          [-0.1005,  0.0736,  0.0281]],\n",
            "\n",
            "         [[ 0.0384, -0.0737, -0.0465],\n",
            "          [-0.0354, -0.0718, -0.1427],\n",
            "          [-0.0442, -0.1322, -0.0781]],\n",
            "\n",
            "         [[ 0.0446,  0.0738, -0.1022],\n",
            "          [ 0.0529,  0.0113, -0.1342],\n",
            "          [-0.0238, -0.0444, -0.0759]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0063,  0.0282, -0.0276],\n",
            "          [-0.0194,  0.0771, -0.0004],\n",
            "          [ 0.0809,  0.1357,  0.0588]],\n",
            "\n",
            "         [[-0.0483, -0.0898, -0.1327],\n",
            "          [-0.0551, -0.1282, -0.1167],\n",
            "          [-0.0958,  0.0169, -0.1459]],\n",
            "\n",
            "         [[-0.1813, -0.0603, -0.0400],\n",
            "          [-0.1680, -0.1337, -0.1154],\n",
            "          [-0.2019, -0.1709,  0.0047]]],\n",
            "\n",
            "\n",
            "        [[[-0.1285, -0.0293, -0.1110],\n",
            "          [-0.0704,  0.0411, -0.0213],\n",
            "          [-0.0671,  0.0017, -0.0349]],\n",
            "\n",
            "         [[-0.0312, -0.0909,  0.1175],\n",
            "          [-0.0577, -0.0360,  0.0124],\n",
            "          [ 0.1018,  0.0977,  0.0033]],\n",
            "\n",
            "         [[ 0.0982,  0.1112,  0.0216],\n",
            "          [ 0.1113,  0.1156,  0.0812],\n",
            "          [ 0.0486,  0.0703,  0.0765]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0897, -0.0811, -0.0307],\n",
            "          [-0.0039,  0.0449, -0.0737],\n",
            "          [ 0.1070,  0.0721, -0.1227]],\n",
            "\n",
            "         [[ 0.1355,  0.1044,  0.0812],\n",
            "          [ 0.1090,  0.0450,  0.0056],\n",
            "          [ 0.0666,  0.0338, -0.1151]],\n",
            "\n",
            "         [[ 0.0961,  0.0732,  0.0135],\n",
            "          [ 0.1109,  0.0460,  0.0170],\n",
            "          [ 0.0126,  0.0623,  0.0937]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0532, -0.0608, -0.0791],\n",
            "          [-0.0015,  0.0428, -0.0528],\n",
            "          [-0.0264, -0.0551, -0.0106]],\n",
            "\n",
            "         [[-0.0497, -0.1539, -0.0175],\n",
            "          [ 0.0745, -0.1013, -0.0296],\n",
            "          [ 0.0412, -0.0887,  0.0647]],\n",
            "\n",
            "         [[-0.0691,  0.0343, -0.0094],\n",
            "          [-0.0987, -0.0398, -0.0952],\n",
            "          [-0.0172, -0.0332,  0.0071]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0695, -0.0973, -0.1616],\n",
            "          [ 0.1051, -0.1072, -0.0911],\n",
            "          [ 0.0456, -0.0247, -0.1245]],\n",
            "\n",
            "         [[ 0.0862,  0.1571,  0.0640],\n",
            "          [ 0.1001,  0.0231,  0.0749],\n",
            "          [ 0.1309,  0.2101,  0.2618]],\n",
            "\n",
            "         [[ 0.1453,  0.0724,  0.0237],\n",
            "          [ 0.1302,  0.1111,  0.0856],\n",
            "          [ 0.1386,  0.0736,  0.1310]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0335,  0.0099, -0.1915],\n",
            "          [ 0.0528,  0.0020, -0.1333],\n",
            "          [-0.0741, -0.0609, -0.0369]],\n",
            "\n",
            "         [[ 0.0507,  0.2232,  0.2230],\n",
            "          [ 0.1015,  0.0670,  0.1715],\n",
            "          [ 0.1744,  0.1794,  0.0825]],\n",
            "\n",
            "         [[ 0.0485, -0.0350, -0.0934],\n",
            "          [ 0.0191, -0.0988, -0.0933],\n",
            "          [-0.0831, -0.0394, -0.0872]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2340, -0.0931, -0.1216],\n",
            "          [-0.2227, -0.1417, -0.1971],\n",
            "          [-0.1166, -0.1961, -0.0247]],\n",
            "\n",
            "         [[-0.0853,  0.0054,  0.0064],\n",
            "          [-0.0759,  0.0038,  0.0079],\n",
            "          [-0.0623,  0.0277,  0.0204]],\n",
            "\n",
            "         [[-0.0216,  0.0284, -0.0480],\n",
            "          [ 0.0266, -0.0303,  0.0650],\n",
            "          [-0.0038,  0.0468,  0.0280]]],\n",
            "\n",
            "\n",
            "        [[[-0.0353, -0.0629, -0.1050],\n",
            "          [-0.0237,  0.0131, -0.0556],\n",
            "          [ 0.0498, -0.0807, -0.0418]],\n",
            "\n",
            "         [[ 0.0008, -0.0194,  0.0510],\n",
            "          [-0.0215,  0.0695,  0.0730],\n",
            "          [-0.0214, -0.0096,  0.0585]],\n",
            "\n",
            "         [[-0.2042, -0.1614, -0.1842],\n",
            "          [-0.1604, -0.1986, -0.1903],\n",
            "          [-0.1214, -0.1741, -0.1498]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0315, -0.0105,  0.0315],\n",
            "          [-0.0426,  0.0478,  0.0707],\n",
            "          [ 0.0593,  0.0131,  0.0049]],\n",
            "\n",
            "         [[ 0.0508, -0.0300, -0.0451],\n",
            "          [ 0.0557, -0.0488, -0.0383],\n",
            "          [ 0.0304,  0.0108,  0.0541]],\n",
            "\n",
            "         [[-0.0159,  0.0140, -0.0511],\n",
            "          [-0.0502, -0.0125,  0.0442],\n",
            "          [-0.0159,  0.0029, -0.0125]]],\n",
            "\n",
            "\n",
            "        [[[-0.1003, -0.0036,  0.0807],\n",
            "          [-0.0617, -0.0832, -0.0015],\n",
            "          [ 0.0171,  0.0070, -0.0262]],\n",
            "\n",
            "         [[ 0.0609, -0.0041,  0.0325],\n",
            "          [ 0.0502, -0.0584, -0.0361],\n",
            "          [ 0.0580,  0.0175,  0.0791]],\n",
            "\n",
            "         [[-0.1438, -0.1308, -0.0839],\n",
            "          [-0.1912, -0.1468, -0.1215],\n",
            "          [-0.1392, -0.1318, -0.0986]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0248,  0.0218, -0.0314],\n",
            "          [-0.0140, -0.0184, -0.0543],\n",
            "          [ 0.0263,  0.0023,  0.0614]],\n",
            "\n",
            "         [[ 0.0239, -0.0004, -0.0643],\n",
            "          [-0.0298,  0.0445,  0.0180],\n",
            "          [-0.0112, -0.0521, -0.0187]],\n",
            "\n",
            "         [[-0.0476, -0.0442, -0.0055],\n",
            "          [-0.0169,  0.0253,  0.0534],\n",
            "          [ 0.0565,  0.0403, -0.0272]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.0150,  0.1341,  0.1417,  0.0168, -0.0293, -0.0338,  0.0342,  0.1893,\n",
            "         0.0240,  0.0329, -0.0767, -0.0594,  0.0064,  0.0019, -0.0893, -0.0044,\n",
            "        -0.0122, -0.0374,  0.0032,  0.0819,  0.0365,  0.1276,  0.0239,  0.0254],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 1.9909e-01,  3.9560e-01,  2.3349e-01,  ...,  1.0617e-01,\n",
            "            2.6704e-01,  2.4301e-01],\n",
            "          [ 3.5363e-01,  5.1104e-01,  2.7256e-01,  ...,  1.4959e-01,\n",
            "            2.3965e-01,  1.8922e-01],\n",
            "          [ 1.9284e-01,  2.2099e-01,  9.4389e-02,  ...,  4.0943e-02,\n",
            "            1.0056e-01,  6.2807e-02],\n",
            "          ...,\n",
            "          [ 1.0556e-01,  1.0444e-01,  2.4082e-02,  ...,  1.7379e-02,\n",
            "            7.3913e-02,  1.3005e-01],\n",
            "          [ 2.7178e-01,  2.6401e-01,  1.2724e-01,  ...,  4.2264e-02,\n",
            "            9.0457e-02,  1.8889e-01],\n",
            "          [ 2.5354e-01,  3.8266e-01,  1.9870e-01,  ...,  9.1674e-02,\n",
            "            1.9094e-01,  1.0339e-01]],\n",
            "\n",
            "         [[-3.3528e-01, -1.5665e-01,  1.4706e-02,  ...,  5.7724e-02,\n",
            "            1.1257e-01, -8.9260e-02],\n",
            "          [-1.2181e-01,  3.5064e-01,  7.1741e-02,  ...,  6.4452e-02,\n",
            "            1.2080e-01,  2.3653e-01],\n",
            "          [-4.7973e-02,  1.1255e-01,  3.7917e-02,  ...,  3.3468e-02,\n",
            "            3.8785e-02,  1.1366e-01],\n",
            "          ...,\n",
            "          [ 5.8002e-02,  1.2388e-01,  6.5405e-02,  ...,  4.8287e-02,\n",
            "            5.6883e-02,  8.3187e-02],\n",
            "          [-2.6887e-01,  2.6751e-01,  1.1720e-01,  ...,  1.0352e-01,\n",
            "            1.4897e-01,  1.2730e-01],\n",
            "          [-3.4941e-01, -6.1247e-02,  7.1740e-02,  ...,  6.6337e-02,\n",
            "            1.3104e-01, -1.3253e-01]],\n",
            "\n",
            "         [[-6.4513e-02, -8.6869e-02,  3.0826e-03,  ..., -8.5353e-02,\n",
            "           -1.8325e-01, -1.1074e-01],\n",
            "          [-1.4931e-01, -2.0619e-01, -6.2977e-03,  ..., -1.1711e-01,\n",
            "           -2.0231e-01, -1.3834e-01],\n",
            "          [-4.9355e-02, -3.5756e-02,  2.6473e-03,  ..., -5.6643e-02,\n",
            "           -1.1172e-01, -6.5466e-02],\n",
            "          ...,\n",
            "          [-1.2186e-01, -1.0234e-01, -5.1923e-02,  ..., -6.8840e-02,\n",
            "           -1.2398e-01, -1.2223e-01],\n",
            "          [-2.7363e-01, -2.2373e-01, -8.0599e-02,  ..., -1.2796e-01,\n",
            "           -2.7728e-01, -3.4989e-01],\n",
            "          [-2.3390e-02, -8.0053e-02, -4.3278e-02,  ..., -7.5231e-02,\n",
            "           -1.5575e-01, -5.1730e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.9351e-03, -1.0852e-02, -5.0628e-03,  ..., -1.7889e-03,\n",
            "           -2.6398e-03, -1.4163e-03],\n",
            "          [-5.7991e-03, -9.9607e-03, -4.1120e-03,  ..., -2.7281e-03,\n",
            "           -4.9717e-03, -4.3630e-03],\n",
            "          [-4.3098e-03, -5.8227e-03, -1.2738e-03,  ..., -7.8658e-04,\n",
            "           -3.5550e-03, -3.0214e-03],\n",
            "          ...,\n",
            "          [-2.9194e-03, -4.0448e-03, -9.1602e-04,  ..., -6.5008e-04,\n",
            "           -3.2932e-03, -2.4634e-03],\n",
            "          [-8.0570e-03, -6.3182e-03, -3.6726e-03,  ..., -2.7377e-03,\n",
            "           -6.2435e-03, -6.4701e-03],\n",
            "          [-3.4153e-02, -1.2717e-02, -5.7413e-03,  ..., -2.6345e-03,\n",
            "           -6.3345e-03, -5.4455e-03]],\n",
            "\n",
            "         [[ 6.9848e-03,  3.3251e-02,  2.9259e-02,  ...,  1.1226e-02,\n",
            "            1.4485e-02,  2.6673e-02],\n",
            "          [ 2.6994e-03,  3.4099e-02,  2.6880e-02,  ...,  1.1624e-02,\n",
            "            1.2724e-02,  2.6425e-02],\n",
            "          [ 2.2749e-02,  2.5300e-02,  1.1554e-02,  ...,  5.4155e-03,\n",
            "            1.0379e-02,  2.0791e-02],\n",
            "          ...,\n",
            "          [ 1.1951e-02,  1.5837e-02,  5.9140e-03,  ...,  3.7523e-03,\n",
            "            8.4934e-03,  1.6998e-02],\n",
            "          [-1.6881e-03,  1.7646e-02,  1.3416e-02,  ...,  8.2324e-03,\n",
            "            6.3242e-03,  1.5372e-02],\n",
            "          [-3.6672e-04,  3.0243e-02,  3.0393e-02,  ...,  1.1748e-02,\n",
            "            2.1506e-02,  2.1971e-02]],\n",
            "\n",
            "         [[ 1.7699e-01,  3.4322e-01,  1.6476e-01,  ...,  1.2151e-01,\n",
            "            2.6907e-01,  3.1599e-01],\n",
            "          [ 1.4612e-01,  6.2682e-01,  3.9851e-01,  ...,  1.6187e-01,\n",
            "            3.0891e-01,  4.3625e-01],\n",
            "          [ 1.0728e-01,  3.4033e-01,  1.9257e-01,  ...,  7.8007e-02,\n",
            "            1.4942e-01,  2.0330e-01],\n",
            "          ...,\n",
            "          [ 2.4933e-01,  2.6730e-01,  1.0809e-01,  ...,  7.5753e-02,\n",
            "            2.2547e-01,  2.7014e-01],\n",
            "          [ 7.7619e-01,  7.3628e-01,  2.8378e-01,  ...,  2.2756e-01,\n",
            "            4.8764e-01,  6.7905e-01],\n",
            "          [ 1.7257e-01,  5.2767e-01,  2.4346e-01,  ...,  1.2869e-01,\n",
            "            1.5811e-01,  3.7867e-01]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 2.9526e-01,  4.2587e-01,  2.7149e-01,  ...,  1.1298e-01,\n",
            "            3.2232e-01,  4.5846e-01],\n",
            "          [ 3.9999e-01,  6.5483e-01,  3.7460e-01,  ...,  2.1367e-01,\n",
            "            5.6738e-01,  6.5104e-01],\n",
            "          [ 2.5467e-01,  3.4570e-01,  1.3720e-01,  ...,  4.9588e-02,\n",
            "            2.1197e-01,  1.6387e-01],\n",
            "          ...,\n",
            "          [ 1.5513e-01,  2.1149e-01,  3.2352e-02,  ...,  6.9683e-03,\n",
            "            1.5274e-01,  2.8855e-01],\n",
            "          [ 4.2913e-01,  5.9613e-01,  3.4930e-01,  ...,  2.6023e-02,\n",
            "            3.9344e-01,  6.2024e-01],\n",
            "          [ 3.6199e-01,  7.4582e-01,  4.2253e-01,  ...,  1.4112e-01,\n",
            "            5.9550e-01,  5.9970e-01]],\n",
            "\n",
            "         [[-4.7690e-02, -6.4023e-03,  4.8003e-03,  ...,  6.3697e-03,\n",
            "            1.1254e-02,  5.4075e-03],\n",
            "          [-5.6373e-03,  2.5724e-02,  1.4157e-02,  ...,  1.2580e-02,\n",
            "            1.7339e-02,  1.6691e-02],\n",
            "          [ 1.8004e-03,  1.5660e-02,  1.3671e-02,  ...,  1.1752e-02,\n",
            "            1.3583e-02,  9.4189e-03],\n",
            "          ...,\n",
            "          [ 6.5697e-03,  1.4640e-02,  1.2876e-02,  ...,  1.0744e-02,\n",
            "            1.2620e-02,  8.8575e-03],\n",
            "          [-3.5816e-03,  2.0957e-02,  1.5821e-02,  ...,  1.3959e-02,\n",
            "            1.5765e-02,  1.1946e-02],\n",
            "          [-1.3398e-02,  2.8570e-03,  6.6290e-03,  ...,  5.8565e-03,\n",
            "            6.1969e-03,  1.3052e-03]],\n",
            "\n",
            "         [[-7.5475e-04, -3.3378e-03, -3.0741e-03,  ..., -3.3322e-03,\n",
            "           -3.3942e-03, -2.7199e-03],\n",
            "          [-6.4434e-03, -1.6157e-02, -8.8833e-03,  ..., -9.3193e-03,\n",
            "           -1.1561e-02, -5.1884e-03],\n",
            "          [-2.5320e-03, -9.4162e-03, -7.5991e-03,  ..., -8.3058e-03,\n",
            "           -9.1172e-03, -5.4142e-03],\n",
            "          ...,\n",
            "          [-6.9028e-03, -1.0792e-02, -8.2330e-03,  ..., -9.3911e-03,\n",
            "           -1.0072e-02, -5.5585e-03],\n",
            "          [-3.0491e-02, -2.4896e-02, -1.1931e-02,  ..., -1.5412e-02,\n",
            "           -2.0001e-02, -8.0716e-03],\n",
            "          [ 7.4671e-03, -4.0570e-03, -3.6976e-03,  ..., -5.4901e-03,\n",
            "           -7.8272e-03, -2.0765e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.4702e-03, -3.6526e-03, -2.7358e-03,  ..., -1.5715e-03,\n",
            "           -2.6329e-03, -6.0095e-03],\n",
            "          [-4.0734e-03, -4.1947e-03, -1.9498e-03,  ..., -6.8867e-04,\n",
            "           -1.3590e-03, -6.8720e-03],\n",
            "          [-2.8483e-03, -2.3257e-03, -1.2390e-03,  ..., -6.2362e-04,\n",
            "           -9.5454e-04, -1.6331e-03],\n",
            "          ...,\n",
            "          [-1.4270e-03, -9.3975e-04, -5.8442e-04,  ..., -3.6188e-04,\n",
            "           -7.4959e-04,  2.2103e-04],\n",
            "          [-5.6396e-03,  2.3130e-05, -1.5899e-03,  ..., -1.2610e-04,\n",
            "           -3.6381e-04,  2.0733e-03],\n",
            "          [-3.2101e-02, -1.2594e-02, -5.6274e-03,  ..., -4.3896e-04,\n",
            "           -1.4058e-03, -1.2522e-02]],\n",
            "\n",
            "         [[ 1.6240e-03,  2.9883e-03,  1.4074e-03,  ...,  9.7140e-04,\n",
            "            1.7737e-03,  2.0044e-03],\n",
            "          [ 1.7372e-03,  4.7081e-03,  1.2610e-03,  ..., -3.4946e-04,\n",
            "            9.2600e-04,  2.5010e-03],\n",
            "          [ 1.0273e-03,  1.7267e-03, -1.2636e-03,  ..., -2.2601e-03,\n",
            "           -9.0337e-04,  1.4137e-03],\n",
            "          ...,\n",
            "          [ 9.9620e-04,  1.4420e-03, -1.5366e-03,  ..., -2.4594e-03,\n",
            "           -1.1756e-03,  1.1416e-03],\n",
            "          [ 1.2993e-03,  4.3354e-03,  1.6679e-03,  ...,  2.3731e-04,\n",
            "            1.0305e-03,  4.4529e-03],\n",
            "          [ 4.1124e-04,  2.2081e-03,  1.3104e-03,  ...,  7.8414e-04,\n",
            "            1.2978e-03,  1.5814e-03]],\n",
            "\n",
            "         [[ 7.0044e-03,  1.3171e-02,  1.1750e-02,  ...,  1.1244e-02,\n",
            "            1.5572e-02,  2.2039e-02],\n",
            "          [ 1.4920e-02,  4.0590e-02,  3.1123e-02,  ...,  2.5234e-02,\n",
            "            3.5151e-02,  3.3176e-02],\n",
            "          [ 1.2757e-02,  2.8016e-02,  1.9866e-02,  ...,  1.7949e-02,\n",
            "            2.5712e-02,  1.9674e-02],\n",
            "          ...,\n",
            "          [ 1.3176e-02,  2.7916e-02,  2.0939e-02,  ...,  1.9609e-02,\n",
            "            2.6820e-02,  2.0331e-02],\n",
            "          [ 1.9062e-02,  4.5366e-02,  3.2159e-02,  ...,  3.0516e-02,\n",
            "            4.4950e-02,  4.4146e-02],\n",
            "          [ 5.2286e-03,  1.3499e-02,  1.1378e-02,  ...,  1.0631e-02,\n",
            "            1.4791e-02,  1.3489e-02]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 3.5862e-02,  2.0544e-01,  1.1851e-01,  ...,  3.8130e-02,\n",
            "            9.2944e-02,  8.4446e-02],\n",
            "          [ 1.7405e-01,  4.3475e-01,  2.3447e-01,  ...,  9.4284e-02,\n",
            "            2.7120e-01,  8.3884e-02],\n",
            "          [ 7.5405e-02,  1.7122e-01,  6.8253e-02,  ...,  2.1401e-02,\n",
            "            1.1638e-01,  3.1799e-02],\n",
            "          ...,\n",
            "          [ 3.0446e-02,  5.6245e-02,  7.2628e-03,  ...,  3.4707e-03,\n",
            "            3.2925e-02,  6.7231e-02],\n",
            "          [ 1.4751e-01,  3.0818e-01,  1.4846e-01,  ...,  1.5748e-02,\n",
            "            1.1822e-01,  7.5323e-02],\n",
            "          [ 1.1528e-01,  3.7377e-01,  1.6795e-01,  ...,  4.6113e-02,\n",
            "            2.1395e-01, -1.9718e-01]],\n",
            "\n",
            "         [[-6.9844e-02,  5.2908e-03,  1.4653e-02,  ...,  1.8732e-02,\n",
            "            3.6264e-02,  1.7093e-02],\n",
            "          [-2.2809e-03,  6.6929e-02,  2.6696e-02,  ...,  2.3267e-02,\n",
            "            3.9235e-02,  3.9714e-02],\n",
            "          [ 6.3068e-03,  3.3513e-02,  2.0170e-02,  ...,  1.7071e-02,\n",
            "            2.3029e-02,  2.1436e-02],\n",
            "          ...,\n",
            "          [ 1.8939e-02,  3.5099e-02,  2.2157e-02,  ...,  1.8246e-02,\n",
            "            2.5609e-02,  2.1332e-02],\n",
            "          [ 1.3388e-02,  5.7490e-02,  3.4295e-02,  ...,  3.0324e-02,\n",
            "            3.9648e-02,  3.6721e-02],\n",
            "          [-2.7016e-02,  1.1655e-02,  1.5735e-02,  ...,  1.4812e-02,\n",
            "            2.0718e-02,  6.4132e-03]],\n",
            "\n",
            "         [[ 6.9696e-03,  1.0178e-02, -9.2952e-04,  ..., -3.4769e-03,\n",
            "            3.2034e-03, -8.9077e-03],\n",
            "          [ 2.9312e-02,  4.5889e-02,  2.6812e-03,  ...,  8.1785e-03,\n",
            "            2.4972e-02,  3.1680e-03],\n",
            "          [ 6.0897e-03,  6.1993e-03, -7.7683e-03,  ..., -3.1635e-03,\n",
            "            4.5609e-03, -3.2907e-03],\n",
            "          ...,\n",
            "          [ 2.5557e-02,  1.5199e-02, -4.3673e-03,  ...,  1.6723e-03,\n",
            "            9.0139e-03, -2.9648e-03],\n",
            "          [ 1.0967e-01,  7.5430e-02,  1.9112e-02,  ...,  3.2534e-02,\n",
            "            6.5189e-02,  3.5388e-02],\n",
            "          [-1.1097e-02,  1.7246e-02,  6.5006e-03,  ...,  1.6462e-02,\n",
            "            3.4174e-02,  8.8534e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.7460e-03,  7.9232e-04,  2.3898e-03,  ...,  4.0103e-03,\n",
            "            6.5739e-03,  7.3273e-03],\n",
            "          [ 6.0790e-03,  1.7307e-03,  3.4372e-03,  ...,  3.4781e-03,\n",
            "            3.6969e-03,  5.2119e-03],\n",
            "          [ 4.0129e-03,  1.9926e-03,  2.6964e-03,  ...,  2.8038e-03,\n",
            "            2.2635e-03,  1.6369e-03],\n",
            "          ...,\n",
            "          [ 3.5883e-03,  2.7869e-03,  2.4166e-03,  ...,  2.6649e-03,\n",
            "            2.3076e-03,  4.6600e-04],\n",
            "          [ 7.6376e-03,  2.1192e-03,  2.5223e-03,  ...,  2.2868e-03,\n",
            "            1.3217e-03, -1.3403e-03],\n",
            "          [ 1.2463e-02,  7.7602e-03,  3.0324e-03,  ...,  9.6858e-04,\n",
            "            3.0648e-04,  7.6389e-03]],\n",
            "\n",
            "         [[ 3.1658e-03,  8.3141e-03,  4.2557e-03,  ...,  1.7231e-03,\n",
            "            3.2140e-03,  6.5723e-03],\n",
            "          [-3.5277e-04,  8.4202e-03,  5.5758e-03,  ...,  1.3904e-03,\n",
            "            2.2058e-03,  6.5461e-03],\n",
            "          [ 1.2918e-03,  2.4856e-03,  8.7320e-05,  ..., -4.1047e-04,\n",
            "            8.7816e-04,  2.5944e-03],\n",
            "          ...,\n",
            "          [ 6.7069e-04, -8.3227e-04, -1.8558e-03,  ..., -1.4524e-03,\n",
            "           -6.8333e-04,  1.8813e-03],\n",
            "          [-9.5553e-04, -3.2484e-03, -2.5369e-03,  ..., -2.7464e-03,\n",
            "           -3.8991e-03,  2.6916e-03],\n",
            "          [ 2.0050e-04,  1.1772e-03,  1.2955e-03,  ...,  3.6533e-04,\n",
            "            2.8727e-03,  3.8682e-03]],\n",
            "\n",
            "         [[ 2.0524e-02,  3.3947e-02,  3.0357e-02,  ...,  2.6102e-02,\n",
            "            5.0866e-02,  7.6464e-02],\n",
            "          [ 3.2768e-02,  1.2278e-01,  7.5986e-02,  ...,  4.8311e-02,\n",
            "            8.2251e-02,  9.9051e-02],\n",
            "          [ 2.6277e-02,  6.0831e-02,  3.2565e-02,  ...,  2.5252e-02,\n",
            "            4.8527e-02,  4.7323e-02],\n",
            "          ...,\n",
            "          [ 2.7559e-02,  5.4738e-02,  2.9498e-02,  ...,  2.7108e-02,\n",
            "            5.1010e-02,  4.9239e-02],\n",
            "          [ 8.4795e-02,  1.2698e-01,  6.5043e-02,  ...,  5.9451e-02,\n",
            "            1.0696e-01,  1.3182e-01],\n",
            "          [ 2.3846e-02,  5.6907e-02,  3.2949e-02,  ...,  2.6958e-02,\n",
            "            4.6065e-02,  4.6958e-02]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0246,  0.0374, -0.0503],\n",
            "          [-0.0694, -0.0679, -0.0638],\n",
            "          [ 0.0604, -0.0221,  0.0039]],\n",
            "\n",
            "         [[-0.1767, -0.0904, -0.2129],\n",
            "          [-0.0041,  0.0755, -0.0127],\n",
            "          [-0.1527, -0.1083, -0.0231]],\n",
            "\n",
            "         [[ 0.1321,  0.0714,  0.1705],\n",
            "          [-0.0023,  0.0417,  0.0837],\n",
            "          [ 0.1146,  0.1558,  0.1337]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1078,  0.0980,  0.0839],\n",
            "          [ 0.0860,  0.1152,  0.0928],\n",
            "          [ 0.0533,  0.1725,  0.2043]],\n",
            "\n",
            "         [[-0.0196,  0.0101,  0.0802],\n",
            "          [ 0.0611, -0.0041, -0.0017],\n",
            "          [ 0.0262, -0.0357,  0.0308]],\n",
            "\n",
            "         [[-0.0699, -0.0331, -0.0783],\n",
            "          [ 0.0288,  0.0493, -0.0803],\n",
            "          [-0.0565, -0.0681, -0.0762]]],\n",
            "\n",
            "\n",
            "        [[[-0.0627, -0.1146,  0.0015],\n",
            "          [-0.0643, -0.2141,  0.0449],\n",
            "          [-0.0131,  0.0008,  0.1638]],\n",
            "\n",
            "         [[ 0.1505,  0.0748,  0.1101],\n",
            "          [ 0.1067,  0.2977,  0.1386],\n",
            "          [ 0.0283,  0.1618,  0.1032]],\n",
            "\n",
            "         [[-0.0908, -0.1122,  0.0073],\n",
            "          [ 0.0277, -0.1514, -0.0931],\n",
            "          [ 0.0586, -0.0261,  0.0617]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0800, -0.2333, -0.1408],\n",
            "          [ 0.1037, -0.1159,  0.0657],\n",
            "          [ 0.2073,  0.1865,  0.3209]],\n",
            "\n",
            "         [[ 0.0267,  0.1124,  0.1333],\n",
            "          [ 0.0567,  0.0749,  0.0848],\n",
            "          [-0.0032,  0.0358,  0.0564]],\n",
            "\n",
            "         [[ 0.0192,  0.0197,  0.0278],\n",
            "          [ 0.0737,  0.1010,  0.0236],\n",
            "          [ 0.1362,  0.1978,  0.0617]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0844,  0.0275,  0.1057],\n",
            "          [ 0.0054, -0.2364, -0.0104],\n",
            "          [ 0.0736, -0.0047,  0.0435]],\n",
            "\n",
            "         [[ 0.1373,  0.0259,  0.0469],\n",
            "          [ 0.0901,  0.2324,  0.1077],\n",
            "          [ 0.1224,  0.0840,  0.1952]],\n",
            "\n",
            "         [[-0.0944, -0.0218, -0.0654],\n",
            "          [-0.1224, -0.0730, -0.1360],\n",
            "          [-0.0689, -0.0575, -0.2046]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0617, -0.0784,  0.1485],\n",
            "          [-0.0846, -0.1398,  0.0590],\n",
            "          [ 0.0747,  0.0141,  0.0689]],\n",
            "\n",
            "         [[ 0.0460,  0.0852,  0.1039],\n",
            "          [ 0.0822,  0.0927,  0.1236],\n",
            "          [ 0.1349,  0.0292,  0.0462]],\n",
            "\n",
            "         [[ 0.0377,  0.0282,  0.0826],\n",
            "          [-0.0168,  0.0424, -0.0122],\n",
            "          [ 0.0749,  0.0805,  0.0989]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1476,  0.1499,  0.2492],\n",
            "          [ 0.0755, -0.0451,  0.2327],\n",
            "          [ 0.1361,  0.2840,  0.0358]],\n",
            "\n",
            "         [[-0.3717, -0.1514, -0.2571],\n",
            "          [-0.0893, -0.0833,  0.0127],\n",
            "          [-0.0260, -0.1302, -0.0014]],\n",
            "\n",
            "         [[ 0.0966,  0.2284,  0.0873],\n",
            "          [ 0.1157,  0.1116,  0.1006],\n",
            "          [ 0.0686,  0.0746,  0.1164]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1764,  0.0388,  0.0580],\n",
            "          [ 0.0142,  0.0826,  0.0386],\n",
            "          [-0.1606, -0.0574, -0.0178]],\n",
            "\n",
            "         [[ 0.0104,  0.0475,  0.0457],\n",
            "          [-0.0054, -0.0572,  0.0103],\n",
            "          [ 0.0294,  0.0093,  0.0278]],\n",
            "\n",
            "         [[ 0.0494, -0.0559, -0.0970],\n",
            "          [-0.0587, -0.0979, -0.1232],\n",
            "          [-0.1797, -0.1031, -0.2140]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0569, -0.0272,  0.1271],\n",
            "          [ 0.1214,  0.0721,  0.0394],\n",
            "          [ 0.2229,  0.1478,  0.1330]],\n",
            "\n",
            "         [[-0.0887, -0.0228, -0.0294],\n",
            "          [-0.1763, -0.0446, -0.0824],\n",
            "          [ 0.0232, -0.0489, -0.1133]],\n",
            "\n",
            "         [[ 0.0627,  0.0296,  0.0023],\n",
            "          [ 0.1087,  0.2507,  0.1769],\n",
            "          [-0.0081,  0.1532,  0.0884]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0394, -0.1631, -0.0158],\n",
            "          [-0.0187, -0.1580, -0.0611],\n",
            "          [ 0.0100, -0.1710,  0.0577]],\n",
            "\n",
            "         [[ 0.0044,  0.0469,  0.0647],\n",
            "          [ 0.0040, -0.0071,  0.0319],\n",
            "          [ 0.0272,  0.1143,  0.0918]],\n",
            "\n",
            "         [[ 0.1137,  0.0875,  0.0543],\n",
            "          [-0.0136,  0.0046, -0.0139],\n",
            "          [ 0.0025, -0.0285, -0.0611]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1453,  0.1376,  0.1824],\n",
            "          [ 0.0945, -0.0162,  0.1708],\n",
            "          [-0.0382, -0.0382, -0.0088]],\n",
            "\n",
            "         [[-0.0411, -0.1156, -0.1689],\n",
            "          [ 0.2463,  0.3614,  0.0754],\n",
            "          [ 0.1455,  0.3804,  0.3188]],\n",
            "\n",
            "         [[-0.0248,  0.0944,  0.0655],\n",
            "          [-0.0054, -0.2136, -0.1443],\n",
            "          [-0.0701, -0.0932, -0.1478]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1168,  0.1022,  0.3271],\n",
            "          [-0.0385, -0.1756,  0.0904],\n",
            "          [-0.1433, -0.2397,  0.0363]],\n",
            "\n",
            "         [[ 0.0984,  0.0646,  0.1356],\n",
            "          [ 0.0472,  0.0083,  0.1289],\n",
            "          [ 0.1894,  0.1443,  0.0832]],\n",
            "\n",
            "         [[ 0.2059,  0.2281,  0.1307],\n",
            "          [-0.0009, -0.1165, -0.1062],\n",
            "          [-0.1233, -0.2294, -0.1042]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.1056,  0.1528,  0.1727],\n",
            "          [ 0.1134,  0.0387,  0.0829],\n",
            "          [ 0.1383,  0.1183,  0.2550]],\n",
            "\n",
            "         [[ 0.0205, -0.0039,  0.0747],\n",
            "          [-0.0790, -0.1410, -0.0695],\n",
            "          [ 0.0288, -0.0042, -0.0722]],\n",
            "\n",
            "         [[ 0.0289, -0.0116,  0.0172],\n",
            "          [ 0.1863,  0.1032,  0.0824],\n",
            "          [ 0.1264,  0.0386,  0.0237]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1787,  0.0061,  0.2111],\n",
            "          [-0.0568, -0.0489, -0.2051],\n",
            "          [-0.0072, -0.2635, -0.2164]],\n",
            "\n",
            "         [[ 0.1765, -0.0032, -0.0376],\n",
            "          [-0.0019,  0.0905,  0.0326],\n",
            "          [ 0.0342,  0.1016, -0.0463]],\n",
            "\n",
            "         [[ 0.0571,  0.0860,  0.1687],\n",
            "          [ 0.1382,  0.0990,  0.1358],\n",
            "          [ 0.0611,  0.0933,  0.1388]]],\n",
            "\n",
            "\n",
            "        [[[-0.0591, -0.0642, -0.1038],\n",
            "          [ 0.1201, -0.0697, -0.0004],\n",
            "          [ 0.1185, -0.0463, -0.0033]],\n",
            "\n",
            "         [[ 0.0555,  0.1907,  0.0525],\n",
            "          [ 0.1640,  0.1567,  0.0475],\n",
            "          [ 0.0501,  0.1675,  0.0976]],\n",
            "\n",
            "         [[-0.0918, -0.0999, -0.0990],\n",
            "          [-0.0766, -0.0301, -0.0784],\n",
            "          [-0.0517, -0.1375, -0.1536]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0336, -0.0325, -0.0406],\n",
            "          [ 0.1028, -0.0255,  0.0434],\n",
            "          [ 0.0698,  0.0624,  0.0738]],\n",
            "\n",
            "         [[ 0.1020,  0.0596,  0.0833],\n",
            "          [ 0.1110,  0.0048,  0.0018],\n",
            "          [ 0.1384,  0.0303, -0.0019]],\n",
            "\n",
            "         [[-0.0023,  0.0831,  0.1464],\n",
            "          [ 0.0620,  0.0632,  0.0430],\n",
            "          [ 0.1326,  0.0477,  0.0377]]],\n",
            "\n",
            "\n",
            "        [[[-0.0251, -0.0248, -0.1066],\n",
            "          [-0.1320, -0.1492, -0.1299],\n",
            "          [-0.1850, -0.1819, -0.0886]],\n",
            "\n",
            "         [[ 0.0194,  0.0340,  0.0808],\n",
            "          [ 0.1698,  0.1082,  0.0788],\n",
            "          [ 0.1565,  0.1416,  0.2154]],\n",
            "\n",
            "         [[-0.0774, -0.0673,  0.0122],\n",
            "          [-0.1464, -0.1263, -0.0632],\n",
            "          [-0.0993, -0.1033, -0.0777]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0517, -0.0928,  0.0662],\n",
            "          [-0.1083, -0.1477,  0.0594],\n",
            "          [-0.0122, -0.0540,  0.1051]],\n",
            "\n",
            "         [[ 0.1127,  0.1363,  0.1495],\n",
            "          [ 0.1810,  0.1048,  0.1510],\n",
            "          [ 0.2042,  0.1292,  0.1298]],\n",
            "\n",
            "         [[ 0.0782,  0.1040,  0.1868],\n",
            "          [ 0.0879, -0.0464,  0.0278],\n",
            "          [ 0.1635,  0.0478,  0.1648]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1455,  0.0428, -0.0510],\n",
            "          [ 0.1437,  0.0398, -0.0799],\n",
            "          [ 0.0895,  0.0960,  0.0065]],\n",
            "\n",
            "         [[-0.1448, -0.0353,  0.0222],\n",
            "          [-0.1231,  0.0159, -0.0067],\n",
            "          [-0.2259, -0.0646, -0.1088]],\n",
            "\n",
            "         [[ 0.2211,  0.0910,  0.0193],\n",
            "          [ 0.1721,  0.0057,  0.0441],\n",
            "          [ 0.2590,  0.1342,  0.1397]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0331, -0.0894, -0.1166],\n",
            "          [ 0.1918,  0.0196, -0.0465],\n",
            "          [ 0.2011,  0.0762,  0.0795]],\n",
            "\n",
            "         [[-0.0725, -0.0454, -0.0777],\n",
            "          [ 0.0020, -0.1127, -0.0800],\n",
            "          [-0.1318, -0.0131, -0.1211]],\n",
            "\n",
            "         [[-0.1322, -0.0450, -0.0659],\n",
            "          [-0.0743, -0.0544, -0.0271],\n",
            "          [-0.0432, -0.0405,  0.0267]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1033,  0.0667,  0.0116],\n",
            "          [ 0.0887,  0.1062,  0.1299],\n",
            "          [ 0.1060,  0.2097,  0.2065]],\n",
            "\n",
            "         [[-0.0221, -0.0405, -0.0654],\n",
            "          [-0.0947, -0.0330, -0.0984],\n",
            "          [-0.0152, -0.0801, -0.0517]],\n",
            "\n",
            "         [[-0.0310,  0.0576, -0.0344],\n",
            "          [ 0.0616,  0.1312,  0.0682],\n",
            "          [ 0.0020,  0.0616,  0.0187]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0253,  0.0743,  0.1196],\n",
            "          [ 0.0343,  0.0619,  0.0310],\n",
            "          [ 0.0849,  0.0287, -0.0385]],\n",
            "\n",
            "         [[ 0.0400, -0.0107,  0.0777],\n",
            "          [ 0.0299,  0.0495,  0.0380],\n",
            "          [-0.0057, -0.0234,  0.0494]],\n",
            "\n",
            "         [[-0.0693,  0.0257, -0.0110],\n",
            "          [-0.0434, -0.1072, -0.0337],\n",
            "          [-0.0557, -0.0053, -0.1324]]],\n",
            "\n",
            "\n",
            "        [[[-0.0533, -0.1024, -0.1437],\n",
            "          [-0.0921, -0.0569, -0.1687],\n",
            "          [-0.1442,  0.0178,  0.1465]],\n",
            "\n",
            "         [[ 0.0780,  0.0951,  0.2063],\n",
            "          [ 0.0248, -0.0225,  0.0235],\n",
            "          [ 0.0379, -0.0476, -0.1551]],\n",
            "\n",
            "         [[-0.1276, -0.0649,  0.0177],\n",
            "          [-0.1973, -0.2376, -0.0966],\n",
            "          [-0.1923, -0.1366,  0.0805]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0638, -0.1118, -0.0366],\n",
            "          [-0.0448, -0.0501, -0.0642],\n",
            "          [-0.0906, -0.1267, -0.0618]],\n",
            "\n",
            "         [[-0.0332,  0.0205,  0.0660],\n",
            "          [-0.0570, -0.0457, -0.0027],\n",
            "          [-0.0026,  0.0526, -0.0309]],\n",
            "\n",
            "         [[ 0.0985,  0.1050,  0.0177],\n",
            "          [ 0.1075,  0.0524,  0.0527],\n",
            "          [ 0.0157, -0.0405,  0.0025]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 3.9300e-02,  7.5168e-02,  8.3144e-02],\n",
            "          [ 4.7655e-02, -5.9253e-02, -2.2775e-02],\n",
            "          [ 5.8776e-02, -1.6608e-01, -7.5639e-02]],\n",
            "\n",
            "         [[-7.1597e-02, -7.2084e-02, -1.4933e-01],\n",
            "          [-1.0918e-01, -1.8136e-02,  3.3906e-02],\n",
            "          [-1.5835e-01,  3.6982e-02,  8.4904e-02]],\n",
            "\n",
            "         [[ 1.9283e-01,  1.8285e-01,  9.5055e-02],\n",
            "          [ 6.3872e-02, -2.8626e-02,  1.2143e-01],\n",
            "          [-7.0505e-03,  5.8824e-02,  1.4458e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.8169e-02, -9.5546e-02, -5.8462e-02],\n",
            "          [-8.6761e-02, -5.4204e-02,  3.1692e-02],\n",
            "          [-8.2497e-02,  7.8341e-02,  1.2701e-01]],\n",
            "\n",
            "         [[ 7.4176e-03,  4.7755e-02,  3.7217e-02],\n",
            "          [ 6.0126e-02,  5.5426e-02,  8.9483e-02],\n",
            "          [-4.9889e-02,  6.1725e-02,  6.5621e-02]],\n",
            "\n",
            "         [[ 1.7067e-02,  8.6601e-02, -2.9314e-02],\n",
            "          [ 1.1461e-01,  1.2030e-01,  6.1865e-02],\n",
            "          [ 5.2290e-02,  1.9222e-02, -3.4178e-03]]],\n",
            "\n",
            "\n",
            "        [[[-6.9748e-02, -1.7120e-01, -1.3255e-01],\n",
            "          [-1.0435e-01, -8.6176e-02, -1.3108e-01],\n",
            "          [-5.6471e-02, -9.2352e-02, -1.7770e-01]],\n",
            "\n",
            "         [[ 7.7519e-02,  1.5201e-02,  1.0063e-01],\n",
            "          [ 7.4103e-02,  5.2092e-02,  7.9167e-03],\n",
            "          [ 3.8481e-02,  1.0768e-01,  5.0178e-02]],\n",
            "\n",
            "         [[-6.8764e-02, -2.5955e-02,  1.5214e-02],\n",
            "          [ 8.7599e-03,  2.3160e-02, -4.6243e-02],\n",
            "          [ 6.2521e-03,  2.4124e-02, -9.5320e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1060e-02,  1.0292e-02, -5.2004e-02],\n",
            "          [-7.7604e-02, -2.6272e-02,  3.5894e-02],\n",
            "          [-1.1271e-03, -2.9856e-02, -1.3098e-01]],\n",
            "\n",
            "         [[-5.3085e-02,  3.3678e-03, -1.5073e-02],\n",
            "          [-2.1690e-02, -2.0938e-03, -5.4272e-03],\n",
            "          [ 4.2992e-02,  2.2192e-02, -3.5809e-02]],\n",
            "\n",
            "         [[-7.3669e-02,  2.3049e-02, -2.1012e-02],\n",
            "          [-6.4234e-02,  5.3836e-02,  3.0030e-02],\n",
            "          [-2.8979e-02,  5.7682e-02, -2.7277e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8237e-02,  7.7510e-02,  6.5878e-02],\n",
            "          [ 4.4842e-02,  1.0399e-01,  1.1123e-01],\n",
            "          [ 6.5765e-02,  8.1583e-02,  9.6979e-02]],\n",
            "\n",
            "         [[-1.5461e-01,  6.0385e-03, -1.5279e-01],\n",
            "          [-1.4210e-01,  2.9262e-02,  4.7653e-02],\n",
            "          [-6.7308e-02, -1.1116e-02,  4.0733e-02]],\n",
            "\n",
            "         [[ 9.3205e-02,  6.6272e-02,  1.1588e-01],\n",
            "          [ 9.1024e-02, -4.1532e-02,  4.1586e-02],\n",
            "          [ 1.3003e-01,  3.3604e-02,  5.7376e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2826e-02, -5.5330e-02,  1.2443e-01],\n",
            "          [-2.7822e-02, -9.3682e-02,  6.9182e-02],\n",
            "          [ 5.0989e-02, -1.6230e-02,  1.1858e-01]],\n",
            "\n",
            "         [[ 4.7856e-02,  1.3099e-02,  2.2870e-02],\n",
            "          [-2.7220e-02,  1.8413e-02,  2.2338e-02],\n",
            "          [ 1.8042e-02, -5.8567e-02,  6.9422e-02]],\n",
            "\n",
            "         [[ 3.4905e-02, -4.7183e-02,  3.3673e-02],\n",
            "          [-2.5257e-02, -4.9054e-02, -5.2720e-02],\n",
            "          [ 6.1125e-02, -6.1054e-02, -3.7478e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.0022e-01, -1.8047e-01, -1.2046e-01],\n",
            "          [-8.3241e-02,  1.8853e-01, -1.0362e-01],\n",
            "          [-5.8737e-02,  4.3345e-02, -6.1485e-02]],\n",
            "\n",
            "         [[ 9.4457e-02,  5.1577e-02,  1.2099e-01],\n",
            "          [ 1.2886e-01, -7.4558e-02, -3.1088e-02],\n",
            "          [ 4.4015e-02, -7.8639e-02, -3.2138e-02]],\n",
            "\n",
            "         [[-1.1257e-01, -5.5411e-02, -1.5283e-01],\n",
            "          [-9.9601e-02,  4.7014e-02, -4.6429e-02],\n",
            "          [-3.0357e-02,  2.9123e-02,  3.5716e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.1817e-01, -3.1942e-01, -2.4640e-01],\n",
            "          [-2.4465e-01, -4.4779e-02,  9.7708e-02],\n",
            "          [-8.2860e-02,  1.3219e-01,  1.0308e-01]],\n",
            "\n",
            "         [[-4.5733e-03, -3.6589e-02, -5.8952e-02],\n",
            "          [-2.4206e-02, -2.9390e-03, -7.2569e-02],\n",
            "          [ 2.7915e-02, -1.6819e-02,  1.5064e-03]],\n",
            "\n",
            "         [[-3.1570e-02, -2.9129e-02, -2.6243e-03],\n",
            "          [-6.8369e-02, -2.7133e-02, -6.0437e-03],\n",
            "          [-6.7703e-02,  2.3510e-02,  6.4454e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0856e-01,  1.0133e-01,  7.8495e-02],\n",
            "          [-8.1181e-03, -3.6319e-01, -8.1131e-02],\n",
            "          [ 1.2879e-01, -4.8336e-03,  1.1955e-01]],\n",
            "\n",
            "         [[ 7.1776e-02,  1.0368e-01, -5.6249e-02],\n",
            "          [-6.1889e-03,  3.0389e-01,  2.0733e-01],\n",
            "          [ 8.2820e-02,  1.3102e-01,  1.3847e-01]],\n",
            "\n",
            "         [[ 4.7135e-02, -8.5894e-02, -2.2989e-02],\n",
            "          [-2.6668e-02, -1.7318e-01, -3.4480e-02],\n",
            "          [-3.6042e-02, -8.2455e-02, -2.5665e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0080e-01, -1.3875e-01,  1.9260e-01],\n",
            "          [-1.5039e-01, -4.0601e-01, -1.4040e-02],\n",
            "          [ 1.3834e-01, -3.2147e-02,  2.7067e-01]],\n",
            "\n",
            "         [[ 1.0464e-01,  2.7237e-04,  8.1068e-02],\n",
            "          [ 8.5878e-03, -3.9204e-03, -2.3694e-02],\n",
            "          [ 5.9513e-02, -4.3068e-03,  5.5305e-02]],\n",
            "\n",
            "         [[ 1.1173e-01, -6.1299e-04, -3.4410e-02],\n",
            "          [ 2.5588e-03, -8.1968e-02,  1.7248e-02],\n",
            "          [ 8.1595e-02, -3.1789e-02,  9.7437e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1229e-03, -9.0921e-02, -2.7527e-02],\n",
            "          [-1.0391e-01, -1.3965e-01, -3.9067e-02],\n",
            "          [-1.3617e-01, -1.7745e-01, -1.8324e-01]],\n",
            "\n",
            "         [[ 2.7722e-01,  1.1225e-01,  1.2961e-01],\n",
            "          [ 1.8605e-01,  1.2137e-01,  1.2959e-01],\n",
            "          [ 1.1040e-01,  1.1746e-01,  2.1184e-02]],\n",
            "\n",
            "         [[-1.3754e-01, -1.4838e-01, -1.5810e-01],\n",
            "          [-8.1473e-02, -9.6094e-02, -1.1028e-01],\n",
            "          [ 4.0582e-03,  5.4951e-02, -5.6728e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.9363e-02, -5.3232e-02, -7.1227e-02],\n",
            "          [-3.5744e-02,  1.1264e-02, -6.2546e-02],\n",
            "          [ 1.7776e-02, -4.9603e-02, -9.2410e-02]],\n",
            "\n",
            "         [[ 3.4927e-02,  4.1096e-03, -2.5454e-02],\n",
            "          [ 3.8501e-02,  7.6573e-03,  7.7963e-02],\n",
            "          [-2.0748e-02,  9.2911e-02,  5.4709e-02]],\n",
            "\n",
            "         [[ 2.1418e-02, -1.1807e-02, -7.8346e-02],\n",
            "          [-1.8590e-02, -4.8561e-02,  1.1467e-02],\n",
            "          [-5.5295e-03, -6.0706e-02, -3.6597e-03]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0353,  0.0246,  0.0208],\n",
            "          [-0.0276, -0.1047,  0.0508],\n",
            "          [ 0.0889,  0.0212,  0.0764]],\n",
            "\n",
            "         [[-0.0669, -0.0355, -0.0871],\n",
            "          [-0.1220, -0.0714, -0.0531],\n",
            "          [-0.2504, -0.0193, -0.1215]],\n",
            "\n",
            "         [[ 0.0292,  0.0994,  0.0743],\n",
            "          [ 0.0800, -0.0676,  0.0109],\n",
            "          [ 0.0160,  0.0956,  0.1593]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0987, -0.1454, -0.0723],\n",
            "          [-0.1989, -0.0280, -0.0418],\n",
            "          [ 0.0155, -0.0578, -0.0631]],\n",
            "\n",
            "         [[ 0.1413,  0.0931,  0.0081],\n",
            "          [ 0.0821,  0.1041,  0.0995],\n",
            "          [ 0.0712,  0.0251,  0.0129]],\n",
            "\n",
            "         [[-0.0503, -0.0448,  0.0714],\n",
            "          [ 0.0263,  0.0932,  0.0518],\n",
            "          [-0.0364, -0.0609, -0.0716]]],\n",
            "\n",
            "\n",
            "        [[[-0.0142,  0.0355,  0.0128],\n",
            "          [ 0.0807, -0.0558,  0.0455],\n",
            "          [ 0.1593,  0.1325,  0.1756]],\n",
            "\n",
            "         [[ 0.0978,  0.0683, -0.0465],\n",
            "          [ 0.1330,  0.1543, -0.0287],\n",
            "          [ 0.0381,  0.0767, -0.0314]],\n",
            "\n",
            "         [[ 0.0290, -0.0748,  0.0040],\n",
            "          [ 0.0218, -0.0595, -0.0169],\n",
            "          [ 0.0388,  0.0719,  0.0033]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0842, -0.0018,  0.0083],\n",
            "          [-0.0107,  0.0137,  0.1416],\n",
            "          [ 0.0985,  0.0955,  0.1913]],\n",
            "\n",
            "         [[ 0.0369,  0.0497,  0.0274],\n",
            "          [ 0.0166,  0.0168,  0.0700],\n",
            "          [ 0.0082, -0.0014, -0.0080]],\n",
            "\n",
            "         [[ 0.0081,  0.0030,  0.1738],\n",
            "          [ 0.0063,  0.0764,  0.1623],\n",
            "          [ 0.0902,  0.0911,  0.2304]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1151,  0.1105,  0.1707],\n",
            "          [ 0.0865, -0.1023,  0.1372],\n",
            "          [ 0.1079,  0.1210,  0.1399]],\n",
            "\n",
            "         [[ 0.0592,  0.0050, -0.0338],\n",
            "          [ 0.0964,  0.0822, -0.0142],\n",
            "          [ 0.0099,  0.0084,  0.0159]],\n",
            "\n",
            "         [[-0.1005, -0.1179, -0.0987],\n",
            "          [-0.0765, -0.0570, -0.0288],\n",
            "          [-0.1154, -0.1042, -0.0381]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1861,  0.0569,  0.1660],\n",
            "          [ 0.1427,  0.0799,  0.1929],\n",
            "          [ 0.1573,  0.0852,  0.2051]],\n",
            "\n",
            "         [[ 0.0885,  0.1406,  0.1141],\n",
            "          [ 0.0632,  0.0816,  0.0368],\n",
            "          [ 0.0287,  0.0160,  0.1223]],\n",
            "\n",
            "         [[ 0.0952,  0.1268,  0.0490],\n",
            "          [ 0.1180,  0.0110,  0.0673],\n",
            "          [ 0.1379,  0.1214,  0.1131]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.1357,  0.1873,  0.1944],\n",
            "          [ 0.1167,  0.1281,  0.2479],\n",
            "          [ 0.0276,  0.2511, -0.0074]],\n",
            "\n",
            "         [[-0.1847, -0.1930, -0.2288],\n",
            "          [-0.1583, -0.1171, -0.1467],\n",
            "          [-0.1446, -0.0594,  0.0078]],\n",
            "\n",
            "         [[ 0.0225,  0.1445,  0.0771],\n",
            "          [ 0.0980,  0.1110,  0.0382],\n",
            "          [ 0.0560,  0.1128, -0.0075]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0283,  0.0676,  0.2017],\n",
            "          [ 0.0716,  0.0978,  0.1281],\n",
            "          [-0.0089,  0.0466,  0.0461]],\n",
            "\n",
            "         [[ 0.0230,  0.0704,  0.0819],\n",
            "          [ 0.0339, -0.0299, -0.0336],\n",
            "          [ 0.0267, -0.0075,  0.0333]],\n",
            "\n",
            "         [[ 0.0400, -0.0550, -0.1012],\n",
            "          [-0.0503, -0.1183, -0.0333],\n",
            "          [-0.1236, -0.0998, -0.0690]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0887,  0.0042,  0.0066],\n",
            "          [ 0.1420,  0.3407,  0.1952],\n",
            "          [ 0.2365,  0.1871,  0.1773]],\n",
            "\n",
            "         [[-0.0441, -0.1810, -0.2104],\n",
            "          [-0.2588, -0.1146, -0.0204],\n",
            "          [-0.1817, -0.1361, -0.0391]],\n",
            "\n",
            "         [[-0.0669,  0.0671, -0.1438],\n",
            "          [ 0.1414,  0.1510, -0.0514],\n",
            "          [ 0.0373,  0.0143,  0.0130]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0224, -0.1518, -0.0605],\n",
            "          [-0.0684, -0.2065,  0.0394],\n",
            "          [-0.0754, -0.0932,  0.0518]],\n",
            "\n",
            "         [[ 0.0022, -0.0136,  0.0045],\n",
            "          [ 0.0100,  0.0460,  0.0793],\n",
            "          [ 0.1166,  0.1082, -0.0183]],\n",
            "\n",
            "         [[ 0.0018, -0.0172,  0.0173],\n",
            "          [ 0.0523, -0.0337, -0.0026],\n",
            "          [ 0.0858, -0.0735,  0.0634]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1575,  0.1051,  0.0694],\n",
            "          [ 0.0644, -0.1144, -0.1234],\n",
            "          [-0.1292, -0.2626, -0.2143]],\n",
            "\n",
            "         [[-0.0726, -0.0106,  0.0261],\n",
            "          [ 0.1070,  0.1095,  0.0507],\n",
            "          [ 0.2051,  0.2095,  0.2200]],\n",
            "\n",
            "         [[ 0.0479,  0.1696,  0.1666],\n",
            "          [-0.0227,  0.0544,  0.0873],\n",
            "          [-0.1823, -0.1428, -0.0611]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1656,  0.0107,  0.0476],\n",
            "          [ 0.0006,  0.0670,  0.1632],\n",
            "          [ 0.0447,  0.0720,  0.1144]],\n",
            "\n",
            "         [[ 0.0102, -0.0152,  0.0623],\n",
            "          [-0.0302,  0.0157,  0.0935],\n",
            "          [ 0.0178, -0.0415,  0.0691]],\n",
            "\n",
            "         [[ 0.0329, -0.0367,  0.0057],\n",
            "          [ 0.0633,  0.0005, -0.0708],\n",
            "          [ 0.0042, -0.0415,  0.0168]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 2.1416e-02,  3.2406e-02, -5.5709e-02],\n",
            "          [-2.0278e-02, -1.0652e-01, -5.7296e-02],\n",
            "          [ 9.1186e-02, -4.5933e-02, -7.2902e-02]],\n",
            "\n",
            "         [[-2.0765e-01, -1.0039e-01, -1.5826e-01],\n",
            "          [-1.4360e-03, -2.4984e-02,  8.4404e-03],\n",
            "          [-1.4618e-01, -1.2450e-02, -8.9819e-02]],\n",
            "\n",
            "         [[ 1.9466e-01,  6.0641e-02,  1.9189e-01],\n",
            "          [ 5.9739e-02,  5.9321e-02,  9.0181e-02],\n",
            "          [ 4.9344e-02,  5.1162e-02,  9.4733e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8092e-01,  1.0238e-01, -1.9802e-02],\n",
            "          [ 5.5406e-02,  1.4319e-01,  1.0632e-01],\n",
            "          [ 1.0020e-01,  1.2180e-01,  1.5155e-01]],\n",
            "\n",
            "         [[ 2.5248e-02,  6.9749e-02,  9.8586e-02],\n",
            "          [ 1.7671e-02,  6.2514e-02,  4.8242e-02],\n",
            "          [-1.4631e-02, -2.2011e-02, -1.9605e-02]],\n",
            "\n",
            "         [[ 3.8196e-03, -6.4659e-02,  1.5123e-02],\n",
            "          [ 2.7114e-02, -3.1223e-02, -7.3036e-02],\n",
            "          [-8.5914e-02, -8.1504e-02, -2.6128e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.7538e-02, -9.2738e-02, -6.9527e-02],\n",
            "          [ 2.6188e-03, -2.0874e-01,  6.6763e-02],\n",
            "          [-6.5302e-03,  4.9883e-02,  1.3363e-01]],\n",
            "\n",
            "         [[ 1.4843e-01,  1.6885e-01,  4.6584e-02],\n",
            "          [ 1.7236e-01,  2.5759e-01,  7.2480e-02],\n",
            "          [ 1.2323e-01,  5.4748e-02,  1.0506e-01]],\n",
            "\n",
            "         [[-3.4392e-02, -7.4570e-03, -9.2249e-02],\n",
            "          [-4.1613e-02, -1.5964e-01, -1.0056e-01],\n",
            "          [-1.0994e-02, -7.5506e-02, -1.7781e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3729e-02, -1.8796e-01, -1.3851e-01],\n",
            "          [ 7.5876e-02, -1.4285e-01,  1.4692e-01],\n",
            "          [ 2.6343e-01,  1.5640e-01,  2.6459e-01]],\n",
            "\n",
            "         [[ 5.7416e-02,  6.8788e-02,  6.3395e-02],\n",
            "          [ 6.5968e-02,  8.5157e-02, -2.5627e-02],\n",
            "          [ 3.8904e-02, -1.5629e-02,  9.5111e-02]],\n",
            "\n",
            "         [[ 4.3579e-02, -5.8844e-03,  5.4935e-02],\n",
            "          [ 1.1386e-01,  2.9796e-02,  2.7549e-02],\n",
            "          [ 1.7081e-01,  1.8259e-01,  6.9094e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.0735e-02,  2.4409e-03,  1.3864e-02],\n",
            "          [ 8.3744e-02, -2.1308e-01, -1.0116e-02],\n",
            "          [ 2.6178e-02,  2.3535e-02,  5.9978e-03]],\n",
            "\n",
            "         [[ 1.1637e-01,  1.0347e-01,  5.8389e-02],\n",
            "          [ 8.2215e-02,  1.9400e-01,  4.9130e-02],\n",
            "          [ 2.0886e-01,  1.7751e-01,  9.2091e-02]],\n",
            "\n",
            "         [[-1.6721e-01,  7.4540e-03, -1.1076e-01],\n",
            "          [-1.1276e-01, -5.4392e-02, -6.7532e-02],\n",
            "          [-1.4588e-01, -1.2891e-01, -1.8179e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2164e-02, -1.5329e-02,  7.0397e-02],\n",
            "          [-3.6910e-02, -1.6935e-01,  1.1500e-02],\n",
            "          [ 3.5481e-02,  7.1184e-02,  1.7647e-01]],\n",
            "\n",
            "         [[ 2.5349e-02,  2.3249e-02,  1.1582e-01],\n",
            "          [ 1.3783e-01,  6.1155e-02,  1.2158e-02],\n",
            "          [ 7.8576e-02,  1.2321e-01,  4.2557e-02]],\n",
            "\n",
            "         [[ 1.2074e-01,  5.5941e-02, -1.2797e-02],\n",
            "          [ 7.0823e-02, -2.3297e-02,  3.6385e-02],\n",
            "          [ 1.4123e-02,  3.5022e-02,  5.8433e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.2637e-01,  8.0329e-02,  3.1882e-01],\n",
            "          [ 1.1160e-01, -2.3810e-04,  2.5451e-01],\n",
            "          [ 9.0775e-02,  2.4022e-01,  1.3645e-01]],\n",
            "\n",
            "         [[-3.3284e-01, -1.1169e-01, -2.5608e-01],\n",
            "          [-1.3746e-01, -2.8909e-02, -3.8297e-02],\n",
            "          [-5.5770e-02, -1.3595e-01,  3.4883e-02]],\n",
            "\n",
            "         [[ 1.9489e-02,  1.9541e-01,  1.8764e-01],\n",
            "          [ 1.5735e-01,  1.1008e-01,  8.2212e-02],\n",
            "          [ 6.5483e-02,  1.8522e-01,  4.4095e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2601e-01,  7.4597e-02,  1.2569e-01],\n",
            "          [-1.8938e-02,  8.3543e-02,  6.4945e-02],\n",
            "          [-1.3022e-01, -9.5542e-02,  1.2454e-02]],\n",
            "\n",
            "         [[ 2.3910e-02, -3.3847e-02,  3.4832e-02],\n",
            "          [-6.5713e-02,  5.2251e-02, -3.2091e-02],\n",
            "          [ 2.7520e-02, -4.1898e-02,  9.8507e-03]],\n",
            "\n",
            "         [[-5.4095e-02, -5.3883e-02, -6.3207e-02],\n",
            "          [-6.3545e-02, -1.4451e-01, -1.2803e-01],\n",
            "          [-2.5342e-01, -1.3656e-01, -1.3118e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 8.7218e-02,  2.4837e-02,  1.1981e-01],\n",
            "          [ 7.5233e-02,  9.8321e-02,  4.6486e-02],\n",
            "          [ 1.2414e-01,  1.1814e-01,  1.7676e-01]],\n",
            "\n",
            "         [[-1.0298e-02, -2.8270e-02, -7.8953e-02],\n",
            "          [-1.1814e-01, -1.0319e-01, -8.7786e-02],\n",
            "          [ 9.5334e-03, -6.0133e-02, -1.3529e-01]],\n",
            "\n",
            "         [[ 7.1340e-02,  8.9241e-02,  5.6924e-02],\n",
            "          [ 1.4768e-01,  2.6585e-01,  1.0138e-01],\n",
            "          [ 1.0903e-01,  1.6725e-01,  8.5413e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.7593e-02, -1.4439e-01,  7.9862e-03],\n",
            "          [-6.8380e-02, -2.3263e-01,  5.5334e-02],\n",
            "          [-6.9867e-02, -1.2197e-01, -3.5726e-02]],\n",
            "\n",
            "         [[ 6.4265e-02,  6.4147e-02,  1.2344e-01],\n",
            "          [ 1.0821e-01,  1.0188e-01,  2.7569e-02],\n",
            "          [ 1.0423e-01,  4.0323e-03, -3.0957e-02]],\n",
            "\n",
            "         [[ 9.1765e-02,  2.9249e-02,  1.0708e-01],\n",
            "          [ 6.1921e-02,  2.4838e-02,  7.3027e-02],\n",
            "          [-8.1371e-03, -3.2336e-02,  5.7087e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2677e-02,  1.2806e-01,  1.6827e-01],\n",
            "          [ 9.4198e-02, -4.8023e-02,  2.0049e-01],\n",
            "          [ 6.2468e-02, -4.2398e-02,  9.0468e-03]],\n",
            "\n",
            "         [[-6.5289e-02, -1.6468e-01, -1.1714e-01],\n",
            "          [ 1.4607e-01,  3.3317e-01,  1.6818e-01],\n",
            "          [ 1.5155e-01,  4.0816e-01,  3.0960e-01]],\n",
            "\n",
            "         [[-7.3376e-02,  2.8207e-02, -1.0117e-03],\n",
            "          [-7.7774e-02, -1.7573e-01, -1.5551e-01],\n",
            "          [ 1.9350e-02, -1.2656e-01, -1.6974e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3094e-01,  6.7201e-02,  3.2747e-01],\n",
            "          [-8.6722e-02, -1.4687e-01,  1.4371e-01],\n",
            "          [-1.1162e-01, -1.3653e-01,  6.4199e-02]],\n",
            "\n",
            "         [[ 7.3062e-02,  4.7758e-02,  1.9871e-01],\n",
            "          [ 1.3004e-01,  8.3395e-02,  1.1841e-01],\n",
            "          [ 2.1794e-01,  1.2641e-01,  1.1380e-01]],\n",
            "\n",
            "         [[ 1.9187e-01,  2.2101e-01,  1.1245e-01],\n",
            "          [ 3.5741e-02, -7.5172e-02,  4.6183e-03],\n",
            "          [-1.3415e-01, -2.6102e-01, -1.3051e-01]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.1347,  0.1757,  0.1021,  0.0568,  0.0896, -0.0203,  0.1651,  0.1586,\n",
            "         0.0772,  0.0752,  0.0400,  0.1598,  0.0424,  0.0734,  0.0475, -0.0031,\n",
            "         0.0416,  0.1795,  0.0831,  0.0293,  0.0525,  0.0774,  0.0636,  0.1813],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.1921,  0.0630,  0.2062],\n",
            "          [ 0.1042,  0.0967,  0.0833],\n",
            "          [ 0.1634,  0.1438,  0.1934]],\n",
            "\n",
            "         [[-0.0017, -0.0794,  0.0134],\n",
            "          [-0.0025, -0.0993, -0.0799],\n",
            "          [-0.0304, -0.0641, -0.0884]],\n",
            "\n",
            "         [[ 0.0568,  0.0065,  0.0715],\n",
            "          [ 0.1751,  0.1825,  0.0556],\n",
            "          [ 0.1267,  0.1211,  0.0208]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1687, -0.0389,  0.2742],\n",
            "          [-0.0715, -0.0273, -0.2275],\n",
            "          [ 0.0163, -0.2564, -0.2223]],\n",
            "\n",
            "         [[ 0.1244,  0.0911,  0.0500],\n",
            "          [ 0.0683,  0.0837,  0.0982],\n",
            "          [ 0.1501,  0.0352,  0.0254]],\n",
            "\n",
            "         [[ 0.0336,  0.1644,  0.1029],\n",
            "          [ 0.1134,  0.1366,  0.0961],\n",
            "          [ 0.0556,  0.0894,  0.1306]]],\n",
            "\n",
            "\n",
            "        [[[-0.0110, -0.1538, -0.0813],\n",
            "          [ 0.0423, -0.1130, -0.0266],\n",
            "          [ 0.0923,  0.0437,  0.0072]],\n",
            "\n",
            "         [[ 0.1297,  0.1055,  0.0616],\n",
            "          [ 0.0429,  0.1692,  0.1203],\n",
            "          [ 0.0073,  0.1531,  0.0487]],\n",
            "\n",
            "         [[-0.0397, -0.0659, -0.1589],\n",
            "          [-0.0442, -0.0436, -0.1377],\n",
            "          [ 0.0531, -0.0446, -0.0904]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0287,  0.0059, -0.1296],\n",
            "          [ 0.1332, -0.0160, -0.0317],\n",
            "          [ 0.0590,  0.1452, -0.0431]],\n",
            "\n",
            "         [[ 0.1027,  0.0689,  0.0079],\n",
            "          [ 0.1113,  0.0724,  0.0829],\n",
            "          [ 0.1468,  0.0822,  0.0398]],\n",
            "\n",
            "         [[ 0.1024,  0.0593,  0.1258],\n",
            "          [ 0.0692,  0.0149,  0.0339],\n",
            "          [ 0.0981,  0.1611,  0.0807]]],\n",
            "\n",
            "\n",
            "        [[[-0.0738, -0.1031, -0.0804],\n",
            "          [-0.0171, -0.1881, -0.0935],\n",
            "          [-0.1177, -0.1782, -0.0344]],\n",
            "\n",
            "         [[ 0.0152,  0.1256,  0.0716],\n",
            "          [ 0.1259,  0.1927,  0.1936],\n",
            "          [ 0.0655,  0.1126,  0.1431]],\n",
            "\n",
            "         [[-0.0669, -0.0465, -0.0226],\n",
            "          [-0.0648, -0.0299, -0.0741],\n",
            "          [-0.0204, -0.0973, -0.0514]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0529, -0.0783,  0.1420],\n",
            "          [-0.1149, -0.2399, -0.0345],\n",
            "          [ 0.0067, -0.0459,  0.0600]],\n",
            "\n",
            "         [[ 0.1087,  0.1399,  0.1116],\n",
            "          [ 0.1677,  0.1109,  0.1247],\n",
            "          [ 0.1801,  0.0691,  0.1185]],\n",
            "\n",
            "         [[ 0.1240,  0.0444,  0.1656],\n",
            "          [ 0.0719,  0.0296,  0.0133],\n",
            "          [ 0.1280,  0.0905,  0.0930]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.2149, -0.0156, -0.1547],\n",
            "          [ 0.0215, -0.0377, -0.1133],\n",
            "          [ 0.1018,  0.0718, -0.0179]],\n",
            "\n",
            "         [[-0.0841, -0.0063, -0.0214],\n",
            "          [-0.1246, -0.0926,  0.0292],\n",
            "          [-0.2401, -0.1729, -0.1589]],\n",
            "\n",
            "         [[ 0.1691,  0.0886,  0.0552],\n",
            "          [ 0.2165,  0.0501,  0.0231],\n",
            "          [ 0.1816,  0.1143,  0.1360]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0193, -0.1848, -0.1113],\n",
            "          [ 0.0681,  0.0387, -0.0593],\n",
            "          [ 0.2140,  0.1535, -0.0120]],\n",
            "\n",
            "         [[-0.0727, -0.0083, -0.0030],\n",
            "          [-0.0607, -0.1072, -0.0382],\n",
            "          [-0.0453, -0.0848, -0.0921]],\n",
            "\n",
            "         [[-0.1325, -0.0334, -0.1239],\n",
            "          [-0.0360, -0.0978,  0.0132],\n",
            "          [-0.0646,  0.0087,  0.0154]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0260, -0.0308,  0.0443],\n",
            "          [ 0.0405,  0.1439,  0.1419],\n",
            "          [ 0.0149,  0.2794,  0.2042]],\n",
            "\n",
            "         [[-0.0340, -0.0496, -0.0479],\n",
            "          [-0.0892, -0.0145, -0.0490],\n",
            "          [-0.0575, -0.1381, -0.0133]],\n",
            "\n",
            "         [[ 0.0134,  0.0413,  0.0752],\n",
            "          [ 0.0317,  0.1232,  0.0962],\n",
            "          [ 0.1063,  0.0320,  0.0122]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1179,  0.0458,  0.1212],\n",
            "          [-0.0215,  0.0398, -0.0127],\n",
            "          [ 0.0156, -0.0100,  0.0062]],\n",
            "\n",
            "         [[-0.0155, -0.0348,  0.0502],\n",
            "          [ 0.0219, -0.0089, -0.0212],\n",
            "          [ 0.0404,  0.0357,  0.0172]],\n",
            "\n",
            "         [[-0.0304, -0.0436,  0.0078],\n",
            "          [-0.0737, -0.0747, -0.0420],\n",
            "          [-0.0455, -0.1284, -0.1147]]],\n",
            "\n",
            "\n",
            "        [[[-0.0171, -0.0631, -0.2280],\n",
            "          [-0.0418,  0.0346, -0.1298],\n",
            "          [-0.1413,  0.0566,  0.1096]],\n",
            "\n",
            "         [[ 0.0421,  0.0416,  0.1634],\n",
            "          [ 0.0645, -0.0439,  0.0152],\n",
            "          [ 0.1017, -0.0613, -0.1175]],\n",
            "\n",
            "         [[-0.1316, -0.0571, -0.0884],\n",
            "          [-0.2349, -0.1717, -0.1307],\n",
            "          [-0.2227, -0.0207,  0.0551]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1534, -0.0771, -0.1410],\n",
            "          [-0.1099, -0.0867, -0.1207],\n",
            "          [-0.0238, -0.1258, -0.0113]],\n",
            "\n",
            "         [[-0.0240, -0.0287,  0.0571],\n",
            "          [-0.0599,  0.0594,  0.0076],\n",
            "          [ 0.0417, -0.0480, -0.0016]],\n",
            "\n",
            "         [[ 0.0242, -0.0256,  0.0383],\n",
            "          [ 0.0785, -0.0371,  0.0205],\n",
            "          [ 0.0008,  0.0182, -0.0034]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.0214, -0.0187,  0.0284,  0.0596, -0.0458, -0.0018, -0.0141, -0.0192,\n",
            "         0.0654, -0.0403,  0.0381, -0.0460,  0.0934,  0.0600,  0.0049, -0.0665,\n",
            "        -0.0671, -0.0048,  0.0766,  0.1217,  0.0467,  0.2513,  0.0271, -0.0387],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 6.8499e-02,  3.7849e-02,  4.8412e-02],\n",
            "          [-1.4649e-02, -1.7819e-02,  6.3114e-04],\n",
            "          [ 5.0637e-02, -9.7214e-02, -5.6680e-02]],\n",
            "\n",
            "         [[-1.3319e-01, -6.0492e-02, -8.4839e-02],\n",
            "          [-1.1053e-01, -1.2811e-02,  2.6616e-02],\n",
            "          [-9.4165e-02, -3.7170e-02,  7.3320e-02]],\n",
            "\n",
            "         [[ 2.0493e-01,  9.6703e-02,  1.2819e-01],\n",
            "          [-5.2106e-02, -1.7894e-02,  1.2034e-01],\n",
            "          [ 8.1965e-02,  3.2256e-02,  6.6793e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3445e-02, -9.2253e-04,  1.6961e-02],\n",
            "          [-4.8698e-02, -3.0020e-03,  2.6995e-02],\n",
            "          [-1.6167e-02,  9.1170e-02,  1.1861e-01]],\n",
            "\n",
            "         [[-1.1874e-02,  1.3091e-02, -5.3958e-03],\n",
            "          [ 9.2130e-02,  1.7041e-02,  2.0494e-02],\n",
            "          [ 5.5351e-02,  9.6065e-02,  3.8568e-02]],\n",
            "\n",
            "         [[-9.7468e-04,  2.0055e-02,  6.7833e-02],\n",
            "          [ 4.7929e-02,  7.2694e-02,  7.7984e-02],\n",
            "          [ 4.3479e-02,  6.2464e-02,  6.9983e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.7392e-01, -5.8576e-02, -1.3468e-01],\n",
            "          [-2.1026e-02, -1.3092e-01, -1.4740e-01],\n",
            "          [-1.5480e-01, -7.4605e-02, -1.4415e-01]],\n",
            "\n",
            "         [[ 1.5373e-01,  9.0770e-02,  5.8844e-02],\n",
            "          [ 9.2566e-02, -1.3632e-02, -2.6324e-02],\n",
            "          [ 1.2852e-01, -1.1242e-02,  8.2442e-02]],\n",
            "\n",
            "         [[-9.4456e-02, -6.1462e-03,  2.4798e-02],\n",
            "          [-3.6988e-02,  2.2758e-02, -2.0931e-02],\n",
            "          [-3.5375e-02,  6.2663e-03, -1.4312e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8070e-02,  2.6359e-02, -3.5987e-02],\n",
            "          [ 3.0254e-02,  4.7570e-02, -1.7440e-02],\n",
            "          [-6.2254e-02,  2.4385e-02, -1.1185e-01]],\n",
            "\n",
            "         [[ 4.1089e-02, -3.1154e-02, -8.0545e-03],\n",
            "          [ 5.6259e-02, -5.1105e-02,  9.1981e-04],\n",
            "          [ 3.0845e-02,  2.0585e-02,  1.0152e-02]],\n",
            "\n",
            "         [[-1.0696e-02, -4.6786e-02, -5.9257e-02],\n",
            "          [-1.5135e-02,  2.0145e-02, -5.3849e-02],\n",
            "          [ 3.5410e-02, -9.3880e-03,  1.3272e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 6.0014e-02,  8.8282e-02,  3.5695e-02],\n",
            "          [ 1.4991e-02,  5.6429e-02,  9.7861e-02],\n",
            "          [ 2.9235e-03,  1.7422e-03,  1.1689e-01]],\n",
            "\n",
            "         [[-1.1153e-01, -4.1618e-02, -9.4295e-02],\n",
            "          [-1.2269e-02,  3.2137e-04,  2.8031e-02],\n",
            "          [-1.4318e-02,  3.6698e-02,  7.1451e-02]],\n",
            "\n",
            "         [[ 1.6388e-01,  6.7921e-02,  1.3568e-01],\n",
            "          [ 1.1455e-01, -1.0965e-02,  5.1460e-02],\n",
            "          [ 5.4247e-02,  9.2263e-02, -5.1787e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6212e-02, -5.9500e-02,  9.2652e-02],\n",
            "          [-4.5686e-04, -8.1203e-02,  3.7691e-02],\n",
            "          [ 3.8533e-03, -6.2817e-02,  7.7395e-02]],\n",
            "\n",
            "         [[ 5.7546e-02,  4.4203e-02, -3.3600e-02],\n",
            "          [ 1.8276e-03,  4.9004e-02, -5.1532e-02],\n",
            "          [ 4.6361e-02,  3.5260e-02, -2.3309e-02]],\n",
            "\n",
            "         [[-4.7219e-02,  6.4549e-02,  7.0050e-02],\n",
            "          [ 6.1162e-02, -5.2843e-02, -6.1494e-02],\n",
            "          [ 3.6184e-02, -6.4333e-02, -1.1148e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.0481e-01, -1.0310e-01, -1.3126e-01],\n",
            "          [-7.2634e-02,  1.9109e-01, -2.8929e-02],\n",
            "          [-1.2344e-01, -1.6628e-02, -1.2007e-01]],\n",
            "\n",
            "         [[ 7.9755e-02,  7.0991e-02,  1.0724e-01],\n",
            "          [ 1.1924e-01, -1.1432e-01,  1.6484e-02],\n",
            "          [ 6.6084e-02,  2.3537e-03,  5.6651e-02]],\n",
            "\n",
            "         [[-1.1870e-01, -9.9157e-02, -1.2077e-01],\n",
            "          [-2.2442e-02,  2.7427e-02,  1.7060e-02],\n",
            "          [-7.9539e-02, -3.2959e-02,  3.1763e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.4088e-01, -2.0920e-01, -1.8821e-01],\n",
            "          [-2.0131e-01, -4.9440e-02,  7.7074e-02],\n",
            "          [-1.3297e-01,  7.0288e-02,  1.5361e-01]],\n",
            "\n",
            "         [[ 2.8183e-02, -9.3112e-02, -5.9830e-02],\n",
            "          [ 4.0086e-02, -7.0604e-02, -4.2936e-02],\n",
            "          [ 5.0498e-02,  5.2901e-02, -1.3244e-02]],\n",
            "\n",
            "         [[-8.6239e-02, -7.6355e-02,  3.7566e-02],\n",
            "          [-6.3415e-02, -1.0954e-01, -4.1228e-02],\n",
            "          [ 4.0998e-02, -3.8293e-02,  3.9021e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0405e-01, -6.1942e-03,  1.4094e-01],\n",
            "          [-2.5974e-02, -2.6132e-01, -8.9823e-02],\n",
            "          [ 5.0347e-02, -2.4485e-02,  1.1499e-01]],\n",
            "\n",
            "         [[-1.9866e-02,  6.6841e-02, -9.6940e-03],\n",
            "          [ 1.9131e-02,  2.6775e-01,  1.9458e-01],\n",
            "          [ 6.7876e-02,  2.1201e-01,  6.5697e-02]],\n",
            "\n",
            "         [[-6.0854e-02, -6.5777e-02, -8.3910e-03],\n",
            "          [-2.6899e-02, -1.8787e-01, -1.2907e-01],\n",
            "          [ 1.9574e-02, -1.3991e-01,  7.2285e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.6225e-03, -8.9830e-02,  2.0781e-01],\n",
            "          [-1.8713e-01, -4.5477e-01, -2.0412e-02],\n",
            "          [ 1.3722e-01,  3.9802e-03,  2.1369e-01]],\n",
            "\n",
            "         [[ 9.2354e-02, -9.7815e-04,  1.4256e-01],\n",
            "          [-3.2751e-02, -9.3729e-05, -3.1706e-02],\n",
            "          [ 2.4809e-02, -5.6860e-02,  3.9663e-02]],\n",
            "\n",
            "         [[ 4.3061e-02,  4.4308e-02,  3.5533e-02],\n",
            "          [ 3.3542e-02, -9.1634e-02, -9.3516e-02],\n",
            "          [ 1.2798e-01,  1.2415e-02,  7.9660e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6010e-02, -3.2878e-02, -6.9211e-02],\n",
            "          [-1.0523e-01, -5.9087e-02, -7.8585e-02],\n",
            "          [-1.3913e-01, -1.4156e-01, -2.0361e-01]],\n",
            "\n",
            "         [[ 2.4146e-01,  9.7775e-02,  2.4596e-01],\n",
            "          [ 1.8594e-01,  1.0057e-01,  1.0020e-01],\n",
            "          [ 1.1938e-01,  9.0119e-02,  9.6470e-02]],\n",
            "\n",
            "         [[-6.8857e-02, -8.7147e-02, -1.9946e-01],\n",
            "          [-8.8827e-02, -3.4525e-03, -8.9216e-02],\n",
            "          [-1.1036e-01, -4.5314e-02,  1.2428e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.4359e-02, -5.2448e-02, -2.9461e-02],\n",
            "          [ 2.2094e-03,  5.2295e-02, -1.1219e-01],\n",
            "          [-1.3278e-02, -2.1037e-02, -1.0214e-01]],\n",
            "\n",
            "         [[ 1.4098e-02,  1.0012e-02, -5.4871e-02],\n",
            "          [ 4.8639e-02,  1.7818e-02,  4.0376e-02],\n",
            "          [ 8.1852e-02,  6.3143e-02, -3.8661e-02]],\n",
            "\n",
            "         [[-9.7874e-02, -6.9687e-02, -3.3503e-02],\n",
            "          [ 3.6371e-02,  7.1784e-03, -7.9217e-02],\n",
            "          [-6.3418e-02, -6.0318e-02, -4.8771e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.0940,  0.0116, -0.0695,  0.0625, -0.0532,  0.0388,  0.0640,  0.0165,\n",
            "         0.0036, -0.0379,  0.0916,  0.0947, -0.0863, -0.0623, -0.0313,  0.0970,\n",
            "        -0.0009, -0.0349, -0.0706, -0.0420, -0.0152,  0.0318,  0.0764,  0.0515],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0552, -0.0455, -0.0683],\n",
            "          [-0.0473, -0.1664,  0.0587],\n",
            "          [ 0.0021, -0.0579,  0.0253]],\n",
            "\n",
            "         [[ 0.0029, -0.1100, -0.0818],\n",
            "          [-0.0605, -0.0207, -0.0089],\n",
            "          [-0.2552, -0.0974, -0.1060]],\n",
            "\n",
            "         [[-0.0132,  0.0006, -0.0269],\n",
            "          [ 0.0824,  0.0302,  0.0836],\n",
            "          [ 0.1325,  0.0867,  0.1796]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2128, -0.1129, -0.0593],\n",
            "          [-0.1246, -0.0008,  0.0126],\n",
            "          [ 0.0126, -0.0592, -0.0978]],\n",
            "\n",
            "         [[ 0.0475,  0.0904,  0.0390],\n",
            "          [ 0.0645,  0.0784,  0.0879],\n",
            "          [-0.0379, -0.0203, -0.0147]],\n",
            "\n",
            "         [[ 0.0057,  0.0062,  0.0455],\n",
            "          [-0.0541, -0.0031,  0.0288],\n",
            "          [-0.0213, -0.0106, -0.0407]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0309, -0.0198, -0.0264],\n",
            "          [ 0.0915, -0.0587,  0.1528],\n",
            "          [ 0.1123,  0.0565,  0.1012]],\n",
            "\n",
            "         [[ 0.0562,  0.0696,  0.0069],\n",
            "          [ 0.0933,  0.1746, -0.0364],\n",
            "          [-0.0249,  0.0342,  0.0600]],\n",
            "\n",
            "         [[ 0.0514, -0.0554, -0.1160],\n",
            "          [ 0.0934, -0.1300, -0.0384],\n",
            "          [ 0.0407, -0.0550, -0.0408]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1006, -0.0555,  0.0586],\n",
            "          [ 0.0362, -0.0308,  0.1258],\n",
            "          [ 0.1784,  0.1205,  0.1278]],\n",
            "\n",
            "         [[ 0.0884,  0.0443,  0.0843],\n",
            "          [ 0.0585,  0.0567, -0.0074],\n",
            "          [ 0.0092,  0.0599,  0.0414]],\n",
            "\n",
            "         [[ 0.0607,  0.0043,  0.1468],\n",
            "          [ 0.0311,  0.1082,  0.0710],\n",
            "          [ 0.1086,  0.0950,  0.1568]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1613,  0.0674,  0.1740],\n",
            "          [ 0.1197, -0.0268,  0.0884],\n",
            "          [ 0.1099,  0.1114,  0.1647]],\n",
            "\n",
            "         [[ 0.0711,  0.0395,  0.0177],\n",
            "          [ 0.0655,  0.1376,  0.0257],\n",
            "          [ 0.0843,  0.0746, -0.0202]],\n",
            "\n",
            "         [[-0.1238, -0.0043, -0.0963],\n",
            "          [-0.0250, -0.1613, -0.0121],\n",
            "          [-0.0634, -0.1038, -0.0355]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1338,  0.1154,  0.1521],\n",
            "          [ 0.0882,  0.0324,  0.1701],\n",
            "          [ 0.1639,  0.0978,  0.1694]],\n",
            "\n",
            "         [[ 0.1599,  0.1167,  0.0767],\n",
            "          [ 0.0718,  0.0431,  0.0681],\n",
            "          [ 0.0596,  0.0540,  0.0644]],\n",
            "\n",
            "         [[ 0.1288,  0.0201,  0.0421],\n",
            "          [ 0.1360,  0.0616,  0.1072],\n",
            "          [ 0.1167,  0.1532,  0.0953]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0957,  0.1391,  0.1406],\n",
            "          [ 0.0746,  0.1248,  0.1371],\n",
            "          [ 0.0146,  0.2282,  0.0331]],\n",
            "\n",
            "         [[-0.2704, -0.1973, -0.1146],\n",
            "          [-0.2091, -0.0728, -0.0639],\n",
            "          [-0.1296, -0.1724, -0.0252]],\n",
            "\n",
            "         [[ 0.1202,  0.1194,  0.1728],\n",
            "          [ 0.0443,  0.0453,  0.0657],\n",
            "          [ 0.1277,  0.0485,  0.0275]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0078,  0.0220,  0.1173],\n",
            "          [-0.0519,  0.0262,  0.0683],\n",
            "          [-0.0301,  0.0637,  0.0955]],\n",
            "\n",
            "         [[-0.0101, -0.0012, -0.0308],\n",
            "          [-0.0538,  0.0309, -0.0137],\n",
            "          [-0.0284,  0.0509,  0.0502]],\n",
            "\n",
            "         [[-0.0387, -0.0791,  0.0022],\n",
            "          [-0.0922, -0.0825,  0.0045],\n",
            "          [-0.0662, -0.0131,  0.0016]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0190, -0.0668, -0.0237],\n",
            "          [ 0.0974,  0.3081,  0.1736],\n",
            "          [ 0.1408,  0.1938,  0.1435]],\n",
            "\n",
            "         [[-0.0277, -0.1038, -0.1728],\n",
            "          [-0.1832, -0.1751, -0.0896],\n",
            "          [-0.1397, -0.1300, -0.0965]],\n",
            "\n",
            "         [[-0.0680,  0.0063, -0.0453],\n",
            "          [ 0.0440,  0.1563,  0.0251],\n",
            "          [-0.0598,  0.0197,  0.0016]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0637, -0.1992, -0.0426],\n",
            "          [-0.1944, -0.1705,  0.0501],\n",
            "          [-0.0843, -0.1070,  0.0405]],\n",
            "\n",
            "         [[-0.0370,  0.0248,  0.0504],\n",
            "          [-0.0259,  0.0725,  0.0471],\n",
            "          [ 0.1195,  0.0079, -0.0533]],\n",
            "\n",
            "         [[ 0.0970, -0.0121,  0.0903],\n",
            "          [-0.0276,  0.0525, -0.0411],\n",
            "          [-0.0334, -0.0520,  0.0479]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1343,  0.1488,  0.0629],\n",
            "          [ 0.0222, -0.1120, -0.0875],\n",
            "          [-0.1223, -0.2434, -0.1087]],\n",
            "\n",
            "         [[-0.0678, -0.1137, -0.0887],\n",
            "          [ 0.0086,  0.1192,  0.0994],\n",
            "          [ 0.2272,  0.2352,  0.2604]],\n",
            "\n",
            "         [[ 0.0687,  0.1224,  0.1661],\n",
            "          [-0.0456,  0.0078,  0.1146],\n",
            "          [-0.1666, -0.2038, -0.0114]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.2225, -0.0475,  0.0650],\n",
            "          [-0.0107,  0.1184,  0.0803],\n",
            "          [ 0.0286,  0.0670,  0.0024]],\n",
            "\n",
            "         [[-0.0017, -0.0045,  0.0860],\n",
            "          [-0.0139,  0.0420, -0.0228],\n",
            "          [ 0.0216,  0.0225,  0.0608]],\n",
            "\n",
            "         [[-0.0470,  0.0115, -0.0497],\n",
            "          [ 0.0476,  0.0332, -0.0014],\n",
            "          [ 0.0327,  0.0279, -0.0648]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.1671,  0.1072,  0.0697,  0.1155,  0.0685,  0.1667,  0.0699,  0.2025,\n",
            "         0.0891,  0.0193, -0.0094,  0.0550,  0.0109, -0.0236,  0.0640, -0.0045,\n",
            "         0.0974,  0.1336,  0.0711,  0.1595,  0.0055,  0.0675,  0.0207,  0.1699],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-7.3634e-01, -4.9765e-01, -1.0532e-01,  ..., -1.0089e-01,\n",
            "           -7.0368e-02, -2.1708e-01],\n",
            "          [-5.9570e-01, -2.3149e-01, -1.2832e-01,  ..., -1.2958e-01,\n",
            "           -1.5902e-01, -2.3000e-01],\n",
            "          [-3.4129e-02, -1.1707e-01, -4.9658e-02,  ..., -4.6748e-02,\n",
            "           -6.9300e-02, -1.8779e-02],\n",
            "          ...,\n",
            "          [-2.6711e-02, -1.0491e-01, -2.0103e-02,  ..., -1.3623e-02,\n",
            "           -4.2429e-02,  2.3905e-02],\n",
            "          [ 2.0780e-01, -1.1500e-01, -1.2754e-02,  ..., -5.0103e-03,\n",
            "           -4.8859e-02,  3.3008e-02],\n",
            "          [-5.0849e-01, -3.1228e-01, -1.3182e-01,  ..., -1.2820e-01,\n",
            "           -1.8071e-01, -2.8507e-01]],\n",
            "\n",
            "         [[-3.0254e-01,  1.1531e-01,  1.9120e-02,  ...,  1.2812e-02,\n",
            "            4.4745e-02, -1.3350e-01],\n",
            "          [ 8.3202e-02,  5.1211e-02,  2.8360e-02,  ...,  3.0654e-02,\n",
            "            5.7286e-02, -9.2286e-04],\n",
            "          [-2.6217e-02,  2.5962e-02,  9.0914e-03,  ...,  1.3364e-02,\n",
            "            3.5985e-02, -1.9153e-02],\n",
            "          ...,\n",
            "          [-3.4699e-02,  2.8596e-02,  1.1436e-02,  ...,  1.3418e-02,\n",
            "            3.5520e-02, -1.5514e-02],\n",
            "          [ 6.5775e-03,  5.8305e-02,  3.2832e-02,  ...,  3.2179e-02,\n",
            "            5.6426e-02,  3.7242e-02],\n",
            "          [-2.2415e-01,  7.4714e-02,  2.4052e-02,  ...,  2.5260e-02,\n",
            "            9.1916e-02, -1.2295e-01]],\n",
            "\n",
            "         [[ 1.7840e+00,  1.9935e+00,  4.5302e-01,  ...,  5.8625e-01,\n",
            "            9.8478e-01,  1.6316e+00],\n",
            "          [ 1.5538e+00,  1.2691e+00,  1.2402e-01,  ...,  2.1434e-01,\n",
            "            4.2583e-01,  1.1528e+00],\n",
            "          [ 4.5331e-01,  1.0865e-01, -2.6881e-02,  ...,  8.9923e-03,\n",
            "            2.1859e-01,  4.5555e-01],\n",
            "          ...,\n",
            "          [ 4.6120e-01,  1.8792e-01,  1.3344e-02,  ...,  1.7574e-02,\n",
            "            2.1749e-01,  4.5380e-01],\n",
            "          [ 1.7917e+00,  4.4661e-01,  1.5029e-01,  ...,  1.8528e-01,\n",
            "            2.9456e-01,  1.4085e+00],\n",
            "          [ 1.8748e+00,  2.0440e+00,  3.3560e-01,  ...,  4.6157e-01,\n",
            "            9.9395e-01,  1.7089e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0068e+00,  4.9451e-01,  3.7019e-01,  ...,  3.8600e-01,\n",
            "            7.7867e-01,  1.4724e+00],\n",
            "          [ 4.7258e-01,  2.8696e-01,  1.8412e-01,  ...,  1.8971e-01,\n",
            "            3.0523e-01,  6.0776e-01],\n",
            "          [ 3.1307e-01,  1.8181e-01,  6.7840e-02,  ...,  8.5789e-02,\n",
            "            2.0747e-01,  3.5775e-01],\n",
            "          ...,\n",
            "          [ 3.1585e-01,  1.8680e-01,  8.7692e-02,  ...,  1.0078e-01,\n",
            "            2.1117e-01,  3.6832e-01],\n",
            "          [ 5.1170e-01,  3.0482e-01,  2.0845e-01,  ...,  2.1226e-01,\n",
            "            3.0988e-01,  7.9615e-01],\n",
            "          [ 8.1691e-01,  4.0720e-01,  3.0717e-01,  ...,  3.2297e-01,\n",
            "            6.2645e-01,  1.3289e+00]],\n",
            "\n",
            "         [[ 4.5931e-04, -5.6674e-02, -1.8183e-02,  ..., -1.1838e-02,\n",
            "           -1.1899e-02,  1.4971e-02],\n",
            "          [ 6.5946e-02, -3.5667e-03,  1.4183e-02,  ...,  1.7298e-02,\n",
            "            1.6616e-02,  4.5151e-02],\n",
            "          [ 3.1802e-02, -1.9656e-03,  1.2311e-02,  ...,  1.4944e-02,\n",
            "            1.4062e-02,  1.4372e-02],\n",
            "          ...,\n",
            "          [ 2.7601e-02, -7.0085e-03,  7.3944e-03,  ...,  1.0161e-02,\n",
            "            8.9014e-03,  2.3320e-03],\n",
            "          [ 1.7227e-02, -2.3420e-02, -3.8380e-03,  ..., -8.4797e-04,\n",
            "           -4.8335e-03, -4.2749e-02],\n",
            "          [ 3.8359e-02, -3.1968e-02,  3.3310e-03,  ...,  6.7952e-03,\n",
            "            2.7703e-03,  3.5594e-02]],\n",
            "\n",
            "         [[ 1.2213e-01,  3.3026e-02,  3.0544e-02,  ...,  3.0841e-02,\n",
            "            3.0301e-02,  1.3286e-01],\n",
            "          [ 3.2197e-02, -1.5984e-03,  1.1432e-02,  ...,  1.1927e-02,\n",
            "            4.0781e-03,  1.9200e-02],\n",
            "          [ 2.9316e-02,  8.0212e-03,  1.7802e-02,  ...,  1.7578e-02,\n",
            "            1.0383e-02,  1.7761e-02],\n",
            "          ...,\n",
            "          [ 2.9905e-02,  7.3352e-03,  1.6698e-02,  ...,  1.6227e-02,\n",
            "            9.1197e-03,  1.6786e-02],\n",
            "          [ 3.5033e-02, -3.0393e-03,  8.5246e-03,  ...,  8.2449e-03,\n",
            "            5.2762e-04,  1.0643e-02],\n",
            "          [ 7.8855e-02,  3.3543e-02,  2.4399e-02,  ...,  2.2262e-02,\n",
            "            1.4845e-02,  1.0796e-01]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 2.0979e-01,  1.9026e-01,  1.3523e-01],\n",
            "          [ 1.9156e-01,  1.7523e-01,  7.8210e-02],\n",
            "          [ 4.7637e-02,  1.0999e-01,  5.5238e-02]],\n",
            "\n",
            "         [[-9.6251e-02, -3.0010e-02, -1.1748e-02],\n",
            "          [-1.1771e-01, -1.9108e-01, -5.3328e-02],\n",
            "          [-1.2808e-01,  8.4427e-04, -7.0350e-02]],\n",
            "\n",
            "         [[ 1.0284e-01,  9.0903e-02,  8.1677e-02],\n",
            "          [ 6.2774e-02,  6.1095e-02,  9.8125e-04],\n",
            "          [ 9.8209e-02,  3.8946e-02, -4.5116e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5561e-01, -6.9608e-02, -2.3144e-03],\n",
            "          [-1.5773e-01, -1.4963e-01, -1.3660e-01],\n",
            "          [-8.2758e-02, -2.9819e-02, -7.4314e-02]],\n",
            "\n",
            "         [[ 6.2894e-03,  1.0045e-01,  4.0829e-02],\n",
            "          [ 8.0532e-02,  5.4002e-02,  9.7313e-03],\n",
            "          [ 7.4757e-02,  3.6549e-02,  1.2915e-01]],\n",
            "\n",
            "         [[-1.3105e-01,  1.4154e-02,  1.1654e-01],\n",
            "          [-2.6636e-02, -1.5656e-01, -9.6420e-03],\n",
            "          [ 5.0307e-02,  8.6811e-03,  4.2698e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 9.1112e-02,  1.2125e-01,  1.4486e-01],\n",
            "          [ 1.3331e-01,  1.7243e-01,  1.9226e-01],\n",
            "          [ 5.7829e-02,  1.6892e-01,  1.0449e-01]],\n",
            "\n",
            "         [[-1.2776e-01, -2.6317e-02, -7.5079e-02],\n",
            "          [-1.5375e-01, -1.3084e-01, -4.7558e-02],\n",
            "          [-1.1089e-01, -3.6858e-02, -1.0954e-01]],\n",
            "\n",
            "         [[ 1.2756e-01,  1.9109e-02,  1.0858e-01],\n",
            "          [ 1.4085e-01,  4.8937e-02,  5.0829e-02],\n",
            "          [ 4.2079e-02,  6.0196e-02,  1.1744e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.6761e-02, -2.4351e-02, -1.1642e-01],\n",
            "          [ 6.1591e-03, -9.5102e-02, -2.5395e-02],\n",
            "          [-4.6083e-02, -1.0849e-01, -1.4806e-01]],\n",
            "\n",
            "         [[ 1.8247e-02,  1.0529e-01,  4.0350e-02],\n",
            "          [ 1.5912e-01,  5.2972e-02,  6.8767e-02],\n",
            "          [ 1.2593e-01,  8.9232e-02,  5.3311e-02]],\n",
            "\n",
            "         [[-1.0884e-01, -9.5682e-03, -5.3881e-02],\n",
            "          [-8.5779e-02, -1.5243e-01, -8.6003e-02],\n",
            "          [-4.4335e-02, -3.2064e-02, -9.3945e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2357e-02,  1.4029e-01,  7.8025e-02],\n",
            "          [ 1.1278e-01,  1.5828e-01,  1.2202e-01],\n",
            "          [ 1.5595e-01,  8.5337e-02,  1.0603e-01]],\n",
            "\n",
            "         [[-1.0251e-01,  3.5196e-02,  3.6170e-02],\n",
            "          [-3.5003e-03, -2.5092e-02, -2.7049e-02],\n",
            "          [-1.6316e-02, -2.4345e-02,  4.1954e-03]],\n",
            "\n",
            "         [[ 1.4383e-02, -2.5408e-02, -1.3759e-03],\n",
            "          [ 2.3437e-02, -2.4810e-02,  5.0216e-02],\n",
            "          [-2.9246e-02,  4.6768e-02,  1.3309e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2527e-02,  5.9727e-03, -5.6397e-02],\n",
            "          [-1.9333e-02, -2.0426e-01, -1.9230e-01],\n",
            "          [-7.1063e-02, -2.4920e-01, -1.4639e-01]],\n",
            "\n",
            "         [[-7.1655e-02, -8.5645e-03, -2.9483e-02],\n",
            "          [-3.7879e-02,  1.0895e-01,  6.0481e-02],\n",
            "          [-1.4736e-02,  3.3060e-02,  6.9851e-02]],\n",
            "\n",
            "         [[ 3.0572e-02, -1.8316e-02,  2.8150e-02],\n",
            "          [-2.9315e-02,  4.3801e-02,  3.4823e-02],\n",
            "          [-6.2318e-02, -1.5401e-02,  8.3387e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 6.0001e-03,  2.2832e-02, -4.5472e-02],\n",
            "          [-2.9740e-02, -6.2250e-02,  9.5622e-05],\n",
            "          [ 2.3890e-02,  5.3821e-02,  8.4405e-02]],\n",
            "\n",
            "         [[ 2.2228e-01,  1.5540e-01,  2.2893e-01],\n",
            "          [ 5.6708e-02,  1.4144e-01,  2.0777e-01],\n",
            "          [ 8.0052e-02,  3.6832e-02,  1.2248e-01]],\n",
            "\n",
            "         [[-7.6647e-02, -9.4767e-02, -9.1776e-02],\n",
            "          [-1.4564e-01, -1.3351e-01, -1.3499e-01],\n",
            "          [-2.1852e-02, -1.5898e-02, -5.7138e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.8891e-02, -6.1645e-02, -8.0919e-02],\n",
            "          [-6.5095e-02, -4.1064e-02,  9.7280e-02],\n",
            "          [-5.0798e-02,  3.2976e-02, -2.9913e-02]],\n",
            "\n",
            "         [[ 1.0768e-02,  6.1935e-02,  7.6447e-03],\n",
            "          [ 7.3392e-02,  6.6070e-02, -1.3906e-01],\n",
            "          [-5.2583e-02,  4.9309e-02, -1.1382e-02]],\n",
            "\n",
            "         [[ 8.0581e-02,  5.9784e-02,  8.9831e-02],\n",
            "          [ 5.7591e-02,  1.5044e-01,  1.1546e-01],\n",
            "          [-3.7576e-03,  1.4495e-01,  1.5239e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0558e-01,  1.0755e-01,  8.3914e-02],\n",
            "          [ 7.2029e-02,  1.2076e-01,  1.0092e-01],\n",
            "          [ 1.0679e-01,  1.8748e-01,  1.3605e-01]],\n",
            "\n",
            "         [[-1.1460e-01, -4.1780e-02, -5.5703e-02],\n",
            "          [-6.4005e-02, -1.3393e-01, -5.9066e-02],\n",
            "          [-9.5636e-02, -6.8745e-02, -9.7287e-02]],\n",
            "\n",
            "         [[ 6.4109e-02,  5.8213e-02,  1.3984e-01],\n",
            "          [ 1.2728e-02,  3.7172e-02,  1.5086e-01],\n",
            "          [ 5.0355e-02,  1.3360e-01,  1.5680e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.3059e-02, -1.3716e-01, -1.0924e-01],\n",
            "          [-4.9891e-02, -6.7442e-02, -7.7601e-02],\n",
            "          [-1.7688e-01, -1.6241e-01, -9.2862e-02]],\n",
            "\n",
            "         [[ 2.0894e-02,  1.1498e-01,  1.0755e-01],\n",
            "          [ 1.4105e-01,  1.6048e-01,  7.3922e-02],\n",
            "          [ 6.5325e-02,  1.3718e-01,  1.1752e-01]],\n",
            "\n",
            "         [[-1.3813e-01, -7.8296e-02, -5.4632e-02],\n",
            "          [-7.9750e-02, -1.5071e-01, -6.7209e-02],\n",
            "          [-1.2166e-01, -1.0901e-01, -1.1423e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7314e-02,  5.6546e-02,  1.1300e-01],\n",
            "          [ 7.2617e-02,  1.7115e-01,  1.9344e-01],\n",
            "          [ 1.1988e-01,  1.3752e-01,  1.3331e-01]],\n",
            "\n",
            "         [[-1.1068e-01, -1.2855e-01, -8.6374e-02],\n",
            "          [-5.0479e-02, -1.3256e-01, -4.0185e-02],\n",
            "          [-7.3291e-02, -3.1907e-02, -1.5258e-01]],\n",
            "\n",
            "         [[ 1.3871e-01,  1.2581e-01,  7.6631e-02],\n",
            "          [ 5.8281e-02,  7.2235e-02,  1.1056e-01],\n",
            "          [ 1.0081e-01,  6.6829e-02,  7.1736e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.8342e-02, -1.0026e-01, -1.4144e-01],\n",
            "          [-2.4937e-02, -5.2019e-02, -1.6241e-01],\n",
            "          [-7.2856e-02, -8.4043e-02, -1.5713e-01]],\n",
            "\n",
            "         [[ 6.6473e-02,  1.5568e-01,  1.1840e-01],\n",
            "          [ 1.5654e-01,  1.8575e-01,  3.7247e-02],\n",
            "          [ 1.2513e-01,  9.3166e-02,  1.2376e-01]],\n",
            "\n",
            "         [[-4.8942e-02, -7.5664e-02, -1.8759e-01],\n",
            "          [-5.3410e-02, -1.1819e-01, -9.5292e-02],\n",
            "          [-1.0262e-01, -1.1839e-01, -1.1750e-01]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 4.6119e-02,  3.4124e-02, -6.9197e-03],\n",
            "          [ 3.6226e-02,  1.6490e-02, -5.0483e-02],\n",
            "          [ 4.4001e-03, -5.6060e-02,  4.6602e-03]],\n",
            "\n",
            "         [[ 4.8632e-02,  2.3710e-02, -4.1837e-02],\n",
            "          [ 4.3840e-02,  1.3784e-02, -2.4120e-02],\n",
            "          [-2.4657e-02, -5.7507e-02, -1.1037e-02]],\n",
            "\n",
            "         [[ 2.7323e-03,  6.7414e-02,  1.5903e-02],\n",
            "          [ 1.6167e-02,  5.0134e-02, -8.3832e-03],\n",
            "          [ 6.2562e-02,  4.9665e-02, -1.1036e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4853e-02,  3.3784e-02, -2.4320e-02],\n",
            "          [-6.3365e-02,  2.2054e-02,  4.3373e-02],\n",
            "          [-5.6638e-02,  2.6365e-02,  1.2988e-02]],\n",
            "\n",
            "         [[-3.3670e-02,  1.9643e-02,  5.3743e-02],\n",
            "          [-3.1253e-02,  3.3843e-03,  7.7986e-03],\n",
            "          [-6.7803e-03, -5.0814e-02, -3.0399e-02]],\n",
            "\n",
            "         [[ 2.0754e-02, -3.3579e-02, -4.3476e-02],\n",
            "          [-8.8286e-03,  5.6870e-02,  6.2025e-02],\n",
            "          [ 6.0874e-02, -6.1972e-02, -4.4630e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.6754e-02,  6.2167e-02,  6.8014e-02],\n",
            "          [-4.0361e-02,  2.5792e-02,  3.2856e-02],\n",
            "          [-5.8795e-02, -4.9901e-02,  5.2520e-02]],\n",
            "\n",
            "         [[ 4.1302e-02, -6.7922e-02, -3.4553e-02],\n",
            "          [-6.7265e-02, -6.2890e-02,  5.2095e-02],\n",
            "          [-2.3862e-02,  2.0729e-02, -2.5054e-02]],\n",
            "\n",
            "         [[ 4.6701e-02,  2.5504e-02, -1.5844e-02],\n",
            "          [ 6.4932e-02, -5.8852e-02,  7.1915e-03],\n",
            "          [-3.3273e-02, -6.5900e-02, -2.1798e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3571e-02, -4.2049e-02, -5.2752e-02],\n",
            "          [ 4.0117e-02,  2.9052e-02,  3.0768e-02],\n",
            "          [ 1.3458e-02, -2.1053e-03,  2.1741e-02]],\n",
            "\n",
            "         [[ 7.4811e-03, -6.7968e-02,  9.5902e-03],\n",
            "          [ 2.7094e-02, -6.7638e-02,  5.4054e-02],\n",
            "          [ 2.0352e-03, -6.1347e-02, -1.8780e-02]],\n",
            "\n",
            "         [[ 1.4005e-02,  5.4831e-03, -6.6123e-02],\n",
            "          [ 1.6549e-02,  7.3598e-03, -5.2686e-02],\n",
            "          [ 1.9559e-02, -3.1037e-02, -4.9232e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 6.4164e-02,  2.6219e-03,  1.4891e-02],\n",
            "          [ 6.5520e-02, -1.8036e-02,  5.7077e-02],\n",
            "          [-4.5540e-02,  3.3902e-02, -4.8550e-02]],\n",
            "\n",
            "         [[ 4.6207e-02,  2.8678e-02, -5.0826e-02],\n",
            "          [-6.8747e-03,  2.8665e-02,  6.7326e-02],\n",
            "          [-3.1411e-02, -2.1408e-02,  3.6312e-02]],\n",
            "\n",
            "         [[ 6.3697e-02, -5.5362e-02,  3.9855e-03],\n",
            "          [-2.7080e-02, -3.9056e-02, -5.9429e-02],\n",
            "          [ 4.3450e-02,  2.7762e-02, -4.2720e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.6520e-02, -8.8180e-03, -1.6164e-02],\n",
            "          [ 6.7852e-03,  3.1429e-02, -1.2726e-02],\n",
            "          [ 3.3806e-02, -4.7083e-02, -6.1249e-02]],\n",
            "\n",
            "         [[ 4.1924e-02, -1.5408e-02,  4.9014e-02],\n",
            "          [-4.5328e-02, -4.0154e-02, -7.1070e-03],\n",
            "          [ 7.5495e-03,  3.6954e-02,  5.5583e-02]],\n",
            "\n",
            "         [[-4.2419e-02,  6.5527e-02,  5.4957e-02],\n",
            "          [-5.9704e-02, -4.3477e-02,  2.0040e-02],\n",
            "          [ 1.3645e-02,  2.4394e-02,  5.6649e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.5866e-02,  1.7717e-02, -5.6557e-03],\n",
            "          [-5.1256e-02, -4.6478e-02,  3.1255e-02],\n",
            "          [ 5.5254e-02, -4.8105e-02, -4.9004e-02]],\n",
            "\n",
            "         [[ 2.4167e-04,  3.8333e-02,  1.9771e-02],\n",
            "          [-2.7424e-02,  1.0127e-02, -1.7006e-02],\n",
            "          [ 5.0365e-02,  6.3061e-02, -9.7503e-03]],\n",
            "\n",
            "         [[-4.1688e-02, -3.0467e-02,  2.7296e-02],\n",
            "          [-5.9911e-02, -2.0809e-02, -5.0381e-02],\n",
            "          [ 1.7230e-02,  1.9669e-02, -5.1346e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.5526e-02, -4.9622e-02, -4.7578e-02],\n",
            "          [-5.2133e-02, -1.9393e-02, -5.2647e-02],\n",
            "          [ 4.2325e-02, -6.1976e-02,  5.3614e-02]],\n",
            "\n",
            "         [[ 6.3437e-03, -3.0804e-02,  2.1518e-02],\n",
            "          [ 2.6912e-02, -5.3053e-02, -4.2815e-03],\n",
            "          [ 5.0797e-02,  4.4695e-02, -2.4236e-03]],\n",
            "\n",
            "         [[ 4.2389e-02, -3.7887e-02, -1.1612e-02],\n",
            "          [ 3.9873e-02, -3.2297e-02, -4.8789e-02],\n",
            "          [ 3.2690e-02, -3.2810e-02,  5.6549e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.4586e-02, -3.8949e-02, -4.8132e-02],\n",
            "          [ 2.4913e-02, -2.3935e-02,  3.9959e-02],\n",
            "          [ 2.3738e-02, -4.0381e-02,  5.2702e-02]],\n",
            "\n",
            "         [[-4.2836e-02,  5.3528e-02,  3.0944e-02],\n",
            "          [-3.5663e-02, -5.2542e-02,  2.6444e-02],\n",
            "          [ 1.5317e-02, -3.4623e-02,  4.8666e-03]],\n",
            "\n",
            "         [[ 5.8835e-02, -4.4324e-02, -2.0527e-02],\n",
            "          [ 7.7323e-03,  1.0487e-02, -1.8528e-02],\n",
            "          [-1.1290e-02,  1.5252e-02, -2.0165e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7862e-02,  2.8930e-02, -3.5859e-02],\n",
            "          [-3.9549e-02,  1.0815e-02, -3.6160e-02],\n",
            "          [ 1.6437e-02,  5.7353e-02, -5.2386e-02]],\n",
            "\n",
            "         [[ 3.3203e-02,  1.7066e-02, -5.2572e-02],\n",
            "          [-4.2313e-02, -3.2856e-02,  1.0406e-02],\n",
            "          [ 5.1155e-02, -1.0050e-02, -2.4535e-02]],\n",
            "\n",
            "         [[ 5.9132e-02, -2.1921e-02, -8.8384e-03],\n",
            "          [-2.9468e-02,  5.6722e-02, -3.4330e-03],\n",
            "          [-2.4699e-02, -2.6464e-03, -2.9014e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.4215e-02,  2.9279e-02, -9.8988e-03],\n",
            "          [ 6.0033e-02,  6.4291e-05,  2.4328e-02],\n",
            "          [ 6.0837e-02,  5.4875e-02,  2.4815e-02]],\n",
            "\n",
            "         [[-4.3317e-02, -4.0271e-02,  5.1899e-02],\n",
            "          [ 6.3723e-02,  1.8324e-02,  5.3879e-02],\n",
            "          [ 2.2516e-02, -3.5819e-02,  4.0242e-02]],\n",
            "\n",
            "         [[-1.2224e-02, -1.7286e-02, -5.7041e-02],\n",
            "          [-5.8580e-02, -1.5150e-02, -6.7822e-02],\n",
            "          [-6.8474e-03,  5.5867e-02,  5.4341e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.6071e-02, -2.0457e-02,  3.5143e-03],\n",
            "          [-3.6038e-03,  3.5740e-02,  6.6905e-02],\n",
            "          [-2.6485e-02,  3.0378e-02,  2.3741e-02]],\n",
            "\n",
            "         [[-3.4001e-02, -5.3070e-02, -3.1319e-02],\n",
            "          [ 3.1772e-02, -4.3708e-03, -1.9904e-02],\n",
            "          [ 6.5470e-02, -1.5753e-02,  2.0792e-02]],\n",
            "\n",
            "         [[ 4.6187e-02, -6.0595e-02,  3.1447e-02],\n",
            "          [-6.7978e-02,  4.4155e-02, -2.4620e-02],\n",
            "          [ 2.9847e-02, -5.3090e-02, -4.3271e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-5.0124e-02, -3.3851e-02,  8.9910e-02],\n",
            "          [-8.1065e-02, -1.8045e-01, -6.4693e-02],\n",
            "          [ 5.5282e-02, -9.9865e-03,  6.1942e-02]],\n",
            "\n",
            "         [[-3.9202e-02,  5.1616e-02,  8.6989e-03],\n",
            "          [-5.6005e-02,  1.1454e-01,  3.4840e-02],\n",
            "          [-8.1958e-02, -1.3042e-01, -1.3475e-01]],\n",
            "\n",
            "         [[ 1.3013e-01,  1.2021e-01,  5.9125e-02],\n",
            "          [ 1.0115e-01,  1.7664e-02,  5.7979e-02],\n",
            "          [ 1.0393e-01,  4.3664e-03,  3.4358e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7086e-01, -1.1466e-01, -1.9296e-01],\n",
            "          [-9.4426e-02, -2.7561e-02, -1.3231e-02],\n",
            "          [-4.3656e-02,  1.8655e-02, -3.7970e-02]],\n",
            "\n",
            "         [[ 1.0956e-01,  9.6354e-02,  5.0661e-02],\n",
            "          [ 8.8150e-02,  1.3003e-01,  2.2251e-01],\n",
            "          [ 1.2256e-01,  1.1161e-01,  1.3349e-01]],\n",
            "\n",
            "         [[-2.1104e-01, -1.5345e-01, -1.9242e-01],\n",
            "          [-5.7129e-03,  9.0517e-03, -1.3545e-01],\n",
            "          [ 1.1947e-02,  1.6579e-01,  7.2798e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.8873e-02, -9.5723e-02, -6.2248e-02],\n",
            "          [ 5.3317e-02,  9.3418e-02,  1.2010e-02],\n",
            "          [-1.0377e-02,  2.7047e-02, -2.4516e-02]],\n",
            "\n",
            "         [[ 5.4820e-02,  5.9245e-02,  1.3851e-01],\n",
            "          [-5.3025e-02, -6.5285e-02,  6.0909e-03],\n",
            "          [ 1.9043e-01,  7.1829e-02,  1.4561e-01]],\n",
            "\n",
            "         [[-9.6997e-02, -6.8469e-02, -1.9269e-01],\n",
            "          [-2.7747e-03, -4.9792e-02, -1.3111e-01],\n",
            "          [-9.8971e-02, -1.1509e-01, -1.4595e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2448e-01,  2.3245e-01,  1.2326e-01],\n",
            "          [ 1.7007e-01, -4.5999e-02,  4.4502e-02],\n",
            "          [ 1.1633e-01, -3.6077e-02, -1.6692e-02]],\n",
            "\n",
            "         [[-8.9383e-02, -1.9174e-01, -9.5423e-02],\n",
            "          [-9.9095e-02, -1.1993e-01, -8.6836e-02],\n",
            "          [-1.0801e-01, -2.1956e-02,  4.2129e-03]],\n",
            "\n",
            "         [[ 1.7280e-01,  1.2055e-01,  1.9282e-01],\n",
            "          [-3.5122e-02, -4.5262e-02, -2.9838e-02],\n",
            "          [ 3.3660e-03, -9.7987e-02, -4.9987e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1049e-01,  7.7213e-02,  1.2345e-01],\n",
            "          [ 2.0856e-02, -4.0448e-02,  1.0979e-02],\n",
            "          [ 9.5809e-02,  4.9470e-02,  3.3656e-02]],\n",
            "\n",
            "         [[ 1.1924e-02, -2.2951e-02,  5.1001e-02],\n",
            "          [ 5.4689e-02,  8.5892e-02,  1.3361e-01],\n",
            "          [-5.9845e-05,  4.6946e-02,  9.2268e-02]],\n",
            "\n",
            "         [[-5.9602e-02, -8.2534e-02, -9.2300e-02],\n",
            "          [-6.2755e-02, -1.1320e-02, -8.7198e-02],\n",
            "          [-2.9964e-02, -1.2459e-02, -1.7577e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1576e-01, -2.9465e-01, -2.8634e-01],\n",
            "          [-1.2470e-01, -1.2868e-01, -7.2404e-02],\n",
            "          [-1.9106e-01, -3.8075e-02, -1.3585e-01]],\n",
            "\n",
            "         [[ 1.0581e-01,  1.2569e-01,  1.0923e-01],\n",
            "          [ 1.0105e-01,  1.7153e-01,  1.3595e-01],\n",
            "          [ 5.0124e-02,  3.5401e-02,  5.7653e-02]],\n",
            "\n",
            "         [[ 4.3163e-02,  7.0142e-02,  5.9060e-02],\n",
            "          [ 6.9513e-02,  1.0845e-01,  6.7234e-02],\n",
            "          [ 2.5943e-02,  9.5322e-02,  1.0443e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0441e-01, -1.2736e-01, -1.5937e-01],\n",
            "          [ 1.3370e-01,  1.6791e-01, -9.3307e-04],\n",
            "          [-1.2791e-01,  1.5981e-04,  1.6446e-01]],\n",
            "\n",
            "         [[ 3.4307e-02, -5.7379e-03,  2.8417e-02],\n",
            "          [ 1.4263e-01, -1.5204e-01, -8.2140e-02],\n",
            "          [ 2.3542e-02,  9.1168e-02,  1.0196e-01]],\n",
            "\n",
            "         [[-2.5911e-01, -2.2741e-01, -3.4151e-02],\n",
            "          [-1.9460e-01, -1.7042e-02,  1.2087e-02],\n",
            "          [-6.3745e-02, -8.2664e-02, -1.1245e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0948e-01,  1.6210e-01,  1.5828e-01],\n",
            "          [ 1.1285e-01, -1.0158e-01,  8.6929e-02],\n",
            "          [-1.5054e-02, -3.8645e-02,  4.1679e-02]],\n",
            "\n",
            "         [[-3.0819e-01, -8.1450e-02, -3.0549e-02],\n",
            "          [-1.6558e-01, -6.7727e-02,  1.6744e-02],\n",
            "          [-1.2320e-01, -1.2765e-01, -7.8568e-02]],\n",
            "\n",
            "         [[ 2.5929e-01,  3.7758e-01,  1.9883e-01],\n",
            "          [ 2.2642e-01,  7.4524e-02,  1.4333e-02],\n",
            "          [-1.0102e-01, -4.3071e-02,  7.8025e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5946e-02,  6.2310e-02,  1.0983e-01],\n",
            "          [-9.4128e-02, -1.1801e-01,  4.8192e-02],\n",
            "          [-2.7671e-02, -6.0231e-02,  1.0514e-01]],\n",
            "\n",
            "         [[-4.2084e-02, -5.3500e-02, -1.8162e-01],\n",
            "          [-3.6639e-02,  1.1764e-01, -9.6427e-02],\n",
            "          [-1.3513e-01, -2.7006e-02, -1.4854e-01]],\n",
            "\n",
            "         [[ 7.5224e-02,  9.1297e-02,  1.3189e-01],\n",
            "          [-3.4964e-02, -1.0314e-01,  8.1390e-02],\n",
            "          [ 2.7798e-02,  1.9736e-02, -2.8581e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2461e-01, -2.7170e-01, -2.4017e-01],\n",
            "          [-1.2004e-01,  3.2385e-03, -9.8232e-02],\n",
            "          [ 3.0817e-02,  4.1546e-02, -6.3004e-02]],\n",
            "\n",
            "         [[ 5.3420e-02,  1.0632e-01,  9.8554e-02],\n",
            "          [ 1.0919e-01,  2.0142e-01,  1.4032e-01],\n",
            "          [ 1.2112e-01,  6.8921e-02,  3.5207e-02]],\n",
            "\n",
            "         [[-1.8900e-01, -5.8058e-02, -2.0583e-01],\n",
            "          [ 6.3520e-02,  1.2511e-01,  1.4766e-02],\n",
            "          [-3.4026e-02,  2.0057e-01,  9.4221e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2251e-01,  5.3708e-02,  1.5785e-01],\n",
            "          [-9.2467e-02, -5.1906e-02,  2.2695e-02],\n",
            "          [ 2.7964e-02, -2.2728e-02,  6.3924e-02]],\n",
            "\n",
            "         [[-1.0670e-01,  8.8077e-02, -1.3725e-01],\n",
            "          [ 4.1413e-02,  5.8385e-02, -3.2364e-02],\n",
            "          [-5.9268e-02, -6.3284e-02, -1.3321e-01]],\n",
            "\n",
            "         [[ 1.3312e-01,  6.9321e-03,  6.9658e-02],\n",
            "          [ 5.2153e-02, -2.9530e-02,  2.2161e-02],\n",
            "          [-5.0602e-02,  8.0705e-02,  4.0353e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1545e-01, -2.2157e-01, -2.2443e-01],\n",
            "          [-8.3490e-02, -3.4120e-02, -9.7215e-02],\n",
            "          [ 5.8164e-03,  7.3600e-02, -3.9940e-03]],\n",
            "\n",
            "         [[ 1.1559e-01,  2.0262e-01,  9.4123e-02],\n",
            "          [ 2.0836e-01,  1.3809e-01,  1.6376e-01],\n",
            "          [ 1.0189e-01,  1.0095e-01,  2.5611e-02]],\n",
            "\n",
            "         [[-2.4926e-01, -1.4215e-01, -1.6425e-01],\n",
            "          [-4.1016e-02,  2.9863e-02,  9.6643e-02],\n",
            "          [ 1.3609e-01,  1.5537e-01,  1.5850e-01]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 1.7124e-01,  1.2568e-01,  7.3166e-02],\n",
            "          [ 2.0031e-01,  8.7873e-02,  1.6916e-01],\n",
            "          [ 5.6327e-02,  1.2934e-01,  4.7689e-02]],\n",
            "\n",
            "         [[-7.2206e-02, -7.8023e-02,  1.9619e-03],\n",
            "          [-8.9261e-02, -1.5555e-01, -4.5522e-02],\n",
            "          [-5.8461e-02, -1.3589e-02, -9.3628e-03]],\n",
            "\n",
            "         [[ 3.8622e-02,  7.0629e-02,  3.0915e-02],\n",
            "          [ 4.6373e-02,  1.3895e-01,  1.1277e-01],\n",
            "          [-3.7320e-03,  6.7276e-02,  7.7839e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7556e-01, -3.3868e-02,  4.7472e-03],\n",
            "          [-8.6156e-02, -1.2478e-01, -4.8286e-02],\n",
            "          [-1.1275e-01, -1.0593e-01, -4.8076e-02]],\n",
            "\n",
            "         [[ 1.4025e-01,  1.3710e-01,  8.9339e-02],\n",
            "          [ 5.5092e-02,  1.6392e-01,  1.5556e-02],\n",
            "          [ 1.0408e-01,  1.1515e-01,  8.9426e-02]],\n",
            "\n",
            "         [[-5.3137e-02, -4.7741e-02,  1.2282e-01],\n",
            "          [-6.9751e-02, -5.6040e-02,  1.0184e-01],\n",
            "          [ 5.3295e-02,  7.1683e-02,  7.4824e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3362e-01,  1.8420e-01,  1.5689e-01],\n",
            "          [ 1.6253e-01,  1.2341e-01,  1.4401e-01],\n",
            "          [ 1.9452e-01,  2.1840e-01,  2.0969e-01]],\n",
            "\n",
            "         [[-1.0564e-03, -1.3976e-01, -8.9140e-02],\n",
            "          [-7.6051e-02, -1.5346e-01, -1.1458e-01],\n",
            "          [-1.3402e-01, -4.4119e-02, -6.5012e-02]],\n",
            "\n",
            "         [[ 7.8164e-02,  2.1596e-02,  3.1648e-02],\n",
            "          [ 1.5149e-01,  1.0667e-01,  7.7352e-02],\n",
            "          [ 3.7453e-02,  7.4825e-02,  4.9293e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.8773e-03, -9.2105e-02, -1.2116e-01],\n",
            "          [ 1.0900e-03, -1.3654e-01, -1.0793e-01],\n",
            "          [-1.5841e-02, -1.4696e-01, -1.2748e-01]],\n",
            "\n",
            "         [[ 1.3270e-01,  5.9692e-02,  1.1092e-01],\n",
            "          [ 1.3799e-01,  8.9148e-02,  4.8023e-02],\n",
            "          [ 8.4681e-02,  1.1313e-01,  5.2237e-02]],\n",
            "\n",
            "         [[-6.9722e-02, -1.9895e-02, -1.4098e-01],\n",
            "          [-1.2751e-01, -1.7469e-01,  2.5444e-02],\n",
            "          [-1.0979e-01, -9.3190e-02, -3.9175e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 8.9451e-02,  1.0551e-01,  1.0686e-01],\n",
            "          [ 1.4243e-01,  1.6887e-01,  1.7796e-01],\n",
            "          [ 1.8296e-01,  6.5680e-02,  9.1861e-02]],\n",
            "\n",
            "         [[-1.1209e-01, -5.6371e-02, -8.6289e-02],\n",
            "          [-1.1468e-01, -1.4320e-01, -5.1517e-03],\n",
            "          [-1.5369e-01, -3.9785e-02, -5.8320e-02]],\n",
            "\n",
            "         [[ 6.9982e-02,  4.1741e-02,  8.7114e-02],\n",
            "          [ 7.1096e-02,  9.2773e-02,  1.8169e-02],\n",
            "          [ 8.8076e-03,  7.0079e-02,  1.0890e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.5632e-02, -7.6487e-03, -7.4771e-02],\n",
            "          [-9.1566e-02, -1.7057e-01, -1.3321e-01],\n",
            "          [-3.9424e-02, -1.5742e-01, -1.9522e-01]],\n",
            "\n",
            "         [[-2.3351e-02, -2.8897e-02,  5.7167e-03],\n",
            "          [-5.5982e-02, -2.0405e-02,  1.0482e-01],\n",
            "          [ 6.1971e-02,  9.3122e-02,  1.8549e-02]],\n",
            "\n",
            "         [[ 2.7011e-03,  4.0402e-02, -5.0915e-02],\n",
            "          [-2.8967e-02,  1.0706e-03, -2.3564e-02],\n",
            "          [-8.8293e-02, -4.8058e-04,  6.4125e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.2658e-02,  5.1522e-02, -1.2569e-01],\n",
            "          [ 8.6251e-02,  4.2883e-02, -1.1191e-01],\n",
            "          [ 3.5109e-02,  8.3543e-02,  1.4067e-01]],\n",
            "\n",
            "         [[ 5.0876e-02,  1.4234e-01,  1.1375e-01],\n",
            "          [ 5.2044e-02,  7.4362e-02,  1.8672e-01],\n",
            "          [ 5.1398e-02,  3.5303e-02,  5.5834e-03]],\n",
            "\n",
            "         [[-1.6491e-01, -1.5736e-01, -7.5513e-02],\n",
            "          [-7.4654e-02, -7.1223e-02, -1.4993e-01],\n",
            "          [-3.9325e-02, -6.0614e-02, -2.5963e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3010e-02,  2.8262e-02, -4.1964e-03],\n",
            "          [-3.8112e-02, -8.3619e-02,  5.1405e-02],\n",
            "          [ 5.1384e-02,  8.2908e-02, -1.8561e-02]],\n",
            "\n",
            "         [[-8.1272e-03, -3.6642e-02, -3.9312e-02],\n",
            "          [-1.1883e-02,  5.8723e-02, -2.1002e-01],\n",
            "          [-1.3120e-02, -8.7657e-02, -8.2037e-02]],\n",
            "\n",
            "         [[ 1.5482e-01,  1.5661e-01,  1.4398e-01],\n",
            "          [ 2.1145e-04,  8.1479e-02,  1.5977e-01],\n",
            "          [ 1.0266e-02,  8.6467e-02,  1.3723e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 9.1247e-02,  6.0150e-02,  8.0869e-02],\n",
            "          [ 1.5212e-01,  1.8182e-01,  2.0119e-01],\n",
            "          [ 1.1210e-01,  8.9135e-02,  1.8143e-01]],\n",
            "\n",
            "         [[-7.3982e-02, -5.4695e-02, -9.7725e-02],\n",
            "          [-2.5770e-02, -8.0387e-02, -7.9699e-02],\n",
            "          [-1.2840e-01, -1.1246e-01, -1.6148e-01]],\n",
            "\n",
            "         [[ 5.3588e-02,  1.3122e-01,  1.0109e-01],\n",
            "          [ 5.8929e-02,  1.2245e-01,  9.7419e-02],\n",
            "          [ 1.0580e-01,  7.8564e-02,  7.2457e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.2831e-02, -2.8659e-03, -6.8063e-02],\n",
            "          [-1.1064e-01, -1.1114e-01, -5.0508e-02],\n",
            "          [-1.1677e-01, -4.8760e-02, -7.0993e-02]],\n",
            "\n",
            "         [[ 6.6055e-02,  1.0625e-01,  8.6765e-02],\n",
            "          [ 9.7790e-02,  4.2126e-02,  1.4306e-01],\n",
            "          [ 3.9586e-02,  1.1982e-01,  6.9157e-02]],\n",
            "\n",
            "         [[-3.8042e-02, -1.5982e-01, -6.8182e-02],\n",
            "          [-8.8444e-02, -1.7793e-01, -1.3895e-01],\n",
            "          [-1.0964e-01, -7.3255e-02, -1.5866e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8057e-02,  6.0411e-02,  1.1997e-01],\n",
            "          [ 5.5483e-02,  1.1690e-01,  8.1165e-02],\n",
            "          [ 3.2698e-02,  1.6603e-01,  1.9669e-01]],\n",
            "\n",
            "         [[-2.9607e-02, -8.8849e-02, -1.1730e-01],\n",
            "          [-1.0628e-01, -1.0085e-01, -9.9439e-02],\n",
            "          [-8.8313e-02, -6.5193e-02, -6.3882e-02]],\n",
            "\n",
            "         [[ 3.3808e-02,  1.3222e-01,  9.5103e-02],\n",
            "          [ 2.9050e-02,  1.2636e-01,  9.2452e-02],\n",
            "          [ 5.8767e-02,  4.2491e-02,  3.6099e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7876e-02, -2.1570e-02, -4.2886e-02],\n",
            "          [-9.1635e-02, -4.5477e-02, -1.7017e-02],\n",
            "          [-1.1413e-01, -8.0722e-02, -1.5018e-01]],\n",
            "\n",
            "         [[ 1.3800e-01,  7.1170e-02,  8.7537e-02],\n",
            "          [ 1.1472e-01,  1.7206e-01,  2.4742e-02],\n",
            "          [ 1.6763e-01,  1.1723e-01,  3.3704e-02]],\n",
            "\n",
            "         [[-1.1444e-01, -7.8563e-02, -9.7779e-02],\n",
            "          [-1.2208e-01, -1.9021e-01, -9.6131e-02],\n",
            "          [-8.0374e-02, -8.5381e-02, -1.1095e-01]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0694, -0.0088, -0.0351],\n",
            "          [-0.0777, -0.0006,  0.0863],\n",
            "          [ 0.0512,  0.0883,  0.0626]],\n",
            "\n",
            "         [[-0.0105, -0.0620,  0.0163],\n",
            "          [ 0.0143, -0.0300,  0.0583],\n",
            "          [-0.0267, -0.0043, -0.0065]],\n",
            "\n",
            "         [[-0.0631, -0.0612, -0.0280],\n",
            "          [-0.0327, -0.0661, -0.0069],\n",
            "          [ 0.0645, -0.0849,  0.0207]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0693, -0.0614,  0.0555],\n",
            "          [-0.0761, -0.0280, -0.0763],\n",
            "          [ 0.0585, -0.0398, -0.0218]],\n",
            "\n",
            "         [[-0.0507, -0.0731, -0.0770],\n",
            "          [ 0.0671, -0.0504,  0.0752],\n",
            "          [ 0.0874,  0.0910,  0.0897]],\n",
            "\n",
            "         [[ 0.0376,  0.0725, -0.0593],\n",
            "          [ 0.0829,  0.0353,  0.0182],\n",
            "          [ 0.0309, -0.0942,  0.0596]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0554,  0.0534,  0.0683],\n",
            "          [-0.0844, -0.0287, -0.0092],\n",
            "          [ 0.0595,  0.0916,  0.0348]],\n",
            "\n",
            "         [[-0.0418,  0.0347, -0.0074],\n",
            "          [ 0.0311,  0.0738,  0.0145],\n",
            "          [-0.0156,  0.0786, -0.0266]],\n",
            "\n",
            "         [[ 0.0263, -0.0066,  0.0145],\n",
            "          [ 0.0654, -0.0456,  0.0449],\n",
            "          [-0.0745, -0.0853,  0.0282]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0682,  0.0519, -0.0517],\n",
            "          [-0.0641,  0.0811, -0.0384],\n",
            "          [-0.0039, -0.0440,  0.0805]],\n",
            "\n",
            "         [[-0.0099, -0.0253,  0.0818],\n",
            "          [-0.0439,  0.0022,  0.0009],\n",
            "          [ 0.0389,  0.0024,  0.0823]],\n",
            "\n",
            "         [[-0.0506, -0.0860, -0.0504],\n",
            "          [-0.0361,  0.0232,  0.0370],\n",
            "          [-0.0099, -0.0197,  0.0037]]],\n",
            "\n",
            "\n",
            "        [[[-0.0415, -0.0352,  0.0336],\n",
            "          [-0.0154,  0.0027, -0.0538],\n",
            "          [-0.0439, -0.0512, -0.0024]],\n",
            "\n",
            "         [[-0.0845, -0.0338,  0.0112],\n",
            "          [-0.0629, -0.0912,  0.0770],\n",
            "          [-0.0450, -0.0903, -0.0426]],\n",
            "\n",
            "         [[-0.0694,  0.0709, -0.0853],\n",
            "          [ 0.0397, -0.0910, -0.0499],\n",
            "          [-0.0640, -0.0865,  0.0312]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0791,  0.0603,  0.0741],\n",
            "          [ 0.0130, -0.0348, -0.0509],\n",
            "          [-0.0611, -0.0355,  0.0202]],\n",
            "\n",
            "         [[ 0.0471,  0.0134, -0.0610],\n",
            "          [ 0.0447,  0.0396,  0.0143],\n",
            "          [ 0.0611, -0.0141,  0.0039]],\n",
            "\n",
            "         [[-0.0928, -0.0682, -0.0458],\n",
            "          [ 0.0605, -0.0595,  0.0786],\n",
            "          [-0.0543, -0.0613, -0.0254]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0666,  0.0446,  0.0242],\n",
            "          [-0.0292,  0.0085, -0.0644],\n",
            "          [ 0.0206,  0.0505,  0.0385]],\n",
            "\n",
            "         [[-0.0476,  0.0731, -0.0862],\n",
            "          [-0.0549,  0.0756, -0.0546],\n",
            "          [-0.0811,  0.0228, -0.0523]],\n",
            "\n",
            "         [[ 0.0101,  0.0641,  0.0887],\n",
            "          [ 0.0897, -0.0380, -0.0327],\n",
            "          [ 0.0318, -0.0561,  0.0543]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0678,  0.0502, -0.0745],\n",
            "          [ 0.0160, -0.0727, -0.0490],\n",
            "          [ 0.0323,  0.0447,  0.0261]],\n",
            "\n",
            "         [[ 0.0643,  0.0840,  0.0908],\n",
            "          [ 0.0858, -0.0148, -0.0020],\n",
            "          [-0.0554,  0.0278, -0.0237]],\n",
            "\n",
            "         [[ 0.0907,  0.0424, -0.0177],\n",
            "          [-0.0962, -0.0664, -0.0395],\n",
            "          [-0.0328,  0.0641, -0.0003]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0467, -0.0449,  0.0381],\n",
            "          [ 0.0520,  0.0240,  0.0452],\n",
            "          [ 0.0070,  0.0027, -0.0074]],\n",
            "\n",
            "         [[-0.0588,  0.0953, -0.0251],\n",
            "          [ 0.0799, -0.0299,  0.0743],\n",
            "          [ 0.0890, -0.0818,  0.0003]],\n",
            "\n",
            "         [[ 0.0575, -0.0889,  0.0822],\n",
            "          [-0.0111,  0.0827,  0.0065],\n",
            "          [ 0.0096,  0.0274, -0.0836]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0775, -0.0140, -0.0151],\n",
            "          [-0.0508, -0.0578, -0.0921],\n",
            "          [ 0.0356,  0.0634, -0.0289]],\n",
            "\n",
            "         [[ 0.0444,  0.0130, -0.0349],\n",
            "          [-0.0336, -0.0887, -0.0562],\n",
            "          [-0.0128, -0.0320,  0.0495]],\n",
            "\n",
            "         [[-0.0409,  0.0275, -0.0492],\n",
            "          [-0.0564, -0.0467,  0.0176],\n",
            "          [ 0.0674, -0.0719,  0.0330]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0222,  0.0300,  0.0041],\n",
            "          [ 0.0905, -0.0928,  0.0162],\n",
            "          [-0.0513, -0.0182,  0.0728]],\n",
            "\n",
            "         [[-0.0025, -0.0648, -0.0695],\n",
            "          [ 0.0366,  0.0651, -0.0715],\n",
            "          [ 0.0045, -0.0377,  0.0441]],\n",
            "\n",
            "         [[-0.0010,  0.0721, -0.0734],\n",
            "          [ 0.0677, -0.0469, -0.0089],\n",
            "          [ 0.0374, -0.0243,  0.0128]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0323,  0.0205, -0.0601],\n",
            "          [ 0.0114,  0.0075, -0.0532],\n",
            "          [-0.0474,  0.0415,  0.0465]],\n",
            "\n",
            "         [[-0.0776,  0.0309,  0.0380],\n",
            "          [ 0.0088, -0.0141, -0.0285],\n",
            "          [ 0.0183, -0.0112,  0.0440]],\n",
            "\n",
            "         [[-0.0860,  0.0316, -0.0801],\n",
            "          [ 0.0464, -0.0703,  0.0534],\n",
            "          [-0.0569,  0.0175,  0.0088]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.1866, -0.0177,  0.1937,  0.1395,  0.0837,  0.1085,  0.1669,  0.0451,\n",
            "         0.0840,  0.1878,  0.1002, -0.0257], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0594,  0.0314,  0.0012],\n",
            "          [-0.0955, -0.0651,  0.0732],\n",
            "          [-0.0612, -0.0729,  0.0957]],\n",
            "\n",
            "         [[-0.0367,  0.0097,  0.0392],\n",
            "          [-0.0258,  0.0407,  0.0856],\n",
            "          [ 0.0009, -0.0053, -0.0885]],\n",
            "\n",
            "         [[ 0.0901, -0.0679, -0.0866],\n",
            "          [ 0.0698, -0.0962,  0.0132],\n",
            "          [ 0.0278,  0.0037,  0.0453]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0584, -0.0952, -0.0465],\n",
            "          [-0.0454,  0.0443, -0.0872],\n",
            "          [ 0.0753, -0.0060, -0.0931]],\n",
            "\n",
            "         [[-0.0083,  0.0115,  0.0409],\n",
            "          [ 0.0444,  0.0761, -0.0298],\n",
            "          [-0.0670, -0.0738, -0.0444]],\n",
            "\n",
            "         [[ 0.0820, -0.0959, -0.0957],\n",
            "          [ 0.0629,  0.0067, -0.0717],\n",
            "          [-0.0449, -0.0199, -0.0433]]],\n",
            "\n",
            "\n",
            "        [[[-0.0752, -0.0880, -0.0005],\n",
            "          [ 0.0563, -0.0747, -0.0081],\n",
            "          [-0.0730,  0.0874,  0.0495]],\n",
            "\n",
            "         [[ 0.0055, -0.0766, -0.0564],\n",
            "          [ 0.0112, -0.0211, -0.0490],\n",
            "          [ 0.0704,  0.0545, -0.0662]],\n",
            "\n",
            "         [[ 0.0106, -0.0437, -0.0562],\n",
            "          [-0.0549,  0.0692, -0.0957],\n",
            "          [ 0.0248, -0.0345,  0.0123]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0874, -0.0754, -0.0844],\n",
            "          [-0.0659,  0.0805,  0.0466],\n",
            "          [ 0.0484,  0.0430, -0.0848]],\n",
            "\n",
            "         [[ 0.0895, -0.0629,  0.0771],\n",
            "          [ 0.0169,  0.0500, -0.0490],\n",
            "          [ 0.0723, -0.0700, -0.0197]],\n",
            "\n",
            "         [[-0.0852, -0.0305,  0.0869],\n",
            "          [ 0.0485, -0.0265,  0.0558],\n",
            "          [ 0.0786,  0.0736, -0.0932]]],\n",
            "\n",
            "\n",
            "        [[[-0.0549,  0.0642, -0.0637],\n",
            "          [-0.0367, -0.0639,  0.0143],\n",
            "          [-0.0162,  0.0191,  0.0191]],\n",
            "\n",
            "         [[-0.0610,  0.0138,  0.0679],\n",
            "          [-0.0598,  0.0888,  0.0472],\n",
            "          [ 0.0326,  0.0428,  0.0102]],\n",
            "\n",
            "         [[ 0.0551,  0.0644, -0.0498],\n",
            "          [ 0.0298,  0.0641,  0.0580],\n",
            "          [ 0.0833, -0.0957, -0.0863]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0958,  0.0816, -0.0712],\n",
            "          [ 0.0110, -0.0369, -0.0189],\n",
            "          [-0.0932, -0.0306,  0.0243]],\n",
            "\n",
            "         [[ 0.0705,  0.0741, -0.0081],\n",
            "          [ 0.0260,  0.0638,  0.0172],\n",
            "          [-0.0455, -0.0776, -0.0288]],\n",
            "\n",
            "         [[-0.0587,  0.0790,  0.0249],\n",
            "          [ 0.0303, -0.0856, -0.0860],\n",
            "          [ 0.0243, -0.0262,  0.0897]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0167,  0.0299, -0.0380],\n",
            "          [-0.0438, -0.0565,  0.0264],\n",
            "          [-0.0447,  0.0301,  0.0467]],\n",
            "\n",
            "         [[ 0.0667,  0.0749, -0.0025],\n",
            "          [-0.0346,  0.0079,  0.0045],\n",
            "          [-0.0959, -0.0500,  0.0780]],\n",
            "\n",
            "         [[-0.0242, -0.0837,  0.0647],\n",
            "          [-0.0500,  0.0284,  0.0121],\n",
            "          [ 0.0856, -0.0525,  0.0825]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0437,  0.0890, -0.0525],\n",
            "          [-0.0495, -0.0451,  0.0556],\n",
            "          [ 0.0038,  0.0763, -0.0729]],\n",
            "\n",
            "         [[-0.0363,  0.0232, -0.0857],\n",
            "          [ 0.0240,  0.0836,  0.0479],\n",
            "          [ 0.0716,  0.0952, -0.0303]],\n",
            "\n",
            "         [[-0.0079, -0.0495, -0.0246],\n",
            "          [-0.0343, -0.0479,  0.0328],\n",
            "          [-0.0682,  0.0040, -0.0279]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0075,  0.0098, -0.0811],\n",
            "          [-0.0434,  0.0832, -0.0472],\n",
            "          [-0.0708, -0.0422,  0.0450]],\n",
            "\n",
            "         [[ 0.0330,  0.0073,  0.0699],\n",
            "          [-0.0048,  0.0561, -0.0240],\n",
            "          [ 0.0468, -0.0179,  0.0351]],\n",
            "\n",
            "         [[ 0.0815,  0.0212,  0.0710],\n",
            "          [ 0.0576, -0.0463,  0.0077],\n",
            "          [ 0.0046,  0.0675, -0.0612]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0716, -0.0035,  0.0490],\n",
            "          [ 0.0946, -0.0826,  0.0707],\n",
            "          [-0.0284,  0.0544,  0.0680]],\n",
            "\n",
            "         [[ 0.0048,  0.0319, -0.0082],\n",
            "          [-0.0764,  0.0924,  0.0128],\n",
            "          [ 0.0905, -0.0685,  0.0520]],\n",
            "\n",
            "         [[ 0.0859, -0.0626, -0.0362],\n",
            "          [-0.0595, -0.0307,  0.0318],\n",
            "          [ 0.0830, -0.0143, -0.0082]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0352,  0.0412,  0.0110],\n",
            "          [ 0.0225,  0.0711,  0.0719],\n",
            "          [ 0.0120, -0.0949, -0.0780]],\n",
            "\n",
            "         [[-0.0929,  0.0047, -0.0054],\n",
            "          [ 0.0393, -0.0488, -0.0446],\n",
            "          [ 0.0046,  0.0433, -0.0493]],\n",
            "\n",
            "         [[ 0.0373, -0.0227, -0.0613],\n",
            "          [-0.0172, -0.0944,  0.0148],\n",
            "          [ 0.0834,  0.0878,  0.0909]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0463, -0.0496, -0.0343],\n",
            "          [-0.0287, -0.0661, -0.0107],\n",
            "          [ 0.0483,  0.0358,  0.0104]],\n",
            "\n",
            "         [[-0.0747,  0.0255, -0.0055],\n",
            "          [-0.0006,  0.0770, -0.0055],\n",
            "          [-0.0380, -0.0064,  0.0495]],\n",
            "\n",
            "         [[-0.0037, -0.0717,  0.0114],\n",
            "          [-0.0956,  0.0820,  0.0263],\n",
            "          [-0.0891,  0.0640, -0.0849]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.0663, -0.0819, -0.0751, -0.0340, -0.0616,  0.0696, -0.0442, -0.0211,\n",
            "         0.0868,  0.0890, -0.0840,  0.0339], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-2.3630e-02, -4.1810e-02,  6.2083e-02],\n",
            "          [ 9.1330e-02,  4.5415e-02, -3.1446e-02],\n",
            "          [ 2.5638e-02,  2.2268e-02, -5.6899e-02]],\n",
            "\n",
            "         [[-4.4772e-02, -7.3522e-02,  2.7343e-02],\n",
            "          [-7.0089e-02,  2.5610e-02, -3.1810e-02],\n",
            "          [-2.0143e-02,  2.9570e-02, -9.0852e-02]],\n",
            "\n",
            "         [[-1.8003e-02, -3.4712e-02,  1.4419e-02],\n",
            "          [-2.9089e-02, -7.6329e-02,  4.0431e-02],\n",
            "          [-4.8772e-02,  5.4252e-02, -3.6552e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.7720e-03,  2.0203e-02,  4.9192e-02],\n",
            "          [ 1.5990e-02, -8.1774e-02,  5.3977e-02],\n",
            "          [-6.5996e-02,  5.6385e-02,  1.4058e-02]],\n",
            "\n",
            "         [[ 2.7875e-02, -3.0982e-02,  7.2207e-02],\n",
            "          [ 5.0896e-02,  4.6764e-02, -8.3216e-02],\n",
            "          [ 6.4926e-02, -9.2595e-02,  6.1657e-02]],\n",
            "\n",
            "         [[ 4.4591e-03,  1.8792e-02,  2.5508e-02],\n",
            "          [-5.0663e-03, -5.4502e-02,  4.7855e-02],\n",
            "          [ 5.2572e-02, -7.4187e-02, -9.1673e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 8.6932e-02,  8.7381e-02,  5.2880e-02],\n",
            "          [-5.8752e-02,  7.4532e-03,  8.2190e-02],\n",
            "          [ 8.2181e-03, -6.9991e-02,  3.8842e-02]],\n",
            "\n",
            "         [[-9.5040e-02, -1.9856e-02, -1.0660e-02],\n",
            "          [-9.5225e-02, -8.1684e-03, -1.7882e-02],\n",
            "          [ 1.0307e-02,  4.0908e-02, -9.0071e-02]],\n",
            "\n",
            "         [[ 4.0926e-02,  5.5650e-02, -8.7110e-02],\n",
            "          [ 1.9583e-02,  4.9624e-02,  8.8723e-02],\n",
            "          [ 3.6356e-02, -1.5905e-02,  3.9616e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3693e-02,  1.3183e-02,  8.5146e-02],\n",
            "          [-7.8242e-02, -4.4815e-02,  4.5915e-02],\n",
            "          [ 3.1104e-03, -3.5691e-02,  2.2996e-02]],\n",
            "\n",
            "         [[ 6.5949e-02, -8.9727e-02,  8.8578e-02],\n",
            "          [-5.7651e-02, -1.1755e-02,  1.8833e-03],\n",
            "          [-8.7709e-02,  4.2803e-02,  5.4689e-03]],\n",
            "\n",
            "         [[-4.0608e-02,  7.0250e-02, -5.2388e-02],\n",
            "          [-6.2237e-02, -2.0307e-02,  1.0884e-02],\n",
            "          [-2.2868e-02,  2.9636e-02, -4.7984e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.5449e-02,  7.2169e-02, -1.6470e-02],\n",
            "          [-9.1747e-02,  3.1942e-03, -7.5837e-02],\n",
            "          [-6.9509e-02,  1.3401e-04, -6.4999e-02]],\n",
            "\n",
            "         [[ 3.8861e-03, -6.6514e-02,  6.2873e-02],\n",
            "          [ 3.4553e-02,  7.4656e-02,  7.4250e-02],\n",
            "          [ 7.8897e-03, -9.5763e-02, -6.3419e-02]],\n",
            "\n",
            "         [[ 9.3200e-02, -1.8047e-02, -6.5242e-02],\n",
            "          [-3.3356e-02,  3.9627e-02, -5.3402e-02],\n",
            "          [ 3.1889e-02, -2.2561e-02, -2.4606e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2352e-02,  4.9590e-02, -3.0679e-02],\n",
            "          [ 4.9316e-02,  6.9499e-02, -9.0661e-02],\n",
            "          [-9.2608e-02, -1.1330e-02, -8.9535e-02]],\n",
            "\n",
            "         [[ 9.0774e-02, -4.0530e-03,  4.7753e-02],\n",
            "          [ 8.1596e-02, -2.5868e-02, -4.1636e-02],\n",
            "          [ 3.9412e-02, -3.3468e-03, -4.4388e-02]],\n",
            "\n",
            "         [[ 1.2786e-02, -6.0165e-02, -8.4118e-02],\n",
            "          [-5.4805e-02,  4.8848e-02, -3.4791e-02],\n",
            "          [ 6.3975e-02, -4.2258e-02,  4.6935e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 8.0259e-02,  7.1770e-02, -3.9105e-02],\n",
            "          [-1.4430e-02,  4.6129e-02,  4.1599e-02],\n",
            "          [ 8.5718e-03,  3.4415e-02,  4.8115e-02]],\n",
            "\n",
            "         [[-6.6712e-02, -1.8750e-02,  3.2164e-02],\n",
            "          [ 5.2772e-04,  6.1519e-03, -8.2528e-02],\n",
            "          [ 5.9638e-02,  7.3612e-02,  1.8430e-02]],\n",
            "\n",
            "         [[ 5.1216e-02, -7.8190e-02,  5.2926e-02],\n",
            "          [-7.8543e-02,  7.1941e-02, -8.0602e-02],\n",
            "          [ 1.3902e-02,  3.0663e-02,  4.2277e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.3810e-02, -1.0591e-03, -5.3569e-02],\n",
            "          [ 7.4773e-02, -5.1932e-02,  4.1373e-03],\n",
            "          [ 6.7925e-02, -3.4847e-02, -3.6728e-02]],\n",
            "\n",
            "         [[-6.4077e-02, -4.8546e-02,  8.1608e-02],\n",
            "          [ 1.7822e-02, -5.5762e-02,  3.5573e-02],\n",
            "          [ 1.7488e-02, -6.1502e-02, -5.4149e-02]],\n",
            "\n",
            "         [[ 8.7264e-02,  7.0229e-02,  3.1696e-02],\n",
            "          [ 3.0731e-02, -6.8302e-02,  3.1397e-02],\n",
            "          [ 6.8675e-02,  6.8913e-02,  4.2756e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.1947e-02, -6.0377e-03, -9.2729e-02],\n",
            "          [-4.9201e-02,  4.0971e-03,  8.0578e-02],\n",
            "          [-7.1653e-02, -8.1410e-02, -8.6801e-02]],\n",
            "\n",
            "         [[ 5.7914e-02, -9.5025e-02,  5.9490e-02],\n",
            "          [ 3.5475e-02, -5.1598e-02,  8.6555e-02],\n",
            "          [-7.2603e-02,  7.3402e-02, -2.3734e-02]],\n",
            "\n",
            "         [[ 6.8571e-02,  6.5869e-02, -1.5141e-03],\n",
            "          [-2.6611e-02, -6.7366e-02, -4.8465e-02],\n",
            "          [ 4.9667e-02, -6.2322e-02, -7.2029e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.0887e-02, -6.0400e-02, -7.9886e-03],\n",
            "          [-7.7902e-02, -1.1065e-02, -3.8622e-03],\n",
            "          [ 3.9137e-02,  3.0823e-03, -6.0355e-02]],\n",
            "\n",
            "         [[-4.6129e-02, -2.6514e-02,  4.4420e-02],\n",
            "          [ 4.7649e-02, -3.4435e-02,  8.3263e-02],\n",
            "          [ 7.7770e-02,  3.7081e-02,  2.3268e-03]],\n",
            "\n",
            "         [[ 9.5416e-02,  2.5277e-02,  3.1086e-02],\n",
            "          [ 5.1003e-02,  4.9885e-02,  3.3616e-02],\n",
            "          [-4.1693e-02,  3.7119e-03, -5.5527e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.2928e-02,  5.0883e-02,  9.4380e-02],\n",
            "          [-2.2021e-02, -1.1182e-02, -7.7512e-02],\n",
            "          [ 3.2991e-02, -8.5207e-03,  2.8254e-02]],\n",
            "\n",
            "         [[ 3.4743e-02,  1.6046e-02,  4.8902e-02],\n",
            "          [-3.9120e-02, -9.1857e-02, -8.5728e-02],\n",
            "          [ 1.5595e-02,  5.0263e-02,  7.4562e-02]],\n",
            "\n",
            "         [[ 1.9444e-02,  3.7987e-02, -8.0728e-02],\n",
            "          [ 8.1804e-02,  2.6356e-03,  3.2954e-02],\n",
            "          [ 5.8671e-02,  4.6916e-02,  3.6371e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2591e-02,  5.5869e-02,  7.8045e-02],\n",
            "          [-3.5872e-02,  2.1524e-02,  7.1440e-02],\n",
            "          [ 7.7938e-02,  7.1256e-02, -2.6657e-02]],\n",
            "\n",
            "         [[ 2.8983e-02, -9.0765e-02,  7.4449e-02],\n",
            "          [ 6.4826e-02, -3.2322e-02, -2.1078e-02],\n",
            "          [ 1.8574e-02, -3.2826e-02,  7.7904e-02]],\n",
            "\n",
            "         [[ 4.9900e-02,  4.1067e-03,  2.8636e-02],\n",
            "          [-7.4221e-02, -5.0269e-05,  4.8803e-02],\n",
            "          [ 8.3622e-02,  4.5544e-02,  6.6533e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.0884,  0.0529,  0.2116,  0.1045,  0.0909,  0.0040, -0.1131,  0.0230,\n",
            "         0.0415,  0.0787, -0.0176, -0.0613], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 8.9323e-02, -3.8233e-02,  3.4874e-02],\n",
            "          [-7.2073e-02, -2.2002e-02,  7.6394e-02],\n",
            "          [ 6.3263e-02,  4.2139e-02, -2.2630e-02]],\n",
            "\n",
            "         [[-3.1139e-02, -5.4311e-02, -4.3435e-02],\n",
            "          [-3.1210e-02, -8.3818e-02, -6.3715e-03],\n",
            "          [ 4.4424e-02,  5.3280e-02,  2.5899e-02]],\n",
            "\n",
            "         [[-2.2882e-02,  8.3429e-02,  6.1406e-02],\n",
            "          [-3.0741e-02, -4.3851e-03,  6.9948e-02],\n",
            "          [ 3.9029e-02, -1.5557e-02,  3.4762e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.0552e-02, -1.2997e-02,  3.0508e-02],\n",
            "          [-8.7321e-02, -1.8224e-02, -6.1368e-02],\n",
            "          [-1.6066e-02,  9.5727e-02, -2.5028e-02]],\n",
            "\n",
            "         [[ 8.2903e-02, -4.1847e-02, -2.9470e-02],\n",
            "          [-2.3464e-02,  1.7824e-02, -9.3955e-02],\n",
            "          [ 7.0119e-02,  6.9460e-02, -5.2776e-02]],\n",
            "\n",
            "         [[ 1.7298e-02,  6.6345e-02, -8.6815e-02],\n",
            "          [-9.0266e-02, -3.6784e-02, -6.1388e-02],\n",
            "          [ 2.3632e-02, -1.0046e-02,  5.8809e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.4975e-02, -4.0056e-02, -8.2620e-02],\n",
            "          [ 6.1963e-02, -3.2607e-02, -8.4471e-02],\n",
            "          [-1.0843e-02,  9.0938e-02, -9.2780e-03]],\n",
            "\n",
            "         [[ 5.9676e-02, -5.4566e-02,  6.6036e-02],\n",
            "          [ 9.4953e-02, -3.1944e-02, -2.6496e-02],\n",
            "          [ 5.1718e-02, -8.1042e-02, -8.6471e-02]],\n",
            "\n",
            "         [[-8.9305e-02, -3.5435e-02,  5.5226e-02],\n",
            "          [ 8.3445e-02,  4.3879e-03, -2.2394e-02],\n",
            "          [ 7.2278e-02, -1.5561e-02,  1.1946e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.1313e-02, -3.1474e-02, -5.9687e-05],\n",
            "          [ 4.8696e-02,  1.2452e-02,  3.3883e-02],\n",
            "          [-1.7543e-02, -7.2601e-02,  6.1147e-02]],\n",
            "\n",
            "         [[ 1.0549e-02,  2.5353e-02, -1.0031e-02],\n",
            "          [ 9.0519e-02, -2.8714e-02,  4.7853e-02],\n",
            "          [ 9.2030e-02, -2.6759e-02, -7.7941e-02]],\n",
            "\n",
            "         [[ 5.6808e-02,  7.9177e-02,  6.3063e-02],\n",
            "          [-9.1556e-02,  3.1389e-03, -2.5469e-02],\n",
            "          [ 8.3246e-02, -8.0086e-03, -8.7104e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.3884e-02, -3.9794e-02,  5.7075e-02],\n",
            "          [-7.1996e-02,  4.4019e-02,  7.9659e-02],\n",
            "          [-7.6725e-02, -6.0519e-02,  5.8164e-03]],\n",
            "\n",
            "         [[-7.9490e-02, -6.9957e-02,  5.5059e-02],\n",
            "          [ 3.2152e-02, -4.5024e-02,  4.9297e-02],\n",
            "          [ 8.9111e-02, -3.4457e-02,  8.7114e-03]],\n",
            "\n",
            "         [[ 7.1961e-02, -5.5435e-02,  4.1813e-02],\n",
            "          [ 7.3195e-02,  9.6136e-02,  1.5501e-02],\n",
            "          [ 7.0210e-04,  4.8695e-02, -2.3457e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3363e-02,  4.9423e-03, -8.0144e-02],\n",
            "          [ 2.3472e-02, -4.4142e-02,  3.6774e-02],\n",
            "          [ 3.9802e-02,  9.5611e-02, -3.3003e-02]],\n",
            "\n",
            "         [[-1.8224e-02,  8.3343e-02, -5.5629e-02],\n",
            "          [ 1.2254e-02,  5.2854e-03,  6.6313e-02],\n",
            "          [-1.8105e-02,  5.4055e-02,  9.5598e-03]],\n",
            "\n",
            "         [[ 4.2600e-02, -2.5337e-02, -6.1685e-02],\n",
            "          [ 1.5694e-03, -1.8911e-03, -3.9420e-02],\n",
            "          [-2.2452e-02, -5.0198e-02, -3.3585e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.9129e-02, -9.3900e-02,  3.2410e-02],\n",
            "          [-3.5773e-02, -3.4636e-02, -3.1282e-02],\n",
            "          [-1.3154e-02, -2.1368e-02,  6.0863e-02]],\n",
            "\n",
            "         [[ 1.7004e-03, -6.3893e-02, -7.3714e-02],\n",
            "          [-5.4133e-02,  4.0900e-02, -4.8415e-02],\n",
            "          [ 1.6087e-02,  2.6794e-02,  1.9162e-02]],\n",
            "\n",
            "         [[ 4.5377e-02,  9.3042e-02, -4.3465e-02],\n",
            "          [-5.3011e-02,  8.4765e-02,  6.4958e-03],\n",
            "          [ 6.8324e-02, -1.6783e-02, -1.1352e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.4148e-03, -8.5294e-02,  5.4870e-02],\n",
            "          [ 2.5322e-02,  6.4441e-02,  4.5557e-02],\n",
            "          [ 7.2340e-03,  5.8132e-02, -2.1798e-02]],\n",
            "\n",
            "         [[-3.3303e-02,  1.2958e-02, -8.4110e-02],\n",
            "          [ 8.3108e-03, -4.0322e-02,  2.7872e-02],\n",
            "          [ 5.8730e-02, -2.7911e-02,  7.2537e-02]],\n",
            "\n",
            "         [[ 3.3304e-02,  4.5852e-02,  5.3793e-02],\n",
            "          [ 3.8975e-02,  8.6543e-02,  7.8959e-02],\n",
            "          [-7.4883e-02, -9.1146e-02,  3.4445e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.1870e-02,  6.0509e-02,  2.6567e-02],\n",
            "          [ 7.5594e-02, -6.3203e-02, -5.1062e-02],\n",
            "          [-5.6790e-02, -2.2923e-03, -2.9553e-02]],\n",
            "\n",
            "         [[-2.9821e-02,  6.1766e-02,  5.4862e-02],\n",
            "          [ 4.1158e-02,  2.7787e-02, -5.2567e-02],\n",
            "          [ 8.4500e-02, -4.9328e-02, -1.3460e-02]],\n",
            "\n",
            "         [[ 4.3686e-02,  5.5188e-02,  4.2857e-02],\n",
            "          [-4.7970e-02, -6.4976e-02,  2.9630e-02],\n",
            "          [ 8.7243e-03, -6.6264e-02,  4.9903e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6705e-02, -6.2444e-02,  6.8436e-02],\n",
            "          [-1.9620e-02, -7.2360e-02,  8.5816e-02],\n",
            "          [-5.1174e-02,  1.7870e-02, -4.6998e-02]],\n",
            "\n",
            "         [[ 4.1992e-02, -5.5887e-02, -9.0173e-02],\n",
            "          [-4.1830e-02,  7.0700e-02,  3.6145e-03],\n",
            "          [ 2.0961e-02, -8.2819e-03,  1.2975e-02]],\n",
            "\n",
            "         [[ 5.7094e-02, -1.8582e-02,  9.2732e-02],\n",
            "          [ 8.1458e-02,  1.0334e-02,  5.1058e-02],\n",
            "          [-5.6370e-02, -3.6497e-02, -9.1391e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.2552e-02,  6.2658e-02,  4.8901e-02],\n",
            "          [ 3.4084e-02,  7.2052e-02,  4.0642e-02],\n",
            "          [ 4.1136e-02, -5.5577e-02, -3.5477e-03]],\n",
            "\n",
            "         [[-2.3363e-02, -4.0338e-02,  4.1424e-02],\n",
            "          [ 3.3292e-02,  7.9000e-04,  3.6952e-03],\n",
            "          [-5.2184e-02, -6.8915e-02, -3.4094e-02]],\n",
            "\n",
            "         [[-5.4861e-02,  6.3422e-02, -9.5358e-02],\n",
            "          [-1.7397e-02,  5.9409e-02, -4.2936e-02],\n",
            "          [ 6.0069e-02, -3.6143e-02,  9.4347e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.4952e-02, -4.0506e-02, -6.7606e-02],\n",
            "          [-1.7978e-02,  4.8187e-03, -3.0137e-02],\n",
            "          [-2.1605e-02,  5.2791e-02,  2.7662e-02]],\n",
            "\n",
            "         [[-1.1047e-02,  7.8661e-02,  7.1452e-02],\n",
            "          [ 4.5163e-02, -2.9610e-02, -5.6130e-02],\n",
            "          [-1.6378e-02,  4.8531e-02,  8.7111e-02]],\n",
            "\n",
            "         [[ 6.1178e-03,  4.8558e-02,  4.9187e-02],\n",
            "          [-1.6599e-02, -7.3616e-02,  4.4250e-02],\n",
            "          [-8.1928e-02,  6.0257e-02,  8.2857e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([ 0.1586, -0.0073,  0.0123,  0.0166,  0.0129,  0.0580,  0.1328,  0.1280,\n",
            "         0.0240,  0.1380,  0.0968, -0.0269], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-2.3250, -0.9113, -0.8734,  ..., -1.0165, -1.0007, -2.2580],\n",
            "          [-0.9107,  0.3954,  0.2952,  ...,  0.1370,  0.1554, -0.9697],\n",
            "          [-0.8843,  0.3248,  0.1994,  ...,  0.0986,  0.1166, -0.9705],\n",
            "          ...,\n",
            "          [-0.9808,  0.1916,  0.1026,  ...,  0.0888,  0.1111, -0.9865],\n",
            "          [-0.7990,  0.2900,  0.1716,  ...,  0.1311,  0.1590, -1.0003],\n",
            "          [-2.2414, -0.7810, -0.7988,  ..., -0.8458, -0.8836, -2.3124]]]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0541,  0.2364,  0.0977],\n",
            "          [ 0.1309,  0.3105,  0.2819],\n",
            "          [ 0.0990,  0.3322,  0.1853]],\n",
            "\n",
            "         [[-0.1233, -0.1724, -0.1720],\n",
            "          [-0.1879, -0.1130, -0.1006],\n",
            "          [-0.1951, -0.3057, -0.1698]],\n",
            "\n",
            "         [[ 0.1898,  0.0916,  0.0384],\n",
            "          [ 0.1692,  0.1296,  0.1611],\n",
            "          [ 0.1494,  0.1146, -0.0034]],\n",
            "\n",
            "         [[ 0.0024, -0.2797, -0.0714],\n",
            "          [-0.2175, -0.4695, -0.2103],\n",
            "          [-0.1915, -0.2938, -0.2255]],\n",
            "\n",
            "         [[-0.2124, -0.1516, -0.0576],\n",
            "          [-0.2664, -0.2994, -0.1471],\n",
            "          [-0.1995, -0.2069, -0.0062]],\n",
            "\n",
            "         [[ 0.0511,  0.2354,  0.1111],\n",
            "          [ 0.1395,  0.1727,  0.1632],\n",
            "          [ 0.1770,  0.2630,  0.1325]],\n",
            "\n",
            "         [[-0.0578, -0.1072, -0.1113],\n",
            "          [-0.1276, -0.1955, -0.1592],\n",
            "          [ 0.0921, -0.0098, -0.0801]],\n",
            "\n",
            "         [[ 0.1249,  0.0645,  0.1795],\n",
            "          [ 0.2280,  0.1681,  0.0956],\n",
            "          [ 0.1291,  0.1023,  0.1211]],\n",
            "\n",
            "         [[ 0.1667,  0.2166,  0.0422],\n",
            "          [ 0.2153,  0.0937,  0.1404],\n",
            "          [ 0.2568,  0.1188,  0.1858]],\n",
            "\n",
            "         [[ 0.0073,  0.0656,  0.0955],\n",
            "          [ 0.1591,  0.1218, -0.0032],\n",
            "          [ 0.0453, -0.0155,  0.1015]],\n",
            "\n",
            "         [[ 0.1043,  0.2581,  0.1054],\n",
            "          [ 0.2521,  0.1079,  0.2446],\n",
            "          [ 0.2412,  0.1922,  0.1447]],\n",
            "\n",
            "         [[ 0.1692,  0.2527,  0.1179],\n",
            "          [ 0.1896,  0.2841,  0.2303],\n",
            "          [ 0.1568,  0.1575,  0.1775]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0108, -0.0927,  0.0103],\n",
            "          [-0.0368, -0.0714,  0.0633],\n",
            "          [ 0.0137,  0.0398,  0.0951]],\n",
            "\n",
            "         [[ 0.0177, -0.0791, -0.0543],\n",
            "          [ 0.0751,  0.0413,  0.0530],\n",
            "          [ 0.0628, -0.0115,  0.0559]],\n",
            "\n",
            "         [[-0.0693, -0.0938, -0.0712],\n",
            "          [ 0.0266,  0.0885,  0.0389],\n",
            "          [ 0.0769,  0.0063, -0.0840]],\n",
            "\n",
            "         [[-0.0249, -0.0497,  0.0581],\n",
            "          [ 0.0062, -0.0272, -0.0130],\n",
            "          [ 0.0835,  0.0048,  0.0445]],\n",
            "\n",
            "         [[-0.0755,  0.0794, -0.0812],\n",
            "          [-0.0653,  0.0334,  0.0041],\n",
            "          [-0.0433, -0.0940,  0.0630]],\n",
            "\n",
            "         [[-0.0457, -0.0427,  0.0349],\n",
            "          [ 0.0160, -0.0217, -0.0177],\n",
            "          [ 0.0868, -0.0488, -0.0290]],\n",
            "\n",
            "         [[ 0.0068,  0.0224,  0.0890],\n",
            "          [ 0.0405, -0.0355,  0.0667],\n",
            "          [ 0.0197,  0.0257, -0.0570]],\n",
            "\n",
            "         [[ 0.0396, -0.0896,  0.0681],\n",
            "          [ 0.0454, -0.0048, -0.0500],\n",
            "          [-0.0795,  0.0490,  0.0674]],\n",
            "\n",
            "         [[ 0.0411, -0.0884, -0.0518],\n",
            "          [ 0.0316,  0.0871, -0.0222],\n",
            "          [-0.0305,  0.0840,  0.0470]],\n",
            "\n",
            "         [[ 0.0498, -0.0844,  0.0696],\n",
            "          [ 0.0574,  0.0750,  0.0096],\n",
            "          [-0.0351,  0.0087,  0.0942]],\n",
            "\n",
            "         [[-0.0593,  0.0363, -0.0136],\n",
            "          [-0.0329,  0.0081,  0.0337],\n",
            "          [ 0.0455,  0.0317,  0.0154]],\n",
            "\n",
            "         [[ 0.0005, -0.0173,  0.0071],\n",
            "          [ 0.0506, -0.0951,  0.0108],\n",
            "          [-0.0926, -0.0119,  0.0858]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-2.2274e-02, -3.0450e-02,  4.8478e-02],\n",
            "          [-1.1926e-02,  1.9254e-01,  2.2734e-01],\n",
            "          [ 8.5069e-02,  2.3722e-01,  2.2924e-01]],\n",
            "\n",
            "         [[-8.8543e-02, -1.7276e-01, -1.8621e-01],\n",
            "          [-1.7048e-01, -2.3381e-02, -1.5499e-01],\n",
            "          [-2.2331e-01, -1.9672e-01, -9.1688e-05]],\n",
            "\n",
            "         [[ 9.9571e-02, -7.6469e-02, -4.2599e-02],\n",
            "          [ 4.8065e-02, -4.8419e-02, -1.6940e-01],\n",
            "          [ 2.5689e-02, -7.0244e-02, -1.9137e-01]],\n",
            "\n",
            "         [[-1.6157e-02, -6.4851e-02, -6.7207e-02],\n",
            "          [-5.3824e-02, -3.0393e-01, -6.7742e-02],\n",
            "          [-7.0187e-02, -1.7234e-01, -1.7513e-01]],\n",
            "\n",
            "         [[-1.2731e-01, -1.7052e-01, -1.2455e-01],\n",
            "          [-2.9968e-02, -1.6630e-01, -1.9907e-01],\n",
            "          [-1.0719e-02, -7.9262e-02, -5.1077e-03]],\n",
            "\n",
            "         [[ 9.1711e-02,  4.3348e-02,  8.2940e-02],\n",
            "          [ 1.2599e-01,  2.2663e-01,  1.2174e-01],\n",
            "          [ 2.3053e-02,  2.0719e-01,  1.2942e-01]],\n",
            "\n",
            "         [[ 6.0225e-02,  1.1275e-01,  4.7050e-02],\n",
            "          [ 1.0161e-01,  7.0648e-02,  1.9872e-01],\n",
            "          [ 1.1844e-01,  2.2188e-01,  8.9410e-02]],\n",
            "\n",
            "         [[ 8.6651e-02,  2.2064e-01,  1.4331e-01],\n",
            "          [ 1.5473e-01,  4.6438e-02,  1.1111e-01],\n",
            "          [ 9.5405e-02,  1.0781e-01,  1.2265e-01]],\n",
            "\n",
            "         [[ 1.6337e-01,  5.5758e-03, -3.5164e-02],\n",
            "          [ 1.1828e-01, -4.0362e-02,  3.6125e-02],\n",
            "          [ 1.5961e-01,  1.3548e-01,  1.7682e-01]],\n",
            "\n",
            "         [[-9.1689e-02, -1.1241e-01, -1.3392e-01],\n",
            "          [-1.6100e-01, -1.4998e-01, -6.0281e-03],\n",
            "          [-7.0023e-02, -2.0429e-02, -2.1653e-02]],\n",
            "\n",
            "         [[ 1.7523e-02,  2.1952e-01,  1.3805e-01],\n",
            "          [ 1.0395e-01,  8.2160e-02,  1.9922e-01],\n",
            "          [ 3.5789e-02, -1.7090e-02,  6.8822e-02]],\n",
            "\n",
            "         [[ 3.4212e-02,  1.2795e-01,  1.6609e-01],\n",
            "          [ 1.8336e-01, -1.4593e-02,  1.8266e-02],\n",
            "          [ 6.2070e-02,  1.0810e-01,  9.7691e-02]]]], device='cuda:0',\n",
            "       dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0386,  0.1978,  0.0411],\n",
            "          [ 0.0401,  0.1521,  0.1195],\n",
            "          [-0.0403,  0.0985, -0.0732]],\n",
            "\n",
            "         [[-0.2245, -0.3517, -0.2341],\n",
            "          [-0.2719, -0.1928, -0.1219],\n",
            "          [-0.1594, -0.1915, -0.0150]],\n",
            "\n",
            "         [[ 0.1872,  0.2213,  0.0283],\n",
            "          [ 0.1961,  0.2076,  0.0286],\n",
            "          [ 0.0469,  0.0594,  0.0235]],\n",
            "\n",
            "         [[-0.0983, -0.2750, -0.2579],\n",
            "          [-0.2496, -0.4641, -0.2907],\n",
            "          [-0.0935, -0.2145, -0.0345]],\n",
            "\n",
            "         [[-0.2368, -0.3386, -0.2068],\n",
            "          [-0.3135, -0.4308, -0.2484],\n",
            "          [-0.0867, -0.2233,  0.0356]],\n",
            "\n",
            "         [[ 0.1464,  0.1572,  0.2002],\n",
            "          [ 0.2590,  0.3280,  0.2185],\n",
            "          [ 0.1990,  0.1132,  0.1094]],\n",
            "\n",
            "         [[ 0.0072,  0.0037,  0.0110],\n",
            "          [ 0.0568, -0.1157, -0.1096],\n",
            "          [-0.0167, -0.1400, -0.0077]],\n",
            "\n",
            "         [[ 0.0377,  0.0982,  0.1018],\n",
            "          [ 0.2388,  0.0532,  0.2031],\n",
            "          [ 0.0615,  0.2283,  0.1464]],\n",
            "\n",
            "         [[ 0.2398,  0.2615,  0.1691],\n",
            "          [ 0.2588,  0.2502,  0.1692],\n",
            "          [ 0.2140,  0.1287,  0.1850]],\n",
            "\n",
            "         [[-0.0368,  0.0272, -0.1024],\n",
            "          [ 0.0394, -0.0167, -0.0901],\n",
            "          [ 0.0469, -0.0317, -0.1048]],\n",
            "\n",
            "         [[ 0.2235,  0.3537,  0.1875],\n",
            "          [ 0.1320,  0.2343,  0.2555],\n",
            "          [ 0.2343,  0.0980,  0.1177]],\n",
            "\n",
            "         [[ 0.2496,  0.2776,  0.2682],\n",
            "          [ 0.3207,  0.1830,  0.2016],\n",
            "          [ 0.2220,  0.1482,  0.1489]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.1527, -0.0319,  0.2847],\n",
            "          [-0.1117,  0.1915,  0.2963],\n",
            "          [-0.1325,  0.1613, -0.1530]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.1289], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.3281, -0.3097,  0.0605],\n",
            "          [-0.0849, -0.1684,  0.2262],\n",
            "          [ 0.2109,  0.0977,  0.1677]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.2102], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[ 0.0530,  0.1826, -0.0495],\n",
            "          [-0.1314, -0.3084, -0.0945],\n",
            "          [ 0.2665,  0.3077,  0.2916]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([0.1474], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([[[[-0.0853, -0.1811, -0.0762],\n",
            "          [ 0.1436,  0.3029, -0.0928],\n",
            "          [ 0.1743, -0.1527,  0.0893]]]], device='cuda:0', dtype=torch.float64)\n",
            "True\n",
            "tensor([-0.1490], device='cuda:0', dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnlctWooTK3",
        "colab_type": "text"
      },
      "source": [
        "# MODEL PRINTING FOR ENC DEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo1apcf2oWJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = torch.randn([1,10,1,64,64]).double()\n",
        "# y = test_model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo39qN4HpFs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from torchviz import make_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRE_QxLIpNy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph = make_dot(y, params = dict(test_model.named_parameters()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMzQ4YMZrw5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d1f8fe17-d072-4318-9c07-62917b617b29"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/data/models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZxUfjhBrZJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph.render(format = 'pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ga1gBkBqiUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-94sHHGnxSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0__frqSBEnF",
        "colab_type": "text"
      },
      "source": [
        "C:\\Users\\Gareth\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([1, 10, 1, 64, 64])) that is different to the input size (torch.Size([1, 10, 2, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
        "  return F.mse_loss(input, target, reduction=self.reduction) \n",
        "  \n",
        "error message from above - why is there an increase in channle size / number?? \n"
      ]
    }
  ]
}