{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_construction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/data_curation/dataset_construction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggKpljorcaJz",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n",
        "We are constructing using the functions in the accompanying .py files as computer doesnt have enough ram to handle full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC7dWRZDg-To",
        "colab_type": "code",
        "outputId": "44b95dc0-ec8e-46a9-c3ab-e5a41562fae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joQx8rOZcK1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import h5py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvT_wvBzfxhR",
        "colab_type": "text"
      },
      "source": [
        "# FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PhZpnw6fzH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def date_to_int_list(date):\n",
        "    # date is in format yyyy-mm-dd\n",
        "\n",
        "    y = int(date[0:4])\n",
        "    m = int(date[5:7])\n",
        "    d = int(date[8:10])\n",
        "\n",
        "#    print(\"date is:\")\n",
        "#    print(y, \" \",m, \" \", d)\n",
        "    return [y,m,d]\n",
        "\n",
        "def monotonic_date(date, baseline = [1989, 1, 1]):\n",
        "\n",
        "    date = date_to_int_list(date)\n",
        "#    print(type(date[0]))\n",
        "#    print(type(baseline[0]))\n",
        "    # turns date since baseline start date into a monotonic function based on\n",
        "    # year and months in line with pgm unit of analysis\n",
        "    return date[1] - baseline[1] + ((date[0] - baseline[0]) * 12)\n",
        "\n",
        "\n",
        "\n",
        "def construct_layer(dataframe, key, prio_key = \"gid\", debug = False):\n",
        "    # returns 360 720 grid layer for a given parameter\n",
        "    # should be given for one parameter per year.\n",
        "    array = np.zeros(360*720)\n",
        "    prio_grid = dataframe[prio_key]\n",
        "    for i in range(len(prio_grid)):\n",
        "        j = prio_grid.iloc[i] - 1\n",
        "        \"\"\"change the below to only be in the case it isnt nan\"\"\" # will this cause problems?\n",
        "        array[j] += dataframe[key].iloc[i]\n",
        "    array.resize(360,720)\n",
        "\n",
        "    return array\n",
        "\n",
        "def construct_combined_sequence(dataframe_prio, dataframe_ucdp, key_list_prio, key_list_ucdp, start = [1989, 1,1], stop = [2014,1,1]):\n",
        "    stop  = '2014-01-01'\n",
        "    # need to adapt ged and other year / month vs ged database for this.\n",
        "    # bool prio to add multiples of 12 to each year usin prio grid.\n",
        "    num_month = monotonic_date(stop, start)\n",
        "    print(num_month)\n",
        "    comb_channel_len = len(key_list_prio) + len(key_list_ucdp)\n",
        "    print(comb_channel_len)\n",
        "    array = np.zeros((num_month, comb_channel_len, 360,720))\n",
        "\n",
        "    stop = date_to_int_list(stop)\n",
        "\n",
        "    month = 0\n",
        "    for i in range(start[0], stop[0]):\n",
        "        for j in range(12): # for each month\n",
        "            # now fill in selected channels as requried.\n",
        "\n",
        "            array[month][:len(key_list_ucdp)] = construct_channels(dataframe_ucdp[dataframe_ucdp.mon_month == month], key_list = key_list_ucdp, prio_key = \"priogrid_gid\")\n",
        "            array[month][len(key_list_ucdp):] = construct_channels(dataframe_prio[dataframe_prio.year == i], key_list = key_list_prio, prio_key = 'gid')\n",
        "            print(month)\n",
        "\n",
        "            month += 1\n",
        "    del month\n",
        "    return array\n",
        "\n",
        "\n",
        "\n",
        "def construct_channels(dataframe, key_list, prio_key = \"gid\"):\n",
        "    # usually used for prio\n",
        "    array = np.zeros((len(key_list), 360, 720))\n",
        "    for i, keys in enumerate(key_list):\n",
        "        array[i] = construct_layer(dataframe, key = keys, prio_key = prio_key)\n",
        "    return array\n",
        "\n",
        "\n",
        "\"\"\"check how we are dealing with cases that go up to and including the final step\"\"\"\n",
        "\n",
        "def construct_sequence(dataframe, key_list, prio_key = 'gid', start = [1989, 1, 1], stop = [2014,1,1], prio = False):\n",
        "    stop  = '2014-01-01'\n",
        "    # need to adapt ged and other year / month vs ged database for this.\n",
        "    # bool prio to add multiples of 12 to each year usin prio grid.\n",
        "    num_month = monotonic_date(stop, start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if prio == False:\n",
        "        # i.e if doing ucdp\n",
        "        # presumes adapted ucdp\n",
        "        # seq length, channels, height, width\n",
        "        array = np.zeros((num_month, len(key_list), 360, 720))\n",
        "\n",
        "        for month in range(num_month):\n",
        "            array[month] = construct_channels(dataframe[dataframe.mon_month == month], key_list = key_list, prio_key = \"priogrid_gid\")\n",
        "\n",
        "            # now for\n",
        "#            array[i] =\n",
        "    elif prio:\n",
        "        stop = [2014,1,1]\n",
        "        array = np.zeros((num_month, len(key_list), 360, 720))\n",
        "        # now for the prio grid data.\n",
        "        # need to make up remainder of start year,\n",
        "        # then multiples of 12 for each year\n",
        "        # then remainder of end year.\n",
        "\n",
        "        \"\"\"this presumes start dates @ start of year no more no less\"\"\"\n",
        "        # need to plus one at the end\n",
        "        month = 0\n",
        "        for i in range(start[0], stop[0]):\n",
        "            for j in range(12): # for each month\n",
        "                array[month] = construct_channels(dataframe[dataframe.year == i], key_list = key_list, prio_key = 'gid')\n",
        "                print(month)\n",
        "\n",
        "                month += 1\n",
        "        del month\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#        start_months = 13 - start[1] # i.e if its 1989,1,1 then there are 12 months left.\n",
        "#        years = stop[0] - start[0]  - 1 # -1 as due to method of making up start months. i.e\n",
        "#        # want [2013,1,1] [2014,1,1] to be dependant of start and stop months and have no year * 12 months additions\n",
        "#        finish_months = stop[1] # stop months\n",
        "#        months_interim = np.arange(start[0], stop[0]+1, 1)\n",
        "#\n",
        "#        ######\n",
        "#\n",
        "#        # now the start month multiples\n",
        "#        for i in range(start_months):\n",
        "#            array[i] = construct_channels(dataframe[dataframe.year == start[0]], key_list = key_list, prio_key = \"gid\")\n",
        "#        # double check prio yearly - try to get monthly values out.\n",
        "#\n",
        "#        for i in range(years):\n",
        "#            for j in range(i *12, (i+1)* 12):\n",
        "#                array[j]\n",
        "\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "def date_column(dataframe, baseline = [1989,1,1]):\n",
        "    # puts new column on dataframe, no need to return.\n",
        "    # date start just as dummy atm\n",
        "#    dataframe = dataframe[\"date_start\"]\n",
        "    vals = dataframe[\"date_start\"].values\n",
        "    new_col = np.array([monotonic_date(string_date) for string_date in vals])\n",
        "    dataframe[\"mon_month\"] = new_col\n",
        "\n",
        "def h5py_conversion(data_array, filename, key_list_ucdp, key_list_prio):\n",
        "    # this is for saving the default 360:720 file to chop out of.\n",
        "    # lazy loading saves the day\n",
        "    # all day\n",
        "    # every day\n",
        "    f = h5py.File(\"{}.hdf5\".format(filename), \"w\")\n",
        "\n",
        "    f.create_dataset(\"data_combined\", data = data_array)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "    csv = open(name + \"_config.csv\", 'w')\n",
        "    csv.write(\"Included data UCDP:\\n\")\n",
        "    for key in key_list_ucdp:\n",
        "        csv.write(key + \"\\n\")\n",
        "\n",
        "    csv.write(\"Included data PRIO:\\n\")\n",
        "    for key in key_list_prio:\n",
        "        csv.write(key + \"\\n\")\n",
        "    csv.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdH8e_Alf5r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raster_test(input_data, chunk_size = 16):\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            print(input_data[:,i:i+chunk_size,j:j + chunk_size])\n",
        "            print(\".\")\n",
        "\n",
        "#    plt.imshow(input_data)\n",
        "\n",
        "def raster_selection(input_data, chunk_size = 16):\n",
        "    # here input_data is sequence step.\n",
        "    # data should be of dimensions seq, channels, height, width.\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    # this is not efficient.\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            input_data[0][i:i+chunk_size,j:j + chunk_size]\n",
        "\n",
        "    plt.imshow(input_data)\n",
        "\n",
        "\n",
        "def random_pixel_bounds(i, j, chunk_size = 16):\n",
        "    # returns the bounds of the image to select with a random pixel size.\n",
        "\n",
        "    height = random.randint(0, chunk_size-1)\n",
        "    width = random.randint(0, chunk_size-1)\n",
        "    # this randomly generates a of the image for where the pixel may be located\n",
        "    # randomly in the cut out image.\n",
        "    i_lower = i - height\n",
        "    i_upper = i + (chunk_size - height)\n",
        "\n",
        "    j_lower = j - width\n",
        "    j_upper = j + (chunk_size - width)\n",
        "\n",
        "    return [i_lower, i_upper, j_lower, j_upper]\n",
        "\n",
        "def random_selection(image, i, j, chunk_size = 16):\n",
        "\n",
        "    i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size = chunk_size)\n",
        "\n",
        "    print(image[i_lower:i_upper,j_lower:j_upper])\n",
        "\n",
        "def random_grid_selection(image, sequence_step, chunk_size= 16, draws = 5, debug = True):\n",
        "    if debug:\n",
        "        print(\"Image shape is:\" , image.shape)\n",
        "\n",
        "\n",
        "    # decide if this is going to be h5py loaded.\n",
        "\n",
        "    # decide what sequence step is going to be like and how to return it\n",
        "\n",
        "    # image is seq, channels, height, width\n",
        "\n",
        "    # here we are using a seq length of 10. - could use 12 but atm we go for 10.\n",
        "\n",
        "    # sequence step is the step at which the TRUTH is being extracted. the predictor sequence\n",
        "    # is extracted from the 10 preceding steps. be careful to send in from i > 11\n",
        "    assert sequence_step > 10, (\"This function selects the datapoints from this test set that contain\"\n",
        "                                \"a conflict event and then selects predictor data from the 10 preceding steps\"\n",
        "                                \" as a result i > 10 must be true\")\n",
        "    # for sequence step, 0th layer - i.e fatalities\n",
        "    y, x = np.where(image[sequence_step][0] >= 1)\n",
        "\n",
        "    if debug:\n",
        "        print(x.shape)\n",
        "\n",
        "    truth_list = []\n",
        "    predictor_list = []\n",
        "\n",
        "    # re arange for loops for speed?\n",
        "    for i,j in zip(y, x): # now over sites where fatalities have occured\n",
        "        for _ in range(draws):\n",
        "            i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size=chunk_size)\n",
        "\n",
        "            # now need to work out how to store these. how to stack ontop ect.\n",
        "            truth = image[sequence_step][0,i_lower:i_upper,j_lower:j_upper]\n",
        "            # check these dimensions\n",
        "            predictors = image[i-10:i, :,i_lower:i_upper,j_lower:j_upper]\n",
        "\n",
        "            truth_list.append(truth)\n",
        "            predictor_list.append(predictors)\n",
        "\n",
        "    # finally we combine the previous arrays.\n",
        "\n",
        "    return np.stack(predictor_list, axis= 0), np.stack(truth_list, axis = 0)\n",
        "\n",
        "\n",
        "def full_dataset_numpy(image, chunk_size = 16, draws = 5, debug = False):\n",
        "    # image is seq, channels, height, width\n",
        "    predictor_list = []\n",
        "    truth_list = []\n",
        "    for i in range(11, len(image)):\n",
        "        t1, t2 = random_grid_selection(image, i)\n",
        "        predictor_list.append(t1)\n",
        "        truth_list.append(t2)\n",
        "\n",
        "    truth_np = np.concatenate(truth_list, axis = 0)\n",
        "    predictor_np = np.concatenate(predictor_list, axis =0)\n",
        "    return predictor_np, truth_np\n",
        "\n",
        "def quick_dataset(data, name):\n",
        "    f = h5py.File(name + \".hdf5\", \"w\")\n",
        "    f.create_dataset(\"main\", data = data)\n",
        "#    f.create_dataset(\"truth\", data = truth)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "def debug_func1(dataframe, month):\n",
        "    a = dataframe[dataframe.mon_month == month]\n",
        "    print(len(a[a.best >0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WG8VQ4Of_A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raster_test(input_data, chunk_size = 16):\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            print(input_data[:,i:i+chunk_size,j:j + chunk_size])\n",
        "            print(\".\")\n",
        "\n",
        "#    plt.imshow(input_data)\n",
        "\n",
        "def raster_selection(input_data, chunk_size = 16):\n",
        "    # here input_data is sequence step.\n",
        "    # data should be of dimensions seq, channels, height, width.\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    # this is not efficient.\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            input_data[0][i:i+chunk_size,j:j + chunk_size]\n",
        "\n",
        "    plt.imshow(input_data)\n",
        "\n",
        "\n",
        "def random_pixel_bounds(i, j, chunk_size = 16):\n",
        "    # returns the bounds of the image to select with a random pixel size.\n",
        "\n",
        "    height = random.randint(0, chunk_size-1)\n",
        "    width = random.randint(0, chunk_size-1)\n",
        "    # this randomly generates a of the image for where the pixel may be located\n",
        "    # randomly in the cut out image.\n",
        "    i_lower = i - height\n",
        "    i_upper = i + (chunk_size - height)\n",
        "\n",
        "    j_lower = j - width\n",
        "    j_upper = j + (chunk_size - width)\n",
        "\n",
        "    return [i_lower, i_upper, j_lower, j_upper]\n",
        "\n",
        "def random_selection(image, i, j, chunk_size = 16):\n",
        "\n",
        "    i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size = chunk_size)\n",
        "\n",
        "    print(image[i_lower:i_upper,j_lower:j_upper])\n",
        "\n",
        "def random_grid_selection(image, sequence_step, chunk_size= 16, draws = 5, debug = True):\n",
        "    if debug:\n",
        "        print(\"Image shape is:\" , image.shape)\n",
        "\n",
        "\n",
        "    # decide if this is going to be h5py loaded.\n",
        "\n",
        "    # decide what sequence step is going to be like and how to return it\n",
        "\n",
        "    # image is seq, channels, height, width\n",
        "\n",
        "    # here we are using a seq length of 10. - could use 12 but atm we go for 10.\n",
        "\n",
        "    # sequence step is the step at which the TRUTH is being extracted. the predictor sequence\n",
        "    # is extracted from the 10 preceding steps. be careful to send in from i > 11\n",
        "    assert sequence_step > 10, (\"This function selects the datapoints from this test set that contain\"\n",
        "                                \"a conflict event and then selects predictor data from the 10 preceding steps\"\n",
        "                                \" as a result i > 10 must be true\")\n",
        "    # for sequence step, 0th layer - i.e fatalities\n",
        "    y, x = np.where(image[sequence_step][0] >= 1)\n",
        "\n",
        "    if debug:\n",
        "        print(x.shape)\n",
        "\n",
        "    truth_list = []\n",
        "    predictor_list = []\n",
        "\n",
        "    # re arange for loops for speed?\n",
        "    for i,j in zip(y, x): # now over sites where fatalities have occured\n",
        "        for _ in range(draws):\n",
        "            i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size=chunk_size)\n",
        "\n",
        "            # now need to work out how to store these. how to stack ontop ect.\n",
        "            truth = image[sequence_step][0,i_lower:i_upper,j_lower:j_upper]\n",
        "            # check these dimensions\n",
        "            predictors = image[i-10:i, :,i_lower:i_upper,j_lower:j_upper]\n",
        "\n",
        "            truth_list.append(truth)\n",
        "            predictor_list.append(predictors)\n",
        "\n",
        "    # finally we combine the previous arrays.\n",
        "\n",
        "    return np.stack(predictor_list, axis= 0), np.stack(truth_list, axis = 0)\n",
        "\n",
        "\n",
        "def full_dataset_numpy(image, chunk_size = 16, draws = 5, debug = False):\n",
        "    # image is seq, channels, height, width\n",
        "    predictor_list = []\n",
        "    truth_list = []\n",
        "    for i in range(11, len(image)):\n",
        "        t1, t2 = random_grid_selection(image, i)\n",
        "        predictor_list.append(t1)\n",
        "        truth_list.append(t2)\n",
        "\n",
        "    truth_np = np.concatenate(truth_list, axis = 0)\n",
        "    predictor_np = np.concatenate(predictor_list, axis =0)\n",
        "    return predictor_np, truth_np\n",
        "\n",
        "def quick_dataset(data, name):\n",
        "    f = h5py.File(name + \".hdf5\", \"w\")\n",
        "    f.create_dataset(\"main\", data = data)\n",
        "#    f.create_dataset(\"truth\", data = truth)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "def debug_func1(dataframe, month):\n",
        "    a = dataframe[dataframe.mon_month == month]\n",
        "    print(len(a[a.best >0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t14HJK9Pgk0E",
        "colab_type": "text"
      },
      "source": [
        "# LOADING DATA AND FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JDVKJ6UhCcU",
        "colab_type": "code",
        "outputId": "9ecf530e-00fd-40a4-9078-7a5950b5faf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KexhPxtxkUTD",
        "colab_type": "code",
        "outputId": "e114d4ed-0371-492c-e561-8f3fe9cc34aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrgYmVdlK8q",
        "colab_type": "text"
      },
      "source": [
        "load in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEibi2zhgrbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_prio = pd.read_csv(\"data/PRIO-GRID Yearly Variables for 1946-2014 - 2019-07-26.csv\")\n",
        "data_ucdp = pd.read_csv(\"data/ged191.csv\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyXn5extlKDV",
        "colab_type": "text"
      },
      "source": [
        "add monotonic month column to the ucdp data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjBSL8xNgycL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date_column(data_ucdp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWId5IE1lPVf",
        "colab_type": "text"
      },
      "source": [
        "construct full image sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWa9_B-qnD4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = construct_combined_sequence(data_prio, data_ucdp, key_list_prio=['prec_gpcc','pop_hyd_sum'], key_list_ucdp=['best'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7TspZujnGPy",
        "colab_type": "text"
      },
      "source": [
        "now put this into our random selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ui6KeUhnD76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a, b = full_dataset_numpy(test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}