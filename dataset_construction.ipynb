{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_construction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/data_curation/dataset_construction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggKpljorcaJz",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n",
        "We are constructing using the functions in the accompanying .py files as computer doesnt have enough ram to handle full dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC7dWRZDg-To",
        "colab_type": "code",
        "outputId": "d69d2d8e-c757-4e6c-821c-e246eb30d385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joQx8rOZcK1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvT_wvBzfxhR",
        "colab_type": "text"
      },
      "source": [
        "# FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PhZpnw6fzH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def date_to_int_list(date):\n",
        "    # date is in format yyyy-mm-dd\n",
        "\n",
        "    y = int(date[0:4])\n",
        "    m = int(date[5:7])\n",
        "    d = int(date[8:10])\n",
        "\n",
        "#    print(\"date is:\")\n",
        "#    print(y, \" \",m, \" \", d)\n",
        "    return [y,m,d]\n",
        "\n",
        "def monotonic_date(date, baseline = [1989, 1, 1]):\n",
        "\n",
        "    date = date_to_int_list(date)\n",
        "#    print(type(date[0]))\n",
        "#    print(type(baseline[0]))\n",
        "    # turns date since baseline start date into a monotonic function based on\n",
        "    # year and months in line with pgm unit of analysis\n",
        "    return date[1] - baseline[1] + ((date[0] - baseline[0]) * 12)\n",
        "\n",
        "\n",
        "\n",
        "def construct_layer(dataframe, key, prio_key = \"gid\", debug = False):\n",
        "    # returns 360 720 grid layer for a given parameter\n",
        "    # should be given for one parameter per year.\n",
        "    array = np.zeros(360*720)\n",
        "    prio_grid = dataframe[prio_key]\n",
        "    for i in range(len(prio_grid)):\n",
        "        j = prio_grid.iloc[i] - 1\n",
        "        \"\"\"change the below to only be in the case it isnt nan\"\"\" # will this cause problems?\n",
        "        array[j] += dataframe[key].iloc[i]\n",
        "    array.resize(360,720)\n",
        "\n",
        "    return array\n",
        "\n",
        "def construct_combined_sequence(dataframe_prio, dataframe_ucdp, key_list_prio, key_list_ucdp, start = [1989, 1,1], stop = [2014,1,1]):\n",
        "    stop  = '2014-01-01'\n",
        "    # need to adapt ged and other year / month vs ged database for this.\n",
        "    # bool prio to add multiples of 12 to each year usin prio grid.\n",
        "    num_month = monotonic_date(stop, start)\n",
        "    print(num_month)\n",
        "    comb_channel_len = len(key_list_prio) + len(key_list_ucdp)\n",
        "    print(comb_channel_len)\n",
        "    array = np.zeros((num_month, comb_channel_len, 360,720))\n",
        "\n",
        "    stop = date_to_int_list(stop)\n",
        "\n",
        "    month = 0\n",
        "    for i in range(start[0], stop[0]):\n",
        "        for j in range(12): # for each month\n",
        "            # now fill in selected channels as requried.\n",
        "\n",
        "            array[month][:len(key_list_ucdp)] = construct_channels(dataframe_ucdp[dataframe_ucdp.mon_month == month], key_list = key_list_ucdp, prio_key = \"priogrid_gid\")\n",
        "            array[month][len(key_list_ucdp):] = construct_channels(dataframe_prio[dataframe_prio.year == i], key_list = key_list_prio, prio_key = 'gid')\n",
        "            print(month)\n",
        "\n",
        "            month += 1\n",
        "    del month\n",
        "    return array\n",
        "\n",
        "\n",
        "\n",
        "def construct_channels(dataframe, key_list, prio_key = \"gid\"):\n",
        "    # usually used for prio\n",
        "    array = np.zeros((len(key_list), 360, 720))\n",
        "    for i, keys in enumerate(key_list):\n",
        "        array[i] = construct_layer(dataframe, key = keys, prio_key = prio_key)\n",
        "    return array\n",
        "\n",
        "\n",
        "\"\"\"check how we are dealing with cases that go up to and including the final step\"\"\"\n",
        "\n",
        "def construct_sequence(dataframe, key_list, prio_key = 'gid', start = [1989, 1, 1], stop = [2014,1,1], prio = False):\n",
        "    stop  = '2014-01-01'\n",
        "    # need to adapt ged and other year / month vs ged database for this.\n",
        "    # bool prio to add multiples of 12 to each year usin prio grid.\n",
        "    num_month = monotonic_date(stop, start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if prio == False:\n",
        "        # i.e if doing ucdp\n",
        "        # presumes adapted ucdp\n",
        "        # seq length, channels, height, width\n",
        "        array = np.zeros((num_month, len(key_list), 360, 720))\n",
        "\n",
        "        for month in range(num_month):\n",
        "            array[month] = construct_channels(dataframe[dataframe.mon_month == month], key_list = key_list, prio_key = \"priogrid_gid\")\n",
        "\n",
        "            # now for\n",
        "#            array[i] =\n",
        "    elif prio:\n",
        "        stop = [2014,1,1]\n",
        "        array = np.zeros((num_month, len(key_list), 360, 720))\n",
        "        # now for the prio grid data.\n",
        "        # need to make up remainder of start year,\n",
        "        # then multiples of 12 for each year\n",
        "        # then remainder of end year.\n",
        "\n",
        "        \"\"\"this presumes start dates @ start of year no more no less\"\"\"\n",
        "        # need to plus one at the end\n",
        "        month = 0\n",
        "        for i in range(start[0], stop[0]):\n",
        "            for j in range(12): # for each month\n",
        "                array[month] = construct_channels(dataframe[dataframe.year == i], key_list = key_list, prio_key = 'gid')\n",
        "                print(month)\n",
        "\n",
        "                month += 1\n",
        "        del month\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#        start_months = 13 - start[1] # i.e if its 1989,1,1 then there are 12 months left.\n",
        "#        years = stop[0] - start[0]  - 1 # -1 as due to method of making up start months. i.e\n",
        "#        # want [2013,1,1] [2014,1,1] to be dependant of start and stop months and have no year * 12 months additions\n",
        "#        finish_months = stop[1] # stop months\n",
        "#        months_interim = np.arange(start[0], stop[0]+1, 1)\n",
        "#\n",
        "#        ######\n",
        "#\n",
        "#        # now the start month multiples\n",
        "#        for i in range(start_months):\n",
        "#            array[i] = construct_channels(dataframe[dataframe.year == start[0]], key_list = key_list, prio_key = \"gid\")\n",
        "#        # double check prio yearly - try to get monthly values out.\n",
        "#\n",
        "#        for i in range(years):\n",
        "#            for j in range(i *12, (i+1)* 12):\n",
        "#                array[j]\n",
        "\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "def date_column(dataframe, baseline = [1989,1,1]):\n",
        "    # puts new column on dataframe, no need to return.\n",
        "    # date start just as dummy atm\n",
        "#    dataframe = dataframe[\"date_start\"]\n",
        "    vals = dataframe[\"date_start\"].values\n",
        "    new_col = np.array([monotonic_date(string_date) for string_date in vals])\n",
        "    dataframe[\"mon_month\"] = new_col\n",
        "\n",
        "def h5py_conversion(data_array, filename, key_list_ucdp, key_list_prio):\n",
        "    # this is for saving the default 360:720 file to chop out of.\n",
        "    # lazy loading saves the day\n",
        "    # all day\n",
        "    # every day\n",
        "    f = h5py.File(\"{}.hdf5\".format(filename), \"w\")\n",
        "\n",
        "    f.create_dataset(\"data_combined\", data = data_array)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "    csv = open(name + \"_config.csv\", 'w')\n",
        "    csv.write(\"Included data UCDP:\\n\")\n",
        "    for key in key_list_ucdp:\n",
        "        csv.write(key + \"\\n\")\n",
        "\n",
        "    csv.write(\"Included data PRIO:\\n\")\n",
        "    for key in key_list_prio:\n",
        "        csv.write(key + \"\\n\")\n",
        "    csv.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdH8e_Alf5r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raster_test(input_data, chunk_size = 16):\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            print(input_data[:,i:i+chunk_size,j:j + chunk_size])\n",
        "            print(\".\")\n",
        "\n",
        "#    plt.imshow(input_data)\n",
        "\n",
        "def raster_selection(input_data, chunk_size = 16):\n",
        "    # here input_data is sequence step.\n",
        "    # data should be of dimensions seq, channels, height, width.\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    # this is not efficient.\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            input_data[0][i:i+chunk_size,j:j + chunk_size]\n",
        "\n",
        "    plt.imshow(input_data)\n",
        "\n",
        "\n",
        "def random_pixel_bounds(i, j, chunk_size = 16):\n",
        "    # returns the bounds of the image to select with a random pixel size.\n",
        "\n",
        "    height = random.randint(0, chunk_size-1)\n",
        "    width = random.randint(0, chunk_size-1)\n",
        "    # this randomly generates a of the image for where the pixel may be located\n",
        "    # randomly in the cut out image.\n",
        "    i_lower = i - height\n",
        "    i_upper = i + (chunk_size - height)\n",
        "\n",
        "    j_lower = j - width\n",
        "    j_upper = j + (chunk_size - width)\n",
        "\n",
        "    return [i_lower, i_upper, j_lower, j_upper]\n",
        "\n",
        "def random_selection(image, i, j, chunk_size = 16):\n",
        "\n",
        "    i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size = chunk_size)\n",
        "\n",
        "    print(image[i_lower:i_upper,j_lower:j_upper])\n",
        "\n",
        "def random_grid_selection(image, sequence_step, chunk_size= 16, draws = 5, debug = True):\n",
        "    if debug:\n",
        "        print(\"Image shape is:\" , image.shape)\n",
        "\n",
        "\n",
        "    # decide if this is going to be h5py loaded.\n",
        "\n",
        "    # decide what sequence step is going to be like and how to return it\n",
        "\n",
        "    # image is seq, channels, height, width\n",
        "\n",
        "    # here we are using a seq length of 10. - could use 12 but atm we go for 10.\n",
        "\n",
        "    # sequence step is the step at which the TRUTH is being extracted. the predictor sequence\n",
        "    # is extracted from the 10 preceding steps. be careful to send in from i > 11\n",
        "    assert sequence_step > 10, (\"This function selects the datapoints from this test set that contain\"\n",
        "                                \"a conflict event and then selects predictor data from the 10 preceding steps\"\n",
        "                                \" as a result i > 10 must be true\")\n",
        "    # for sequence step, 0th layer - i.e fatalities\n",
        "    y, x = np.where(image[sequence_step][0] >= 1)\n",
        "\n",
        "    if debug:\n",
        "        print(x.shape)\n",
        "\n",
        "    truth_list = []\n",
        "    predictor_list = []\n",
        "\n",
        "    # re arange for loops for speed?\n",
        "    for i,j in zip(y, x): # now over sites where fatalities have occured\n",
        "        for _ in range(draws):\n",
        "            i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size=chunk_size)\n",
        "\n",
        "            # now need to work out how to store these. how to stack ontop ect.\n",
        "            truth = image[sequence_step][0,i_lower:i_upper,j_lower:j_upper]\n",
        "            # check these dimensions\n",
        "            predictors = image[i-10:i, :,i_lower:i_upper,j_lower:j_upper]\n",
        "\n",
        "            truth_list.append(truth)\n",
        "            predictor_list.append(predictors)\n",
        "\n",
        "    # finally we combine the previous arrays.\n",
        "\n",
        "    return np.stack(predictor_list, axis= 0), np.stack(truth_list, axis = 0)\n",
        "\n",
        "\n",
        "def full_dataset_numpy(image, chunk_size = 16, draws = 5, debug = False):\n",
        "    # image is seq, channels, height, width\n",
        "    predictor_list = []\n",
        "    truth_list = []\n",
        "    for i in range(11, len(image)):\n",
        "        t1, t2 = random_grid_selection(image, i)\n",
        "        predictor_list.append(t1)\n",
        "        truth_list.append(t2)\n",
        "\n",
        "    truth_np = np.concatenate(truth_list, axis = 0)\n",
        "    predictor_np = np.concatenate(predictor_list, axis =0)\n",
        "    return predictor_np, truth_np\n",
        "\n",
        "def quick_dataset(data, name):\n",
        "    f = h5py.File(name + \".hdf5\", \"w\")\n",
        "    f.create_dataset(\"main\", data = data)\n",
        "#    f.create_dataset(\"truth\", data = truth)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "def debug_func1(dataframe, month):\n",
        "    a = dataframe[dataframe.mon_month == month]\n",
        "    print(len(a[a.best >0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WG8VQ4Of_A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raster_test(input_data, chunk_size = 16):\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            print(input_data[:,i:i+chunk_size,j:j + chunk_size])\n",
        "            print(\".\")\n",
        "\n",
        "#    plt.imshow(input_data)\n",
        "\n",
        "def raster_selection(input_data, chunk_size = 16):\n",
        "    # here input_data is sequence step.\n",
        "    # data should be of dimensions seq, channels, height, width.\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    # this is not efficient.\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            input_data[0][i:i+chunk_size,j:j + chunk_size]\n",
        "\n",
        "    plt.imshow(input_data)\n",
        "\n",
        "\n",
        "def random_pixel_bounds(i, j, chunk_size = 16):\n",
        "    # returns the bounds of the image to select with a random pixel size.\n",
        "\n",
        "    height = random.randint(0, chunk_size-1)\n",
        "    width = random.randint(0, chunk_size-1)\n",
        "    # this randomly generates a of the image for where the pixel may be located\n",
        "    # randomly in the cut out image.\n",
        "    i_lower = i - height\n",
        "    i_upper = i + (chunk_size - height)\n",
        "\n",
        "    j_lower = j - width\n",
        "    j_upper = j + (chunk_size - width)\n",
        "\n",
        "    return [i_lower, i_upper, j_lower, j_upper]\n",
        "\n",
        "def random_selection(image, i, j, chunk_size = 16):\n",
        "\n",
        "    i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size = chunk_size)\n",
        "\n",
        "    print(image[i_lower:i_upper,j_lower:j_upper])\n",
        "\n",
        "def random_grid_selection(image, sequence_step, chunk_size= 16, draws = 5, debug = True):\n",
        "    if debug:\n",
        "        print(\"Image shape is:\" , image.shape)\n",
        "\n",
        "\n",
        "    # decide if this is going to be h5py loaded.\n",
        "\n",
        "    # decide what sequence step is going to be like and how to return it\n",
        "\n",
        "    # image is seq, channels, height, width\n",
        "\n",
        "    # here we are using a seq length of 10. - could use 12 but atm we go for 10.\n",
        "\n",
        "    # sequence step is the step at which the TRUTH is being extracted. the predictor sequence\n",
        "    # is extracted from the 10 preceding steps. be careful to send in from i > 11\n",
        "    assert sequence_step > 10, (\"This function selects the datapoints from this test set that contain\"\n",
        "                                \"a conflict event and then selects predictor data from the 10 preceding steps\"\n",
        "                                \" as a result i > 10 must be true\")\n",
        "    # for sequence step, 0th layer - i.e fatalities\n",
        "    y, x = np.where(image[sequence_step][0] >= 1)\n",
        "\n",
        "    if debug:\n",
        "        print(x.shape)\n",
        "\n",
        "    truth_list = []\n",
        "    predictor_list = []\n",
        "\n",
        "    # re arange for loops for speed?\n",
        "    for i,j in zip(y, x): # now over sites where fatalities have occured\n",
        "        for _ in range(draws):\n",
        "            i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size=chunk_size)\n",
        "\n",
        "            # now need to work out how to store these. how to stack ontop ect.\n",
        "            truth = image[sequence_step][0,i_lower:i_upper,j_lower:j_upper]\n",
        "            # check these dimensions\n",
        "            predictors = image[i-10:i, :,i_lower:i_upper,j_lower:j_upper]\n",
        "\n",
        "            truth_list.append(truth)\n",
        "            predictor_list.append(predictors)\n",
        "\n",
        "    # finally we combine the previous arrays.\n",
        "\n",
        "    return np.stack(predictor_list, axis= 0), np.stack(truth_list, axis = 0)\n",
        "\n",
        "\n",
        "def full_dataset_numpy(image, chunk_size = 16, draws = 5, debug = False):\n",
        "    # image is seq, channels, height, width\n",
        "    predictor_list = []\n",
        "    truth_list = []\n",
        "    for i in range(11, len(image)):\n",
        "        t1, t2 = random_grid_selection(image, i)\n",
        "        predictor_list.append(t1)\n",
        "        truth_list.append(t2)\n",
        "\n",
        "    truth_np = np.concatenate(truth_list, axis = 0)\n",
        "    predictor_np = np.concatenate(predictor_list, axis =0)\n",
        "    return predictor_np, truth_np\n",
        "\n",
        "def quick_dataset(data, name):\n",
        "    f = h5py.File(name + \".hdf5\", \"w\")\n",
        "    f.create_dataset(\"main\", data = data)\n",
        "#    f.create_dataset(\"truth\", data = truth)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "def debug_func1(dataframe, month):\n",
        "    a = dataframe[dataframe.mon_month == month]\n",
        "    print(len(a[a.best >0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfrOf9qBMADY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_event_column(dataframe):\n",
        "    \"\"\" as it is hard to encode categorical information about battles when\n",
        "    battle deaths = 0, we then encode a binary - 0, 1 layer pertaining to\n",
        "    whether an event took place\"\"\"\n",
        "    new_col = np.ones(len(dataframe))\n",
        "    dataframe[\"binary_event\"] = new_col\n",
        "\n",
        "def nan_to_one(dataframe, key):\n",
        "    \"\"\"takes column from dataframe\"\"\"\n",
        "    dataframe[key] = dataframe[key].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlgt1oO5J14F",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t14HJK9Pgk0E",
        "colab_type": "text"
      },
      "source": [
        "# LOADING DATA AND FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JDVKJ6UhCcU",
        "colab_type": "code",
        "outputId": "6131c3f5-7a4c-4a33-eeda-312b404de6b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KexhPxtxkUTD",
        "colab_type": "code",
        "outputId": "0cf0151c-f2aa-46c0-98a8-8b7bd9da075a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrgYmVdlK8q",
        "colab_type": "text"
      },
      "source": [
        "load in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEibi2zhgrbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_prio = pd.read_csv(\"data/PRIO-GRID Yearly Variables for 1946-2014 - 2019-07-26.csv\")\n",
        "data_ucdp = pd.read_csv(\"data/ged191.csv\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeDqDYZRFaXa",
        "colab_type": "text"
      },
      "source": [
        "sub selecting only the african region - just to test for simplicity - compare training on both at a later date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soxg4jfzFiOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "north = 37.32\n",
        "south = -34.5115\n",
        "west = -17.3113\n",
        "east = 51.2752\n",
        "\n",
        "data_ucdp = data_ucdp[(data_ucdp.latitude >= south) & (data_ucdp.latitude <= north) & (data_ucdp.longitude >= west) & (east >= data_ucdp.longitude)]\n",
        "#data = dummy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyXn5extlKDV",
        "colab_type": "text"
      },
      "source": [
        "add monotonic month column to the ucdp data, turn nans into 1s for petroleum and drugs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjBSL8xNgycL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date_column(data_ucdp)\n",
        "binary_event_column(data_ucdp)\n",
        "nan_to_one(data_prio, \"petroleum_y\")\n",
        "nan_to_one(data_prio, \"drug_y\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWId5IE1lPVf",
        "colab_type": "text"
      },
      "source": [
        "construct full image sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYxGEKVANpJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_list_prio = [\"gcp_ppp\", \"petroleum_y\",\"drug_y\", \"prec_gpcp\"] # not temp - needs better imputation\n",
        "# excluded also useful - talk to nils about exclusion.\n",
        "\n",
        "# no mountainous regions so far\n",
        "key_list_ucdp = [\"best\", \"binary_event\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWa9_B-qnD4g",
        "colab_type": "code",
        "outputId": "fb003f98-b417-4f63-dd16-52211d8b7d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "test = construct_combined_sequence(data_prio, data_ucdp, key_list_prio=key_list_prio, key_list_ucdp=key_list_ucdp)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n",
            "6\n",
            "0\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bf9b76b01c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_combined_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_ucdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_list_prio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_list_prio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_list_ucdp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_list_ucdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-2a33a23a41e9>\u001b[0m in \u001b[0;36mconstruct_combined_sequence\u001b[0;34m(dataframe_prio, dataframe_ucdp, key_list_prio, key_list_ucdp, start, stop)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_list_ucdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe_ucdp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataframe_ucdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmon_month\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_list_ucdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprio_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"priogrid_gid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_list_ucdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe_prio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataframe_prio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_list_prio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprio_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2a33a23a41e9>\u001b[0m in \u001b[0;36mconstruct_channels\u001b[0;34m(dataframe, key_list, prio_key)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprio_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprio_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2a33a23a41e9>\u001b[0m in \u001b[0;36mconstruct_layer\u001b[0;34m(dataframe, key, prio_key, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprio_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m\"\"\"change the below to only be in the case it isnt nan\"\"\"\u001b[0m \u001b[0;31m# will this cause problems?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m720\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mis_list_like_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;31m# allow a list_like, but exclude NamedTuples which can be indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2691\u001b[0;31m     return is_list_like(key) and not (isinstance(key, tuple) and\n\u001b[0m\u001b[1;32m   2692\u001b[0m                                       type(key) is not tuple)\n\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/inference.py\u001b[0m in \u001b[0;36mis_list_like\u001b[0;34m(obj, allow_sets)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     return (isinstance(obj, compat.Iterable)\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0;31m# we do not count strings/unicode/bytes as list-like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_and_binary_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    188\u001b[0m             if (cls._abc_negative_cache_version ==\n\u001b[1;32m    189\u001b[0m                 \u001b[0mABCMeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abc_invalidation_counter\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 subclass in cls._abc_negative_cache):\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Fall back to the subclass check.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_weakrefset.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pending_removals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mwr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at14JdkSmO_4",
        "colab_type": "text"
      },
      "source": [
        "saving "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErmEch2UmOfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"array_1\", test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7TspZujnGPy",
        "colab_type": "text"
      },
      "source": [
        "now put this into our random selection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWTELI9pmexd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pvjTDDmgpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_dataset_h5py(image, filename, chunk_size = 16, draws = 5, debug = False):\n",
        "    \"\"\" dataset is too large to combine in 12gb of ram - need to combine in h5py\n",
        "    array. i.e lazy saving as well as lazy loading\"\"\"\n",
        "    \n",
        "    with h5py.File(filename + \".hdf5\", 'w') as f:\n",
        "        for i in range(11, len(image)):\n",
        "            t1, t2 = random_grid_selection(image, i)\n",
        "            if i == 11:\n",
        "                # creat h5py file at first step.\n",
        "                f.create_dataset('predictor', data= t1, maxshape=(None,None, None, None,None)) # compression=\"gzip\", chunks=True, taken out\n",
        "                f.create_dataset(\"truth\", data= t2, maxshape=(None,None,None))\n",
        "        \n",
        "            else:\n",
        "                f[\"predictor\"].resize((f[\"predictor\"].shape[0] + t1.shape[0]), axis = 0) # expand dataset\n",
        "                f[\"truth\"].resize((f[\"truth\"].shape[0] + t2.shape[0]), axis = 0)\n",
        "            \n",
        "                f[\"predictor\"][-t1.shape[0]:] = t1 # place new data in expanded dataset\n",
        "                f[\"truth\"][-t2.shape[0]:] = t2\n",
        "    \n",
        "#     f.close()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ui6KeUhnD76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a, b = full_dataset_numpy(test)\n",
        "full_dataset_h5py(test, \"data_prio_run_test\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx6RkdVjnZt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "f = h5py.File(\"test_run_full_dataset3.hdf5\", 'r')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t__yhKfhtf0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f[\"predictor\"].shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu6Z80fLtuN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f[\"predictor\"].shape\n",
        "\n",
        "for i in range(10):\n",
        "    plt.figure()\n",
        "    plt.imshow(f[\"predictor\"][100000][i][0], vmax = 1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46NHJR9i2xHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f[\"predictor\"].shape\n",
        "k = 0\n",
        "for i in range(248580):\n",
        "    k +=1\n",
        "print(\"done\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW4M0YkrDblm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(np.where(f[\"predictor\"][100000][:,0] > 0)[0])\n",
        "# f[\"predictor\"][100000][:,0]\n",
        "# plt.imshow(f[\"predictor\"][100000][:,0][0])\n",
        "len([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8En8z9VL3uKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def data_set_analysis(dataset):\n",
        "    dat = np.zeros(len(dataset[\"predictor\"]))\n",
        "    for i in range(len(dataset[\"predictor\"])):\n",
        "        dat[i] = len(np.where(f[\"predictor\"][i][:,0] > 0)[0])\n",
        "        if (i % 1000) == 0:\n",
        "            print(i)\n",
        "    return dat\n",
        "\n",
        "dats = data_set_analysis(f)\n",
        "np.save(\"dats_saved\", dats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELhdQd4BCfjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save(\"dats_saved\", dats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFlE86e8Co5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique(dats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtGK9LexQXXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot = sns.distplot(dats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcImFZYhQ18D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plot.get_figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtVXvcc7RZQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure.savefig(\"seabornplot.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi7dl5t-3nYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f[\"predictor\"][0][:,0][0].shape\n",
        "plt.imshow(f[\"predictor\"][0][:,0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}