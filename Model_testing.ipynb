{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/Model_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "outputId": "4cecc29e-e076-4a0c-d356-ccaf5509ec1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "outputId": "331b3bee-9caa-4e94-9ff6-00c8b02a832e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=58a401f51797492246f6bc471d0c5df89175893b2192e3a8545eaca9f0fd0c50\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "outputId": "cf6d8e67-775b-47d1-bf1f-f8edb3f35aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.066s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "outputId": "fcbee595-e738-4d5e-dcfc-581745ae9851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    print(\"GPUs:\", torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "outputId": "54c62629-8633-45ae-97f2-97a9366b6457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "#         truth -= self.avg[0]\n",
        "#         truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7, threshold = 0.5):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    plt.figure()\n",
        "    x[sample][0][0][threshold > x[sample][0][0]] = 0\n",
        "    plt.imshow(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader, loss_func):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        #loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "outputId": "26dad087-cb47-431a-8959-1278188d5981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 5).to(device)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZFoI1Sk0On",
        "colab_type": "code",
        "outputId": "83f2a751-04c0-4b3a-8af0-3ec101969442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "f = h5py.File('test_fixed_25.hdf5','r')\n",
        "print(f['predictor'].shape)\n",
        "f.close()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "(2452, 10, 5, 16, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "outputId": "0fe29730-9cdd-42ed-f246-03ca9185aeb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"now changed to fixed dataset\"\"\"\n",
        "\n",
        "avg = np.load(\"fixed_25_avg.npy\")\n",
        "std = np.load(\"fixed_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('test_fixed_25.hdf5', valid_frac = 0.1, dataset_length = 2452,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"recent_test\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "outputId": "cad89979-dce5-44bd-b3dc-6704bf674aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "outputId": "d4014f36-5869-4971-86be-e54014c226c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_image_save(test_model, train_loader, name + \"comparison\", sample = 200, threshold = 0)\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([2000, 1, 5, 16, 16])\n",
            "torch.Size([2000, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD1CAYAAABX2p5TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAAbuCAYAAAAIX+1GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceaV2m7ZdHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(model, train_loader, avg = 'macro'):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "#     print(x[0][0][0])\n",
        "    print(x[222,0,0].shape)\n",
        "    print(b[222].shape)\n",
        "#     print(b)\n",
        "    print(b[222].view(-1, 256).numpy().shape)\n",
        "    truth = set(list(b[222].view(256).numpy()))\n",
        "    pred = set(list(x[222,0,0].view(256).numpy()))\n",
        "    print(truth - pred)\n",
        "    scores = []\n",
        "    for i in range(len(b)):\n",
        "        score = f1_score(b[i].view(256).numpy(), x[i,0,0].view(256).numpy(), average=avg)\n",
        "        scores.append(score)\n",
        "        truth = set(list(b[i].view(256).numpy()))\n",
        "        pred = set(list(x[i,0,0].view(256).numpy()))\n",
        "        if len(truth - pred) > 0:\n",
        "            print(i)\n",
        "#     score = f1_score(b[222].numpy(), b[222].numpy(), average=avg)\n",
        "    return scores\n",
        "#     print(score)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKhzR5rvPov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(model, train_loader, name = 'default'):\n",
        "    \"\"\"Calculate TN, FN, TP, FP for multilabel classification\"\"\"\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    # reshape\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = x[:,0,0].view(-1,256).numpy()\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0 \n",
        "    fp = 0\n",
        "    \n",
        "    print(truth.shape)\n",
        "    print(pred.shape)\n",
        "    for i in range(len(b)):\n",
        "        for j in range(256):\n",
        "            # true positive\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 1):\n",
        "                tp += 1\n",
        "            # true negative\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 0):\n",
        "                tn += 1\n",
        "            \n",
        "            #false positive\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 1):\n",
        "                fp +=1\n",
        "            #false negative\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 0):\n",
        "                fn += 1\n",
        "    \n",
        "    print(\"tn:\" ,tn)\n",
        "    print(\"tp:\" , tp)\n",
        "    print(\"fn:\" , fn)\n",
        "    print(\"fp\" ,fp)\n",
        "    \n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp/ (tp + fn)\n",
        "    \n",
        "    f_1 = 2 * prec * rec / (prec + rec)\n",
        "    \n",
        "    print(\"prec:\" ,prec)\n",
        "    print(\"rec:\" , rec)\n",
        "    print(\"f1: \", f_1)\n",
        "    \n",
        "    f = open(name + \"metrics.csv\", 'w')\n",
        "    for i in [tn, tp, fn, fp, prec, rec, f_1]:\n",
        "        \n",
        "        f.write(str(i) + \"\\n\")\n",
        "\n",
        "    f.close()\n",
        "    \n",
        "                \n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HLsqwEvahiy",
        "colab_type": "code",
        "outputId": "9b4c1feb-2c5f-4902-db70-6eb421afb87c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "    scores = f1(test_model, train_loader, avg = 'binary')\n",
        "    np.save(name + \"scores\", scores)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 16])\n",
            "(1, 256)\n",
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLTv_en0WN3",
        "colab_type": "code",
        "outputId": "3d8848f2-fd6e-4d3c-f464-a3c52b3afbce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "metrics(test_model, train_loader, name = name)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482628\n",
            "tp: 10080\n",
            "fn: 4208\n",
            "fp 15084\n",
            "prec: 0.4005722460658083\n",
            "rec: 0.7054871220604704\n",
            "f1:  0.511000709723208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiCx0wFnYxf",
        "colab_type": "code",
        "outputId": "03076bd9-699f-4314-8ed6-e4f8f8da905e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5003645562832529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe7BIv-liRj",
        "colab_type": "code",
        "outputId": "a15180d6-5d8a-4fa7-e989-6f37fa630bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(scores)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f10c93e2a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAxQq7siiNoeIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszwtyDuVvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, b in train_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7oAiBWWX-2",
        "colab_type": "code",
        "outputId": "eaea8b8b-9b27-4c31-82f4-bc0fc52c6161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.count_nonzero(b) / (2000)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwUwgS2W3GC",
        "colab_type": "text"
      },
      "source": [
        "# testing flipping the input direction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nD-LmrX2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d, b in train_loader:\n",
        "    break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMHy4qTW7a9",
        "colab_type": "code",
        "outputId": "3a3fd4ec-a143-4aea-ed1f-5ecaa7b094b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# set up some axes\n",
        "a = d\n",
        "a = a.numpy()\n",
        "c = np.flip(a, 1)\n",
        "\n",
        "# c = torch.flip(a, (0,1))\n",
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(c[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAOICAYAAAAU5r/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1152x1152 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08NdPNhZoJG",
        "colab_type": "code",
        "outputId": "5feda764-8ada-4402-a380-1f449a57f280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c.shape\n",
        "c = np.array(c)\n",
        "torch.tensor(c)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.2740, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026,  0.0377,  0.7753],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.4872,  1.4265],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.1818, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0205],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907, -1.1484],\n",
              "           [-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907,  0.2907],\n",
              "           [-0.7675, -0.7675, -0.7675,  ..., -0.1003,  0.6341,  0.6341],\n",
              "           ...,\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.4469,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]]],\n",
              "\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ...,  0.0089, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0147],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.6831, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY3GRCuQYkQZ",
        "colab_type": "code",
        "outputId": "0859f3fe-907c-4055-dc21-c3c6d6110e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "e.shape"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-f73a2bc07fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBo6C1MAZdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = torch.tensor(c)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGYIr8YYKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(e.numpy()[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsDysmeW87k",
        "colab_type": "text"
      },
      "source": [
        "# after flipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQF2DnA6yB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d) // 3\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "losses = batch_loss_histogram(test_model, train_loader, loss_func = c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ic2GUlrX799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sirv8aO61kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbxmoet7IUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d).to(device)\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCoJCKbhiPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloIqpwpW6Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a,b in train_loader:\n",
        "    break\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbcXFd1pU6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = a.to(device)\n",
        "b = b.to(device)\n",
        "c(a[0][0][0],b[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIbwcFpW99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b[0]\n",
        "# sdaddasdasadad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "# truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = (46898  - incident_map)// incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8uk9UI6hGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UQbluAaTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)\n",
        "loss_default = nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg-X4tXNanfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = b[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_KNeqBapXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d[1 > d] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhnCUQDawzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsFFKJ7WaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(a[0][-1][0],b[0]))\n",
        "print(loss_default(a[0][-1][0], b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEo_Gr8PXhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.ones_like(a[0][-1][0])\n",
        "c *= -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8aP4eTX7cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizNoE4vXoGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(c,b[0]))\n",
        "print(loss_default(c, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyngsiPa5Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(d,b[0]))\n",
        "print(loss_default(d, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9AGiQKOePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = batch_loss_histogram(test_model, train_loader, loss_func)\n",
        "l2 = batch_loss_histogram(test_model, train_loader, loss_default)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-l3gnzjPGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(l1)\n",
        "plt.figure()\n",
        "sns.distplot(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}