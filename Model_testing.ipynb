{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/Model_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "outputId": "13b78cb5-192a-4c43-c654-7b502404f50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "outputId": "c956d6dd-8c21-4920-8443-c3bd3db27596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=04414a4a0f0ab8f6544c1e9d753857d0772e653104092cd3358675e3d9d622a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROspCb3F4Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfGC8lSoPcJ-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "outputId": "272224d1-af24-44d0-b0bd-90223145ed28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 0.983s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "outputId": "88ba1269-3cfa-4215-98ee-1c0a243f3d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    print(\"GPUs:\", torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "outputId": "d7ae85ec-c3a2-4a49-bf9a-16f1e356d01f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "#         truth -= self.avg[0]\n",
        "#         truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7, threshold = 0.5):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    plt.figure()\n",
        "    x[sample][0][0][threshold > x[sample][0][0]] = 0\n",
        "    plt.imshow(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader, loss_func):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        #loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "outputId": "1b558900-fccb-483e-d8dd-73ae5bd55ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 5).to(device)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZFoI1Sk0On",
        "colab_type": "code",
        "outputId": "4c8186d8-68b6-415b-93c0-054f0555cdc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "f = h5py.File('emerg_25.hdf5','r')\n",
        "print(f['predictor'].shape)\n",
        "f.close()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "(4940, 10, 5, 16, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "outputId": "bb67bbf6-ab32-4a33-8f43-d09da759d13a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"now changed to fixed dataset\"\"\"\n",
        "\n",
        "avg = np.load(\"fixed_25_avg.npy\")\n",
        "std = np.load(\"fixed_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('emerg_25.hdf5', valid_frac = 0.1, dataset_length = 4940,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"bce_3\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "outputId": "0869f8cc-4e47-4dc8-f3e5-c8a37eca71bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "outputId": "b4d9ba5e-526b-4183-c43f-f8fe210b03a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_image_save(test_model, train_loader, name + \"comparison\", sample = 2, threshold = 0)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([2000, 1, 5, 16, 16])\n",
            "torch.Size([2000, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD1CAYAAABX2p5TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF34zs8z8f44PgudTdMsvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlOGXGvW/kpWvGe9s46UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAAbuCAYAAAAIX+1GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceaV2m7ZdHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(model, train_loader, avg = 'macro'):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "#     print(x[0][0][0])\n",
        "    print(x[222,0,0].shape)\n",
        "    print(b[222].shape)\n",
        "#     print(b)\n",
        "    print(b[222].view(-1, 256).numpy().shape)\n",
        "    truth = set(list(b[222].view(256).numpy()))\n",
        "    pred = set(list(x[222,0,0].view(256).numpy()))\n",
        "    print(truth - pred)\n",
        "    scores = []\n",
        "    for i in range(len(b)):\n",
        "        score = f1_score(b[i].view(256).numpy(), x[i,0,0].view(256).numpy(), average=avg)\n",
        "        scores.append(score)\n",
        "        truth = set(list(b[i].view(256).numpy()))\n",
        "        pred = set(list(x[i,0,0].view(256).numpy()))\n",
        "        if len(truth - pred) > 0:\n",
        "            print(i)\n",
        "#     score = f1_score(b[222].numpy(), b[222].numpy(), average=avg)\n",
        "    return scores\n",
        "#     print(score)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKhzR5rvPov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    \"\"\"Calculate TN, FN, TP, FP for multilabel classification\"\"\"\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    # reshape\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = x[:,0,0].view(-1,256).numpy()\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0 \n",
        "    fp = 0\n",
        "    \n",
        "    print(truth.shape)\n",
        "    print(pred.shape)\n",
        "    for i in range(len(b)):\n",
        "        for j in range(256):\n",
        "            # true positive\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 1):\n",
        "                tp += 1\n",
        "            # true negative\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 0):\n",
        "                tn += 1\n",
        "            \n",
        "            #false positive\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 1):\n",
        "                fp +=1\n",
        "            #false negative\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 0):\n",
        "                fn += 1\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp/ (tp + fn)\n",
        "    \n",
        "    f_1 = 2 * prec * rec / (prec + rec)                \n",
        "                \n",
        "    if verbose:\n",
        "        print(\"tn:\" ,tn)\n",
        "        print(\"tp:\" , tp)\n",
        "        print(\"fn:\" , fn)\n",
        "        print(\"fp\" ,fp)\n",
        "        print(\"prec:\" ,prec)\n",
        "        print(\"rec:\" , rec)\n",
        "        print(\"f1: \", f_1)\n",
        "    \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'w')\n",
        "        for i in [tn, tp, fn, fp, prec, rec, f_1]:\n",
        "        \n",
        "            f.write(str(i) + \"\\n\")\n",
        "\n",
        "        f.close()\n",
        "    \n",
        "                \n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6iVIEUNEPR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def area_under_curve_metrics(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "#     truth = b.contiguous().view(-1).numpy()\n",
        "#     pred = sig(x[:,0,0].contiguous().view(-1)).numpy()\n",
        "#     fpr, tpr, thresholds = roc_curve(truth, pred)\n",
        "#     plt.plot(fpr, tpr)\n",
        "#     plt.plot(fpr, fpr)\n",
        "    r = roc_auc_score(truth, pred)\n",
        "    av = average_precision_score(truth, pred)\n",
        "    if verbose:\n",
        "        print(r)\n",
        "        print(av)\n",
        "    \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'a')\n",
        "        for i in [r,av]:\n",
        "        \n",
        "            f.write(str(i) + \"\\n\")\n",
        "\n",
        "        f.close()\n",
        "    \n",
        "        \n",
        "        \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDKnjygZerpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def brier_score(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    diff = (x[:,0,0] - b)**2\n",
        "    brier = np.average(diff)\n",
        "    \n",
        "    if verbose:\n",
        "        print(brier)\n",
        "        \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'a')\n",
        "        f.write(str(brier) + \"\\n\")\n",
        "        f.close()\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgdaoLwaU8j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_metrics(model, train_loader, name = 'default'):\n",
        "    metrics(model, train_loader, name = name)\n",
        "    area_under_curve_metrics(model, train_loader, name = name)\n",
        "    brier_score(model, train_loader, name = name)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWepJCa5W_sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZs5gRhzGYwK",
        "colab_type": "code",
        "outputId": "2187ab37-4ce6-44cf-8be6-f60a16512e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "\n",
        "\n",
        "full_metrics(test_model, train_loader, name)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482095\n",
            "tp: 7520\n",
            "fn: 10875\n",
            "fp 11510\n",
            "prec: 0.39516552811350497\n",
            "rec: 0.408806740962218\n",
            "f1:  0.40187040748162994\n",
            "FINISHING ONE PASS\n",
            "0.8884509501705424\n",
            "0.3439482905521257\n",
            "FINISHING ONE PASS\n",
            "0.043720703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPsmKD10Kg31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def curves(model, train_loader):\n",
        "#     dataframe = pd.dataframe()\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "    plt.figure()\n",
        "#     for i in range(16):\n",
        "    for j in range(16):\n",
        "        t = b[:,j,j].contiguous().view(-1).numpy()\n",
        "        p = sig(x[:,0,0,j,j].contiguous().view(-1)).numpy()\n",
        "        fpr, tpr, thresholds = roc_curve(t, p)\n",
        "        plt.plot(fpr, tpr)\n",
        "    plt.plot(fpr, fpr)\n",
        "    plt.xlim(0, 1.1)\n",
        "    plt.ylim(0,1.1)\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXD5JpCaLIzd",
        "colab_type": "code",
        "outputId": "5641a92d-6bbb-487f-88d5-5c4b7b080c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "curves(test_model, train_loader)\n",
        "# sns.set()\n",
        "# x = [1,2,3]\n",
        "# y = [4, 5,7]\n",
        "# ax = sns.lineplot(x, y, palette= 'red')\n",
        "# ax.plot([1,2], [11,22])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-OCXRDP3cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etMgGdJjf2Tj",
        "colab_type": "code",
        "outputId": "ebbbc663-8afb-4a0c-e15a-44ce6e7ac1c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "brier_score(test_model, train_loader)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "0.043720703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HLsqwEvahiy",
        "colab_type": "code",
        "outputId": "49d4a54f-5fba-4c6d-e773-e8fbacf53f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "    scores = f1(test_model, train_loader, avg = 'binary')\n",
        "    np.save(name + \"scores\", scores)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 16])\n",
            "(1, 256)\n",
            "set()\n",
            "178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1304\n",
            "1341\n",
            "1975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLTv_en0WN3",
        "colab_type": "code",
        "outputId": "67d51452-b851-4a63-edfd-1e648da52a3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "metrics(test_model, train_loader, name = name)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482095\n",
            "tp: 7520\n",
            "fn: 10875\n",
            "fp 11510\n",
            "prec: 0.39516552811350497\n",
            "rec: 0.408806740962218\n",
            "f1:  0.40187040748162994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiCx0wFnYxf",
        "colab_type": "code",
        "outputId": "5ba3f05b-63cf-40f1-96b0-cf590ab0cdc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36876444053062724"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe7BIv-liRj",
        "colab_type": "code",
        "outputId": "52191057-eafd-4b58-bfec-b6e64ead7569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import seaborn as sns\n",
        "plot = sns.distplot(scores)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl03PV56P/3M6Nd1r5asmVJtuV9\nAcsbtjE7hiaQpklrCMQQAiUNSXub29zk9tckP3JvT9IsbZqQEiAO0BYIkAUnMRiDMTbesIxXeZXl\nRYu1r9Y+M5/7x4x9BluyRtJovrM8r3PmeOa7zTNjzTzz2cUYg1JKqchlszoApZRS1tJEoJRSEU4T\ngVJKRThNBEopFeE0ESilVITTRKCUUhFOE4FSSkU4TQRKKRXhNBEopVSEi7I6gMFkZmaawsJCq8NQ\nSqmQsW/fviZjTNZozg3KRFBYWEhZWZnVYSilVMgQkXOjPVerhpRSKsJpIlBKqQiniUAppSLcsIlA\nRCaLyHsiclREykXkbwc5RkTk30WkQkQOicj1XvvWicgpz22dv1+AUkqpsfGlsdgBfM0Y85GIJAH7\nRGSzMeao1zF3AdM9t6XAfwBLRSQd+DZQChjPuRuMMa1+fRVKKaVGbdgSgTHmgjHmI8/9TuAYkH/F\nYfcCLxq33UCqiEwE7gQ2G2NaPF/+m4E1fn0FSimlxmREbQQiUghcB+y5Ylc+UOX1uNqzbajtg137\nMREpE5GyxsbGkYSllFJqDHxOBCIyAfgN8HfGmA5/B2KMecYYU2qMKc3KGtWYCKWUUqPgUyIQkWjc\nSeC/jTG/HeSQGmCy1+NJnm1DbVdKKRUkhm0sFhEBfgkcM8b8eIjDNgBPiMgruBuL240xF0RkE/DP\nIpLmOe4O4Jt+iFuFsJf2nPf52PuXFvj1mr5eT6lI4kuvoRXAg8BhETng2fa/gQIAY8zTwEbgbqAC\n6AYe9uxrEZHvAns95z1pjGnxX/hKKaXGathEYIz5AJBhjjHAl4fYtx5YP6roQtB4/NpVSqnxpCOL\nlVIqwmkiUEqpCKeJQCmlIpwmAqWUinCaCJRSKsJpIlBKqQiniUAppSKcJgKllIpwQbl4vYpMHb0D\nVLV00zvgot/hJC7azrTsCRRlJpKVFGt1eEqFLU0EylIDThe7K5s5UNXGhfbeq/a/tq8agCkZCSwp\nTOfmmdmsLskiMVb/dJXyF/00KcscqWnnzSMXaO0eoDAjgTvn5FKcmUhibBTRdqFnwMl1BWmcrOvk\nw7MtbD5Wz2v7qomNsnHLzGz+snQyq6ZnEmXXGk6lxkITgQo4lzG8XV7HtlNN5CbH8YUVk5iWPeGq\n45LiolldksXqkiwevbEYh9PF3rOtbCqv4w8Ha3nzSB05ybGsu6GQWLud+Bi7Ba9GqdCniUAFVL/D\nxev7qjlQ1cbSonQ+MT8Pu+2acxpeFmW3sXxqBsunZvC/757Feyca+K/d5/iXt04QE2VjeXEGq0uy\niIsO3oSg02WrYKSJQAWMMYZ/eP0gB6rauGN2DqtLsnAvdzFyMVE27pyTy51zcimvbeebvz3M+ycb\n2Xu2hVtnZrO0OAPbKK+tVKTRRKD8Zrhfu9tONvJWeR13zM7hphnZfnveOXkprF1cwKppPbxZfoE/\nHLrAwep2Pn19PtlJcX57HqXClSYCFRAn6jrYVF7HvPwUVpeMz5rU+WnxPLKiiANVbfzx0AV+tqWC\nP5s/kSWF6ZdLHrpehFJX0+4Watx19A7walk1uSlx/MX1k0ZdHeQLEeG6gjT+7rbpFGUm8saBWn7z\nUQ0DTte4PadSoW7YRCAi60WkQUSODLH/H0TkgOd2REScIpLu2XdWRA579pX5O3gVGv5wsJYBp4u1\niwuIiQrMb4+kuGjW3VDIzTOy+eh8K89ur6SrzxGQ51Yq1PjyqXweWDPUTmPMD4wxC40xC3EvTP/+\nFesS3+zZXzq2UFUoOlLTTnltB7fOzA746GCbCLfPzuFzSwuoa+/lF9sqaevuD2gMSoWCYROBMWYb\n4OuC8/cBL48pIhU2evqdbDhYS15KHCunj0+7gC/m5KXw8IoiLvYN8PT7p2m62GdZLEoFI7+V00Uk\nAXfJ4Tdemw3wtojsE5HH/PVcKjRsOV5PV5+DT18/yeexAuOlKDORR1cV43AZfvnBGVq6tGSg1CX+\nrLD9JLDjimqhlcaY64G7gC+LyI1DnSwij4lImYiUNTY2+jEsZYXmi33srmxh0ZQ08lLjrQ4HgIkp\n8Tyysoh+h4vntlfSqtVESgH+TQRruaJayBhT4/m3AfgdsGSok40xzxhjSo0xpVlZ1lUjKP/YVF6H\n3SbcNjvH6lA+ZmJKPF9YWUSvw8nzO87S3a8NyEr5JRGISAqwGnjDa1uiiCRdug/cAQza80iFl/PN\nXRyp7WBVSSbJcdFWh3OV/NR4HlxWSGt3P/+565x2LVURz5fuoy8Du4AZIlItIo+IyOMi8rjXYX8O\nvG2M6fLalgN8ICIHgQ+BPxlj3vJn8Cr4GGN4q7yOpLgoVk0L3pJdUWYiny2dzPmWbl4tq8JljNUh\nKWWZYUcWG2Pu8+GY53F3M/XeVgksGG1gKjSdbuzibHM39yzIC9iYgdGal59Cx7yJ/OnwBf546AKf\nnD9xXAe7KRWsdIoJ5TfGGN49Xk9yXBSlU9KsDscnK6Zl0t4zwAcVTaTGR3PjOE1/oVQw00Sg/OZ0\nYxfnPKWBUFosZs3cXNp7BnirvI7k+CgWTg6NJKaUv2giUH5hjOHdY6FVGrjEJsJnF03iYp+D3+yr\nYUJs9KAL5SgVrkLnZ5sKarsrWzjX0s3qGdkhVRq4JMpu44GlU8hKiuW/95yjtq3H6pCUCpjQ+8Sq\noPT0+6dJjA290oC3+Bg7624oJC7azgs7z1LV0m11SEoFhCYCNWZHazt4/2QjK6ZmEB2CpQFvKfHR\nPHRDIQMuF+t+9SGtOhWFigCh/alVQeHp90+TGGNnaVGG1aH4RU5yHA8uK6S6tYcvvLBXp69WYU8T\ngRqTqpZu/niolvuXFhAfE7yLxo9UUWYi/752IQer2nj4V5oMVHjTRKDG5NntldhtwiMri60Oxe/W\nzJ3IT9Zex77zrTz0qw+5qMlAhSntPqpGrfliH6+WVfGphfnkpozPIvEjWWN4PHxyQR4i8LevHGDt\nM7v45brF5CSPz2tVyipaIlCj9sLOs/Q5XPz16vArDXj7xPw8nv38Iiobu/jUUzs4dqHD6pCU8itN\nBGpUuvocvLDrHLfPymFadpLV4Yy7W2bm8Nrjy3EZw6d/vpP/2n0OoxPVqTChiUCNyit7q2jvGeDx\nm6ZaHUrAzMlL4Q9PrKS0MI3/7/dHeOSFMmp04JkKA5oI1IhdWuFrSVE61xeE7gCy0chOjuOFh5fw\nnU/OZkdFE7f8cCs/3HRCG5JVSNNEoEZsw8FaLrT38qXVkVMa8GazCQ+tKOLdr61mzdxcfvZeBSu/\nv4WfvnuKjt4Bq8NTasQ0EagRcbkMv3j/NDNzk7hpRmRP2TwpLYGfrL2ON768gkUFafxo80lWfG8L\nP3r7hI5IViFFE4EakS3HGzjVcJHHV0/VRVw8FkxO5ZcPLeaPX1nJiqmZ/HRLBSu+v4Wn3qvQZTBV\nSPBlqcr1ItIgIoOuNywiN4lIu4gc8Ny+5bVvjYicEJEKEfmGPwNX1nj6/dPkp8bzifkTrQ4l6MzN\nT+HpBxfx9v+4kVXTM/nBphN86qkdlNe2Wx2aUtfkS4ngeWDNMMdsN8Ys9NyeBBARO/AUcBcwG7hP\nRGaPJVhlrbKzLZSda+XRVUUhOdV0oJTkJPGLB0t5+oHraejs49M/38n7JxutDkupIfmyZvE2ESkc\nxbWXABWetYsRkVeAe4Gjo7iWsoj3yN4Xd50lIcYOiOUjfgPB19d4/9KCQbevmTuRxYXpPPjLD3n0\nhTJ+dv91/gxPKb/x18+65SJyUETeFJE5nm35QJXXMdWebSoE1XX0cryuk+XFGUG/KH0wyZgQy8uP\nLmN2XjJf+u+PqGi4aHVISl3FH5/oj4ApxpgFwE+B34/mIiLymIiUiUhZY6MWo4PN9pONRNuF5cXh\nMdV0IKUkRPNfX1xKUWYir+2rolvHHKggM+ZJ54wxHV73N4rIz0UkE6gBJnsdOsmzbajrPAM8A1Ba\nWqpj94NIW3c/B6vbWFacQUKszlN4JV+rkNbMyeU/tp7mt/tr+NzSAu11pYLGmEsEIpIrnr9oEVni\nuWYzsBeYLiJFIhIDrAU2jPX5VODtqGgCYOW0TIsjCW15qfHcMSeHoxc6KDvXanU4Sl027M87EXkZ\nuAnIFJFq4NtANIAx5mngM8CXRMQB9ABrjXs2LoeIPAFsAuzAemNM+bi8CjVuuvsd7D3byoJJqaQm\nxFgdTshbMS2TYxc62VRex/z8FGKjw2cxHxW6fOk1dN8w+38G/GyIfRuBjaMLTQWD3ZXN9DtdrCqJ\n7FHE/mITYc3cXJ5+/zS7Kpu5aUa21SEppSOL1dB6+p3sPN3MjJwkcnUxFr8pSE9gRk4S20810Tvg\ntDocpTQRqKG9WlZFd7+TG7U04He3zc6hZ8B5uf1FKStpIlCDcjhdPLu9koL0BAozEqwOJ+zkp8Yz\nJy+ZDyq0VKCsp4lADerNI3VUt/Zw4/RM7eY4TlaXZNHncHGgqs3qUFSE00SgrmKM4bntlRRlJjJz\nYrLV4YSt/NR4JqbEUXa2xepQVITTRKCusvdsKwer2/nCyiJsWhoYNyJCaWE6te291LTqkpfKOpoI\n1FWe3V5JWkI0n7l+ktWhhL2Fk1KJtgt7tVSgLKTzBUSooaZFaLrYxztH67lpRha/2z/kjCDKT+Jj\n7MzNS+FgdRt3zcslNkoHmKnA0xKB+pgdFU3YbMIynVwuYBYXptPncHGkRhewUdbQRKAu6+pz8NH5\nVhZOTiUpLtrqcCLGlIwE0hNjOKyJQFlEE4G6bM+ZZgacRieXCzARYU5eMqcbuujp1zEFKvA0ESgA\nBpwudlW2UJIzgRydTiLg5ual4DSG43Udwx+slJ9pIlAAHKxqo6vPwcppOp2EFfLT4kmJj+ZIrSYC\nFXiaCBTGGD6oaCI3OY6pWYlWhxORbCLMnpjMqfpO+hxaPaQCSxOB4kxzFw2dfdwwNUOnk7DQnPxk\nHC7DyXpd11gFliYCxZ7KFuKj7cyflGp1KBGtMCORxNgo7UaqAk4TQYTr6B2gvLadRVPSiInSPwcr\nuauHkjhZ30m/w2V1OCqCDPvJF5H1ItIgIkeG2P85ETkkIodFZKeILPDad9az/YCIlPkzcOUfZWdb\ncBlYUpRudSgKmJGTRJ/DxUfndU1jFTi+/AR8Hlhzjf1ngNXGmHnAd4Fnrth/szFmoTGmdHQhqvHi\ndBk+PNPC9OwJZE6ItTocBRRnTcAm8P7JRqtDURFk2ERgjNkGDDkjljFmpzHm0s+X3YDOVBYiTtR1\n0tHrYGmRTicRLOKi7UzJSOT9E5oIVOD4u1L4EeBNr8cGeFtE9onIY35+LjVG+861kBQbxYzcJKtD\nUV5KcpI4eqGDho5eq0NREcJviUBEbsadCP6X1+aVxpjrgbuAL4vIjdc4/zERKRORssZG/TU03jp7\nBzhR38l1BWnYbdplNJiU5EwAtHpIBY5fEoGIzAeeA+41xjRf2m6MqfH82wD8Dlgy1DWMMc8YY0qN\nMaVZWTq6dbztP9+Gy8CiKWlWh6KukJscR1ZSrCYCFTBjTgQiUgD8FnjQGHPSa3uiiCRdug/cAQza\n80gFljGGfedaKUhPICtJG4mDjYiwuiSL7aeacLqM1eGoCOBL99GXgV3ADBGpFpFHRORxEXncc8i3\ngAzg51d0E80BPhCRg8CHwJ+MMW+Nw2tQI/TR+TYaL/ZRqqWBoLW6JIv2ngEOVuvC9mr8DbtCmTHm\nvmH2fxH44iDbK4EFV5+hrPb6vmqi7cK8/BSrQ1FDWOGZCnzX6WauL9CErcaXDiWNMH0OJxsPX2BO\nXgqx0bosYrBKT4xhZm4Su043D3+wUmOkiSDCbDvZRHvPAAsmaWkg2C0rzqDsXItON6HGnSaCCLPh\nYC1pCdFMy9axA8FuWXEGvQMubSdQ404TQQTp6nPwztF67p43UccOhIBlxemIoNVDatxpIogg7xyr\np2fAyT0L8qwORfkgNSGGWbnJ7K7URKDGlyaCCLLhQC0TU+JYXKgzjYaKZcUZ7DvXqquWqXGliSBC\ntHcPsO1UI59ckIdNq4VCxvKpGfQ5XBw4r+0EavxoIogQ7x6vZ8BpuHveRKtDUSOwpMjTTqDVQ2oc\naSKIEG8dqWNiShzzdRBZSEmJj2ZOXrI2GKtxpYkgAnT3O3j/ZCN3zsnVaqEQtLw4g/1VbfQOaDuB\nGh+aCCLA+yca6XO4uGNOjtWhqFFYPjWDfl2+Uo0jTQQRYFN5HWkJ0SzR3kIhqbQwHZvAbq0eUuNE\nE0GY63e4ePdYA7fPziHKrv/doSg5Lpp5+SnsrhxyxVilxkS/GcLcztNNdPY5uHNOrtWhqDFYNjWD\n/VWt9PRrO4HyP00EYe6dY/UkxNgvT2usQtOy4gwGnEbbCdS40EQQxowxvHe8kZXTMonTKadD2uLC\ndOw20W6kalwMuzCNCh0v7Tn/scd17b3UtPWwpCj9qn0qtEyIjWJefooOLFPjwqcSgYisF5EGERl0\nzWFx+3cRqRCRQyJyvde+dSJyynNb56/A1fBO1HUAMCNHp5wOB8unZnCwqo3ufofVoagw42vV0PPA\nmmvsvwuY7rk9BvwHgIikA98GlgJLgG+LiK67FyDH6zrJS40jOT7a6lCUHywvzsDhMuw7p+0Eyr98\nSgTGmG3Atfqu3Qu8aNx2A6kiMhG4E9hsjGkxxrQCm7l2QlF+0t3n4HxLNzNzk60ORfnJoilp2G2i\n01Irv/NXY3E+UOX1uNqzbajtapydbLiIQauFwklibBTzJ+l4AuV/QdNrSEQeE5EyESlrbGy0OpyQ\nd7yug8TYKPLT4q0ORfnRsuIMDlVrO4HyL38lghpgstfjSZ5tQ22/ijHmGWNMqTGmNCsry09hRSaX\nMZyqv8iMnAnYRCeZCyeXxxOc0/UJlP/4KxFsAD7v6T20DGg3xlwANgF3iEiap5H4Ds82NY5q23ro\nGXAyXReoDzul2k6gxoFP4whE5GXgJiBTRKpx9wSKBjDGPA1sBO4GKoBu4GHPvhYR+S6w13OpJ40x\nWsE5zk41XARgavYEiyNR/pboGU+giUD5k0+JwBhz3zD7DfDlIfatB9aPPDQ1WqfqL5KXGseEWB0v\nGI6WFWfwyw8q6e53kBCj/8dq7IKmsVj5R9+Ak/MtXVotFMaWFadrO4HyK00EYaayqQuXgWlaLRS2\nSj3zDmn1kPIXTQRh5lTDRaLtwpT0BKtDUePk0rxDe85oIlD+oYkgzFQ0dFKcOUEXoQlzy4ozOFDV\npusTKL/Qb4sw0trVT9PFfq0WigBLL7UT6PoEyg80EYSRCk+30emaCMKejidQ/qSJIIycaugkJT6a\nrKRYq0NR4ywpLpq5Op5A+Yl2Qg4TTpfhdGMXsycmIzqtRMjzZSGhlLgodlU209PvJD5GV6BTo6cl\ngjBxuKadngEn03K0WihSFGVOYMBp2K/tBGqMNBGEie0nGxFgWpYmgkgxJSMBm6DVQ2rMNBGEie2n\nmshLjSdRp5WIGHHRdl3HWPmFJoIw0Nk7wEfnW7XbaARaNtU9nqCrT9cnUKOniSAM7K5sweEy2m00\nAq2clsmA0/DhWZ3UV42eJoIwsP1UIwkxdgp0WomIs7gwnZgoGztONVkdigphmgjCwPZTTSwrztBp\nJSJQXLSdxYVpfFChiUCNnn5zhLiqlm7ONHWxanqm1aEoi6yYlsnxuk4aO/usDkWFKE0EIW67p0pg\n1XRd5zlSrZzm/hGw87SWCtToaCIIcdtPNZKXEsfUrESrQ1EWmZOXQkp8NB9oO4EaJZ8SgYisEZET\nIlIhIt8YZP+/isgBz+2kiLR57XN67dvgz+CDkTEmYFMDO5wudlQ0sWp6lk4rEcHsNuGGqRnsqGjC\nvWqsUiMz7OgjEbEDTwG3A9XAXhHZYIw5eukYY8z/8Dr+K8B1XpfoMcYs9F/IwcnpMmw8fIGfbqmg\nobOXP5ufx7Ki9HH9gj5U005Hr4OV2j4Q8VZMy+TNI3VUNnUxVUeXqxHypUSwBKgwxlQaY/qBV4B7\nr3H8fcDL/gguVBhjePj5vXzl5f04XYaizET+cLCW3+2vweF0jdvzbj/ZhIj7S0BFths9bUTbTjZa\nHIkKRb7MR5APVHk9rgaWDnagiEwBioAtXpvjRKQMcADfM8b8fohzHwMeAygoKPAhrOCx+Wg92042\n8j/vKCE1IQaAd4/V896JRmKjbPzZ/Lxxed7tpxqZl59CemLMuFxfhY6CjASKMhPZeqKRh1cUWR2O\nCjH+bixeC7xujPGuJJ9ijCkF7gf+TUSmDnaiMeYZY0ypMaY0Kyt0esA4nC6+/9ZxirMSeXz1VGwi\n2ES4fXYuiwvT2V3ZQvNF/3fra+8ZYH9V2+UeI0qtLslid2UzvQO6fKUaGV8SQQ0w2evxJM+2wazl\nimohY0yN599KYCsfbz8Iea/tq+Z0Yxdfv3PmVQO6bp2Vjc0Gbx+t9/vzbj/ViNNluGVmtt+vrULT\nTTOy6HO4dBI6NWK+JIK9wHQRKRKRGNxf9lf1/hGRmUAasMtrW5qIxHruZwIrgKNXnhuqevqd/Ovm\nkyyaksadc3Ku2p8cF82q6VkcrmmnqqXbr8+95VgDqQnRXFeQ5tfrqtC1rDiD2Cgb75/QdgI1MsMm\nAmOMA3gC2AQcA141xpSLyJMico/XoWuBV8zH+6/NAspE5CDwHu42grBJBG8fraOhs4+v3V4yZO+g\nVdMySYyN4q3yOr89r9Nl2HqykZtnZGO3abdR5RYXbWf51Ay2nmiwOhQVYnyavN4YsxHYeMW2b13x\n+DuDnLcTmDeG+ILaxsMXyE6KZVlxxpDHxEbbWT09k41H6qht6yEvNX7Mz3ugqo2Wrn5u1mohdYWb\nZ2Tz7RPlnG3qojBTBxkq3+jI4lHq6nOw9UQjd83NxTbMr/JFU9KJtgt7zvhnquAtx+ux24TVOq2E\nusJNM9x/E1oqUCOhiWCUthxvoM/h4u55E4c9Nj7GzoJJqRyoavVLj44txxspnZJGSkL0mK+lwsuU\njESKMhPZou0EagR0XcNRevPIBTInxFJamO7T8UuLMyg718pH51u5YerIuny+tOf85ftt3f0cu9DB\nXXNzP7ZdqUtun53Dr3acobN3gKQ4/bGghqclglHo7new5XgDd83N9bmxNj81nslp8eyubBnTfDDH\n6zoBmJGTNOprqPB2++wcBpyGrVoqUD7SRDAKW0800jvg4q55uSM6b2lxBk0X+6hs6hr1cx+qbic7\nKZaspNhRX0OFt+sL0shIjGHzOIxfUeFJq4ZGYfPRetITY1haNHRvocHMy09h4+EL7K5sHtXEYG3d\n/Zxt7uL22Tk626gCGLJ6sCgzkU3ldby46yxRNvfvvfuXhtbULSpwtEQwQsYYdp5uYsW0zBH34Y+2\n2yidksaxCx209wyM+LkPVbcDMD8/ZcTnqsgya2IyfQ4XZxpHX/pUkUMTwQhVNnVR39HH8muMHbiW\nJUUZGAN7z468K+nB6jYmp8WTMUGrhdS1TcueQLRdOHqhw+pQVAjQRDBCO0+753G5YeroEkF6Ygwl\nOUnsPdPCwAimqG7o6OVCey8LJqeO6nlVZIm22yjJSeLYhQ5culiNGoYmghHadbqJvJQ4pmQkjPoa\nS4vT6exz8Ha57415B6vbEdztDEr5YvbEZDp6HVT7eZ4rFX40EYyAy2XYdbqZ5VMzx9RYW5KTRFpC\nNM9ur/SpK6nTZdh/vpWpWRO0X7jy2ayJyUTZhIM17VaHooKcJoIROF7XSWv3wKirhS6xibC6JJsD\nVW1s9WFFqQNVbbT1DLBi2tieV0WWuGg7M3KTOFLdrtVD6po0EYzApXnel48xEQAsmpLG5PR4/nXz\nyWuWCpwuw9YTDeSlxlGig8jUCM2flEpnn4MzYxi7osKfJoIR2HW6iaLMRL/MIGq3CV+5ZTqHqtt5\n99jQE4T98VAtzV393DwjW8cOqBGbkZNETJTtctdjpQajicBHTpdhT2XLNaecHqlPX5fPlIwEfrz5\n5KCL3LtchqfeqyA7KZZZE5P99rwqcsRE2Zg9MZkjNe30O3zvpaYiiyYCHx2v66Czz8HSIt8mmfNF\nlN3GP9w5g6MXOvinN8qvqiL64dsnOFl/kZtnZmPT0oAapfn5KfQMOPmgQuceUoPTROCjfedaASgt\n9O/SkJ+Yn8ff3DSVlz88z0+3VFze/tR7Ffx862nuX1qgI4nVmEzLmUB8tJ3ffjTUUuMq0vmUCERk\njYicEJEKEfnGIPsfEpFGETnguX3Ra986ETnlua3zZ/CBtPdsK7nJceT7oX3gSv9w5ww+fX0+P958\nkjX/to1PPbWDH2w6wacW5vF/7p2rbQNqTKJsNhZOTuXt8nrauvutDkcFoWETgYjYgaeAu4DZwH0i\nMnuQQ39tjFnouT3nOTcd+DawFFgCfFtEQnK19X1nWygtTBuXL2UR4ft/MZ+/uWkqBekJJMTYeeiG\nQn742QXDrn6mlC8WTUmj3+nijQO1VoeigpAvs48uASqMMZUAIvIKcC/gyyL0dwKbjTEtnnM3A2uA\nl0cXrjVq2nqobe/lsSnjl8Oi7Ta+vmbmuF1fRba81Hjm5CXz2r4q1t1QaHU4Ksj4UjWUD1R5Pa72\nbLvSX4jIIRF5XUQmj/BcROQxESkTkbLGxuBq1CrzTBDn62pkSgWjvyydzJGaDsprtSup+jh/NRb/\nASg0xswHNgMvjPQCxphnjDGlxpjSrKzgWpS97GwriTF2ZubqgC4Vuu5dmEeM3cZrZdVWh6KCjC+J\noAaY7PV4kmfbZcaYZmNMn+fhc8AiX88NBWXnWrmuII0ou3ayUqErNSGGO+bk8Lv9NfT0O60ORwUR\nX77Z9gLTRaRIRGKAtcAG7wNEZKLXw3uAY577m4A7RCTN00h8h2dbyOjoHeB4XYffu40qZYXPLy+k\nvWeA3x8Iud9jahwNmwiMMQ5p6Zt+AAAQUElEQVTgCdxf4MeAV40x5SLypIjc4znsqyJSLiIHga8C\nD3nObQG+izuZ7AWevNRwHCr2n2/DGCidou0DKvQtLkxj1sRknt9x1qeZb1Vk8GnNYmPMRmDjFdu+\n5XX/m8A3hzh3PbB+DDFaquxsC3absLBAF4RRoU9EePiGQr7+m0PsrmzxywSKKvRppfcwys62Mmti\nEhNifcqZSgW9exbmkZYQzfM7z1gdigoSmgiuYcDp4kBVm1YLqbASF21n7ZICNh+tp0pXL1NoIrim\no7Ud9Aw4taFYhZ3PL5+C3SY8u73S6lBUEND6jmsouzTRnJYIVBh4ac/5jz1eMCmVl/acJy81nmSv\nJVDvX1oQ6NCUxbREcA1lZ1uYlBZPbkqc1aEo5XerS7Jwugw7TjVZHYqymCaCIRhjKDvXSuk4zi+k\nlJUyJsQyf1IKe8600N3nsDocZSFNBEM439JNY2efzi+kwtrqGdn0O13sON1sdSjKQpoIhlB2dnwW\nolEqmOQmxzE3L5kdp5vo7B2wOhxlEU0EQyg710JSXBQl2TrRnApvd8zOxeF08d6J4Jr1VwWOJoIh\nfHimhUVT0nRhGBX2MpNiKS1M58MzzTRf7Bv+BBV2NBEMoqGjl9ONXSwv1uH3KjLcMjMbu014+2i9\n1aEoC2giGMSuSnfDmc7DoiJFclw0K6dlcbimnb1nQ2peSOUHmggGsbuyhaTYKGZPTLY6FKUCZnVJ\nFinx0fzT74/gcLqsDkcFkCaCQeyubGZJUbouRKMiSkyUjT+bN5HjdZ28uOuc1eGoANJvuivUtfdy\npqlLq4VURJqTl8zqkix+vPkkDR29VoejAkQTwRV2VbqH2y/ThmIVgUSE79wzh36ni39644guXhMh\nNBFcYdfpZlLio7V9QEWsosxEvnZ7CZvK69lwsNbqcFQA+JQIRGSNiJwQkQoR+cYg+/9eRI6KyCER\neVdEpnjtc4rIAc9tw5XnBptdlc0sLUrX8QMqon1xVTHXFaTyrTfKtYooAgybCETEDjwF3AXMBu4T\nkdlXHLYfKDXGzAdeB/7Fa1+PMWah53YPQay6tZuqlh5tH1ARz24TfvjZBfQOOPnmbw9rFVGY86VE\nsASoMMZUGmP6gVeAe70PMMa8Z4y5tNTRbmCSf8MMjEtD7FdNz7I4EqWsNzVrAv9rzUzePd7A8zvP\nWh2OGke+JIJ8oMrrcbVn21AeAd70ehwnImUisltEPjWKGAPm3WP1FGYkMDUr0epQlAoKD68o5LZZ\nOfzzxmMcqm6zOhw1TvzaWCwiDwClwA+8Nk8xxpQC9wP/JiJThzj3MU/CKGtsDPzkV939DnaebuaW\nmTmIaPuAUuDuRfSDz8wnc0IsT7y0nw6doTQs+ZIIaoDJXo8nebZ9jIjcBvwjcI8x5vLMVcaYGs+/\nlcBW4LrBnsQY84wxptQYU5qVFfiqmR0VzfQ7XNw6Kzvgz61UMEtLjOGn911HbVsPf/fKAZwubS8I\nN74kgr3AdBEpEpEYYC3wsd4/InId8AvcSaDBa3uaiMR67mcCK4Cj/gren7YcrycpNorFuhCNUlcp\nLUzn25+czZbjDfx48wmrw1F+Nuzi9cYYh4g8AWwC7MB6Y0y5iDwJlBljNuCuCpoAvOapVjnv6SE0\nC/iFiLhwJ53vGWOCLhG4XIZ3jzVwY0kWMVE6tEKpwTywbApHL3Tw1HunmZmbzCcX5FkdkvKTYRMB\ngDFmI7Dxim3f8rp/2xDn7QTmjSXAQCiv7aChs49bZmq1kFJDERH+/3vmUtFwka+9dpDclDgtQYcJ\n/fkLbD5WjwjcrIlAqWuKibLxzIOlTEqN59EXyzjdeNHqkJQfRHwiMMbwxoEalhVlkJ4YY3U4SgW9\ntMQYnn94CVE2Yd36D6lr15HHoc6nqqFwVnaulXPN3Xz1lulWh6JUUHhpz3mfjvur0gJe2HWWzz23\nm1//9XIyJ8SOb2Bq3ER8ieD1smoSY+zcNS/X6lCUCin5afGsf2gxNW09PPDcHtq6+60OSY1SRJcI\nuvsd/OnwBe6eN5GEmIh+K5QalYqGi9y3pIAXd53jrp9s5+EVRUyIvfqzdP/SAguiU76K6BLBpvI6\nLvY5+MyikJwaSamgMD07iQeXTaGxs4/ntlfSqaOPQ05EJ4LX91VTkJ6gXeCUGqOSnCTW3VBIa3c/\nz2yrpPli3/AnqaARsYngaG0HOyqa+eyiSbr2gFJ+MDVrAo+sKKK738nT2yqpbu0e/iQVFCI2Efzk\n3ZMkxUXx+RsKrQ5FqbBRkJHI46unEmMXnt1eyZGadqtDUj6IyERQXtvOpvJ6vrCiiJT4aKvDUSqs\nZCXF8terp5KTHMdLH57nnWP1uHSiuqAWkYngJ++cIikuii+sLLI6FKXCUnJcNI+uKmZRQRpbjjfw\nhRf20qTtBkEr4hLB4ep23j5azyMrtTSg1HiKttv49PX53Lswj12nm1nzb9vZeqJh+BNVwEVUIugd\ncPL3rx4gKymWh1doaUCp8SYiLC3KYMMTK0lPjOahX+3l7189QEuXDj4LJhGVCP554zFONVzkR59d\noKUBpQJoRm4SG55YyRM3T2PDgVpu/dFW/nPXWQacLqtDU0RQInj3WD0v7jrHIyuLuLFEF6dXKtDi\nou38zztn8KevrmJ6ThL/9EY5t//4fd44UINDE4KlIiIRbD/VyFde3s+sicl8fc0Mq8NRKqLNyE3i\n148tY/1DpcRG2fnbVw6w+gdbeW57pVYZWSTsJ9jZePgCf/vKfqZmTeCFLywmNspudUiX+TrLo1Lh\nRkS4ZWYON5Vks+V4A89ur+T//OkY33vzODfPzOYT8yeyuiSL1ASdGj4QfEoEIrIG+AnupSqfM8Z8\n74r9scCLwCKgGfgrY8xZz75vAo8ATuCrxphNfov+Gpou9vGDt07w6r4qFhWk8cuHFmu7gFJBxmYT\nbpudw22zczhe18HvPqrhd/tr2Hy0HpvAwsmpLC5KZ/GUdGblJZOXEodnOVzlR2LMtQd6iIgdOAnc\nDlTjXsz+Pu+1h0Xkb4D5xpjHRWQt8OfGmL8SkdnAy8ASIA94Bygxxjiv9ZylpaWmrKxsxC/GGEN5\nbQd/OFTLS7vP0zPg5KEbCvnaHTOIjwlMSUB/5Ss1Ni5jqGnt4XhdJxUNndS29eL0fE/F2G1kJcWS\nnRTLrbOyyU6KI2NCDBkTYslIjCE1IZrYKDvRdrmcMEbymQzlWVJFZJ8xpnQ05/pSIlgCVBhjKj1P\n9gpwL+C9CP29wHc8918Hfibu/4V7gVeMMX3AGRGp8Fxv12iCvZbufgef+OkHVDZ2EWUTbp2VzdfX\nzGRq1gR/P5VSahzZRJicnsDk9ARun53DgNNFTWsP9Z29NHT20djZR2VTF/vfPjnkNUQgNspGbJQd\nh8tgExDPtUXcVVPejy/9+9KH57CJeG5gt7kTSmyUjYQYO/HRduJj7MRF2y8/jvNs836cEBPlOdbm\n3h/t/iFqAGPAYHC53N9bXX1Omrr6aOzoo9/p4oFlUwLyPnvzJRHkA1Vej6uBpUMdY4xxiEg7kOHZ\nvvuKc/NHHe01JMREsWpaJo+uKmbNnFzSdNlJpcJCtN1GYWYihZmJH9v+59fl09zVR/PFfpq7+mi6\n2E979wB9Did9Dpf7NuDk2IVO9xevcdcaGOMudRj42DZjDNlJcbiMwelyb3O6DE5j6Ox10NjZR8+A\nk+5+J739TroHnDj9PHVGSnx00CaCgBCRx4DHgEzgooicsDik0coEmqwOYgw0fmtp/NayPH75zqhP\nnSIijxljnhnpib4kghpgstfjSZ5tgx1TLSJRQAruRmNfzgXAE/wzIlJmjCn0Kfog5Il/VPV0wUDj\nt5bGb61wiB8YcSLwZRzBXmC6iBSJSAywFthwxTEbgHWe+58Bthh3K/QGYK2IxIpIETAd+HCkQSql\nlBo/w5YIPHX+TwCbcHcfXW+MKReRJ4EyY8wG4JfAf3oag1twJws8x72Ku2HZAXx5uB5DSimlAsun\nNgJjzEZg4xXbvuV1vxf47BDn/l/g/44gphEXa4KMxm8tjd9aGr+1RhX/sOMIlFJKhbeImGtIKaXU\n0CxPBCKSLiKbReSU59+0QY5ZKCK7RKRcRA6JyF9ZEesVMa0RkRMiUiEi3xhkf6yI/Nqzf4+IFAY+\nyqH5EP/fi8hRz/v9rogEvnPzNQwXv9dxfyEiRkSCqieIL/GLyF96/g/KReSlQMd4LT78/RSIyHsi\nst/zN3S3FXEORkTWi0iDiBwZYr+IyL97XtshEbk+0DFeiw/xf84T92ER2SkiC4a9qHswhXU34F+A\nb3jufwP4/iDHlADTPffzgAtAqoUx24HTQDEQAxwEZl9xzN8AT3vurwV+bfV7PcL4bwYSPPe/FGrx\ne45LArbhHtRYanXcI3z/pwP7gTTP42yr4x5h/M8AX/Lcnw2ctTpur9huBK4Hjgyx/27gTdyDkZcB\ne6yOeYTx3+D1d3OXL/FbXiLAPQ3FC577LwCfuvIAY8xJY8wpz/1aoAGwclGBy9NuGGP6gUvTbnjz\nfl2vA7dK8MyWNWz8xpj3jDHdnoe7cY8BCRa+vP8A3wW+D/QGMjgf+BL/o8BTxphWAGNMMK3x6Ev8\nBkj23E8BagMY3zUZY7bh7t04lHuBF43bbiBVRCYGJrrhDRe/MWbnpb8bfPzsBkMiyDHGXPDcrwNy\nrnWwiCzB/Svk9HgHdg2DTbtx5dQZH5t2A7g07UYw8CV+b4/g/oUULIaN31Ocn2yM+VMgA/ORL+9/\nCVAiIjtEZLdnBuBg4Uv83wEeEJFq3D0OvxKY0PxipJ+PYObTZzcgU0yIyDtA7iC7/tH7gTHGiMiQ\n3Zg8Wfk/gXXGGF3SKABE5AGgFFhtdSy+EhEb8GPgIYtDGYso3NVDN+H+RbdNROYZY9osjcp39wHP\nG2N+JCLLcY8zmquf28ARkZtxJ4KVwx0bkERgjLltqH0iUi8iE40xFzxf9IMWgUUkGfgT8I+e4pqV\nxjLtRjDwaeoPEbkNd7JebdwzyAaL4eJPAuYCWz21cbnABhG5xxgz8vnN/c+X978ad93uAO6Ze0/i\nTgx7AxPiNfkS/yPAGgBjzC4RicM9j08wVXENxeepcYKViMwHngPuMsYM+70TDFVD3tNTrAPeuPIA\nz9QWv8Ndb/d6AGMbylim3QgGw8YvItcBvwDuCbL6aRgmfmNMuzEm0xhTaNzzVu3G/TqCIQmAb38/\nv8ddGkBEMnFXFVUGMshr8CX+88CtACIyC4gDGgMa5ehtAD7v6T20DGj3qr4OeiJSAPwWeNAYM/Rc\n3d6CoAU8A3gXOIV74Zp0z/ZS3KuhATwADAAHvG4LLY77btwL9pzGXUoBeBL3Fw64//BfAypwz69U\nbPV7PcL43wHqvd7vDVbHPJL4rzh2K0HUa8jH919wV28dBQ4Da62OeYTxzwZ24O5RdAC4w+qYvWJ/\nGXfPwwHcJa9HgMeBx73e+6c8r+1wEP7tDBf/c0Cr12e3bLhr6shipZSKcMFQNaSUUspCmgiUUirC\naSJQSqkIp4lAKaUinCYCpZSKcJoIlFIqwmkiUEqpCKeJQCmlItz/Axa3oS99d39gAAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8h8Kkd7X86",
        "colab_type": "code",
        "outputId": "e4ce27c8-3993-4803-cba1-a277991cf752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "plot.set_title(\"Histogram of average F1 Score per Image prediction\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Histogram of average F1 Score per Image prediction')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEHaB5b8EQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszwtyDuVvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, b in train_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7oAiBWWX-2",
        "colab_type": "code",
        "outputId": "23f704f5-bffd-4a7d-aa97-9b693b8ad2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.count_nonzero(b) / (2000)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.1975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwUwgS2W3GC",
        "colab_type": "text"
      },
      "source": [
        "# testing flipping the input direction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nD-LmrX2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d, b in train_loader:\n",
        "    break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMHy4qTW7a9",
        "colab_type": "code",
        "outputId": "49ce9d9a-fb38-4988-82a4-f094a59da7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# set up some axes\n",
        "a = d\n",
        "a = a.numpy()\n",
        "c = np.flip(a, 1)\n",
        "\n",
        "# c = torch.flip(a, (0,1))\n",
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(c[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAOICAYAAAAU5r/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x1152 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08NdPNhZoJG",
        "colab_type": "code",
        "outputId": "98d28794-5212-44c8-e08f-6b5bc98fe5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c.shape\n",
        "c = np.array(c)\n",
        "torch.tensor(c)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0032, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0026, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3608, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.6608, -0.3570, -0.3570,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792],\n",
              "           [ 0.3218, -0.3522, -0.3522,  ..., -0.4792, -0.4792, -0.4792]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0147, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           [ 0.8654,  0.8654,  0.8654,  ...,  0.7964,  0.7964,  0.7873],\n",
              "           ...,\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809],\n",
              "           [-0.9955, -0.9955, -0.9955,  ..., -1.0101, -1.0101, -0.9809]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.1184,  0.1760],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0032],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0262, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.0262, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288],\n",
              "           [ 0.6050,  0.6050,  0.4959,  ...,  0.8288,  0.8288,  0.8288]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0493, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-1.1484, -1.1484, -1.1484,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830],\n",
              "           [ 0.8893,  0.8893,  0.8696,  ...,  1.2830,  1.2830,  1.2830]]]],\n",
              "\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0954],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0320, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6582, -0.6582, -0.6582,  ..., -0.1417, -0.1417, -0.7352],\n",
              "           [-0.4262, -0.4262, -0.4262,  ..., -0.0473, -0.0473, -0.2232],\n",
              "           [-0.4262, -0.4262, -0.4262,  ..., -0.0473, -0.0473, -0.2232],\n",
              "           ...,\n",
              "           [ 0.4872,  0.4872,  0.4872,  ..., -0.1823, -0.1823, -0.6837],\n",
              "           [ 0.4872,  0.4872,  0.4872,  ..., -0.1823, -0.1823, -0.6837],\n",
              "           [ 0.4872,  0.4872,  0.4872,  ..., -0.1823, -0.1823, -0.6837]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.1069],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.1242],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0089, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.4296],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.3547],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.1184],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.6608, -0.6608, -0.6608,  ..., -0.1369, -0.1369, -0.8852],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           [-0.4801, -0.4801, -0.4801,  ..., -0.0260, -0.0260, -0.1311],\n",
              "           ...,\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480],\n",
              "           [ 0.6263,  0.6263,  0.6263,  ...,  0.0568,  0.0568, -0.5480]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084,  0.0320,  0.0781,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           [-0.9381, -0.9381, -0.9381,  ..., -0.7312, -0.7312, -0.6129],\n",
              "           ...,\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.4332, -0.4332, -0.4332,  ..., -0.4818, -0.4818, -0.4915],\n",
              "           [-0.2233, -0.2233, -0.2233,  ..., -0.3150, -1.1484, -1.1484]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0723]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1457, -0.1457, -0.1457,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859],\n",
              "           [-0.2375, -0.2375, -0.2375,  ..., -0.5847, -0.5859, -0.5859]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.0147, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084,  0.0896, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0089]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084,  0.0377,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084,  0.0262,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.0435, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.2706, -0.2706, -0.2706,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553],\n",
              "           [-0.3522, -0.3522, -0.3522,  ..., -0.4792, -0.2553, -0.2553]]]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY3GRCuQYkQZ",
        "colab_type": "code",
        "outputId": "cac64879-3fce-49ca-d518-fbece4b31f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "e.shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-f73a2bc07fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBo6C1MAZdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = torch.tensor(c)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGYIr8YYKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(e.numpy()[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsDysmeW87k",
        "colab_type": "text"
      },
      "source": [
        "# after flipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQF2DnA6yB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d) // 3\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "losses = batch_loss_histogram(test_model, train_loader, loss_func = c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ic2GUlrX799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sirv8aO61kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbxmoet7IUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d).to(device)\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCoJCKbhiPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloIqpwpW6Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a,b in train_loader:\n",
        "    break\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbcXFd1pU6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = a.to(device)\n",
        "b = b.to(device)\n",
        "c(a[0][0][0],b[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIbwcFpW99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b[0]\n",
        "# sdaddasdasadad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "# truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = (46898  - incident_map)// incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8uk9UI6hGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UQbluAaTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)\n",
        "loss_default = nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg-X4tXNanfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = b[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_KNeqBapXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d[1 > d] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhnCUQDawzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsFFKJ7WaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(a[0][-1][0],b[0]))\n",
        "print(loss_default(a[0][-1][0], b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEo_Gr8PXhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.ones_like(a[0][-1][0])\n",
        "c *= -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8aP4eTX7cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizNoE4vXoGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(c,b[0]))\n",
        "print(loss_default(c, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyngsiPa5Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(d,b[0]))\n",
        "print(loss_default(d, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9AGiQKOePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = batch_loss_histogram(test_model, train_loader, loss_func)\n",
        "l2 = batch_loss_histogram(test_model, train_loader, loss_default)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-l3gnzjPGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(l1)\n",
        "plt.figure()\n",
        "sns.distplot(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}