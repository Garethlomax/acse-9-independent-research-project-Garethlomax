{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/Model_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3349b754-57fe-4e1c-f821-9859fabfefda"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "729851f8-9472-4479-a717-e1ba2bce67ff"
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=1537fe72ead6fb3c977fc5887d70db994190e51d9f6bbcaab33031d188d87e9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ca3266d6-1ff7-47a7-e2ab-35f42fc57359"
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.514s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4854d38f-4101-4c67-f767-34ced53098c7"
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    print(\"GPUs:\", torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76e71d6a-398d-44b8-fd14-5a2612b76ed1"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "        truth -= self.avg[0]\n",
        "        truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e6206933-1131-442b-f806-305201c022c2"
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 3).to(device)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a3cdda07-9d1d-4efa-e1e0-411686c8a407"
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "avg = np.load(\"min_event_25_avg.npy\")\n",
        "std = np.load(\"min_event_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('data_min_events_25.hdf5', valid_frac = 0.1, dataset_length = 52109,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(valid, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"bce\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d2955e4-5850-41c5-ca21-0db2082538f1"
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_image_save(test_model, train_loader, name + \"comparison\", sample = 1200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81ad7435-abea-4861-ac0b-e3d7c76a2400"
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([46898, 16, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ff97bb6-962b-49e2-d89e-0f7bfd66e482"
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "3c2b56a3-9e4d-4734-ddd6-36e9a374ecb3"
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "99c03571-02ab-4503-d679-457601233a61"
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efe4a4cfbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlOGXGmX4pUYZfqlRhl9q1P8DEje0JvynfIkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "982b8c94-0822-4542-f4fc-773573f1149b"
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46898, 16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5b6ed854-bba3-4df9-d87b-a554e23d00d1"
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "58b68afd-5900-4183-8d10-0f96c82e24a0"
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xu8XFV9/vHPk5OQkHsCJIQECCCg\ngooQKdWqWLwgUi61VsRWtGikStVqRSz+RGq1UrXUthYbEW9VvIAK3kEr4A0R5BbklgCBBBICCYSQ\nEHLO+f7+WOuYnZMzc2Zmzz5nMnneee1X5uy912Vm9qxZs/ba362IwMzMtn9jRrsCZmbWHm7Qzcy6\nhBt0M7Mu4QbdzKxLuEE3M+sSbtDNzLpExzTokiZICknzRrsuAJI+JumCUSz/45IekXTvaNWh20k6\nVdJ3R7seAJK+LOnd7d53eyLpPElrJC0d7bpstyKi5gKsLyz9wMbC368fJu3RwJJ6+wzafwIQwLwa\n268BngB2L6w7Fri90TKaWYCPARdUkXcDZe+fX+OZNbafBvxkiPUrgT8p/P184HLgMeCR/Bq+vvD+\n9Bfez/uBi4DnDvGePJH3WQ6cC4yp8/49wdbHzTsqfq2GfC0qLvMLhef3FLC58Pe3R+OYaeNzOx24\nKz+XB4HvAjs1kK7UZxF4BrAOmNHAvv+aj7UjBq1/FXBTPgaXAccWth0D3Aw8np/fXxe29QAfzp+B\nx4AvAhML22cD38qfoYeAzxe3d9JSt4ceEZMHFuA+4M8K675SL21FngT+cRTKLUXS2CaT7A2sjIg1\nJco8ktSY/wjYB9gVeAfpoB9wd35vp5Ia/3uAX0l64aDsDsz7vQI4FXhDnaIPLB43EfEfrT6HThUR\nbyx8Lv4N+GLh+Z44eP8W3v9RIelVwPuAE/JzexbwHUAjUPzewAMRsbbeTpIOBl4KPDpo/WHABcDf\nk47n5wGL87bJwMWkzshU4G+A/5G0f05+GnAccDiwFzAH+Hgh+4+TGv29gKcDBwBntvg8q9XEN+i9\nwEsHrdsZ+DTpm3w56YmPA3Yh9eaLPcBdgBcAvyG9GQ8A5wFjc16N9ND/H+kbdq/BvYKh0gNfAz6Q\nHx8NLAE+ADwMrCB9ax8PLCV9+76nkPZjpB7rJbnM3wIHFbbvCVya87obOG1Q2q8CX89p/2qI5zMz\n77Oa1JCeQfrgHDvotfvMEGmH7aED1wGfrPN+DvkLivSh+EWd1/S7wCeGSFfz/QPmk3pNUwrr/jgf\nNz3577cCdwBrgO8DcwfluzC/T2uB8/K255K+5Hvza7Uyrz8euD2/9vdT41dC8XWsV84wn4ttfskB\nB+d6nUb6XPwA2InUy1tFOv5/CuxfSHMxcGbxuCYd7w/nPE5qcd/dgR+Ter+/IvVuf1TjuXwI+N86\nz3Ui8J+5jAeBT+XnNYttP+/Thki/C+kzOfCZeU9ef8Kg9P9Vo3wBVwMvznkcUdh2GfC+Gun2y8eI\nCutuI3VQIXV6/raw7eX59Rpom34OvKGw/X3AJcMdG6OxlB1DPwd4Numb/DDgSOCMiHgEOJHcA8zL\nI6SfpqfnN/aFwJ8Bb26ivHuALwMfbLG+83Mddid9EC8E/iI/h5cCH5E0t7D/q0k/v2aSGu9vSeqR\n1EP6kP4K2IPUOP6jpBcPkXYa6UthsM+Qvvz2AV4G/C1wckR8j61fu9OafZKSppPej4ubTUtqdI6Q\nNG6IfJ9FaohvaCbDiLiX9FP4hMLqk4GvR0SfpNcC7yIdD7Nz/v87KJujSQ34ocCbJB0ZETfkdFfm\n12r3vO+FpA/gFOAQ0geyUduU00Taop1yPvuTjgWRXtv9SD3ApaSf7rXsx5Zj9e9JPcqJLex7Aanz\nMgt4G3BKnTKvAU6U9AFJQx0Dn8r5HEQaIjkIeG9EPAS8Briz8Hl/bIj8P0v60tyb9Dq/Q9JrI+I7\ng9KfXqN+bwTuj4irhth2BDBe0m2SHpB0oaSpABGxlNRJeEP+/L6E1AZdU0ivQY+n5HpC+hI7UdJU\nSbuRjuMf1qjj6Gq05WfoHvoK4E8Lfx/Plh7zsGPopJ8tF8XWPaR6PfS/IjWgj5M+KM320B8jj/8C\nu+X9n1PY/1bg6NjS87qysG0sqRf/PFIP4a5B9TsHOL+Q9vI6z3s80AfsW1j3TnLPabjXjmF66KQP\neADz6+RRq4d+SE67S+E1fYzUY70LOJtCT6eQrrjvo4XlxXn76cAP8uMeUk/18Pz3zyickyF90W0m\nNe4D+S4obL8MeFet14I0zvkmCr8IhnsdhyunTh61eugBzKqTbl4+BnbKfw/uda9h6x7lBuDgZvYF\nJuV6zC1s+3dq9NDz9hNInZV1+b38KKmB22ngPSns+zLglkI9ao6hk3r3/eRf13nde4DvNZh+JulL\ncE7++w899Fy3IP3C24fUifoB8D+F9K/Jr1Mv6bxH8Xh7F3BLfk9mAlfk/J6Vt+9N6hT05edwKbn3\n3mlLyz10SSL1CJYVVi8D5g6dAiQ9U9IPJa2StI7U0961mXIj4gFgEennYbNWR0R/frwx/7+qsH0j\nMLnw9/2FcntJw0R7kN7g+ZIeHViAd5Nej23SDmF30gyj+wrr6r52g/SSGr3BBhrCgbH3OQ3mVzSX\ndOCuK6w7KCJmRMT+EXFO5KO8hoMiYnphGehNfQN4iaRdSb+G1kXEtXnb3sBnCq/l6vwcizOeVhYe\nb2Dr92mw40m94vsk/Z+k5w3znIuaKaeepyL1XAGQNC7P4rgnH/uLScfAjBrpHxr0OterS6195wD9\nEbGisK3ecUlEfCcijsn1Ogn4O9KvqXmkTs0dhffpYlKPvRFzSI1ksfxmjvl/AT4dEQ8OsW0z6XhZ\nFBH3RPp1cC5pSBVJh5J+DZ1IavwPJf0af0lO/19s+cV9I2mICtLQEqQG/Dek13Q66YtuUYP1HlEt\nN+j5AFrJlp8lkE4aDBw8Q33oPwv8DtgvIqYC/0RrJ1w+RvpGP7iwbmC2QfFnabGBbcWeAw/yMMse\npEb9flJvothwTYmtT4jVa/RWknsrhXXF124497H1607+eTkTWBbpxNL1pEatWScC10TE5hbS1pQb\nt6tJQ1wnk84fDLgfeOOg13PniLi+kayHKOvXEXEsqYd/+aCyRsrger2ZNCT5onzsDxy7VZ5wfBAY\nI2mPwro9a+1cFBF9EfFD4Jekuj5A+qKfX3iPpkXE7IEkjdRlUPnNHPNHAWdKWilpJelY/76k03Nb\ntHhQHYqPnw3cEBFXRUR/RCwGfkI6yU9E9EbE+yJir4jYizS+vyQi1ubP/bNIv743RsQ6UmN+TIP1\nHlFlx9AvAs6WtIukWcBZbBn7XAXMymeYB0wBHouI9ZIOAt7SSqERsRr4D+C9hXX9pJ9Nr8/jZMeR\nxnvLeL6kY/NY4hmkIZffAb8AkPSuPH9+rKRn555AI/XfBHwb+KikSZL2Iw25DB43ruUXwFhJ75Y0\nXtIU0smuqyJioIf5D8Bpkt4paYaSwyRtU0beNk/Sh0nDWmc1WI9mfZU0FHICWzeynwE+IOnAXJ8Z\nkhr9MloF7Dkw3ptfz5PyF9xm0vBcf70MRsgU0onStfn9+ueqC4yIJ0g9z3/Kx+lzSL3uIUn6S0mv\nljQ9HxMvJI1NXxMRT5LOCX0qf94laS9JL83JVwGzJU2qUZcNpJ7uv+T36Gmk3n+jx/wRpIb5kLys\nBf6aLechPg8slLRnbnPOAL6Xt10PPEfSC/LzfDqpMb85/z1L0t75OT2H1GE8O9e7j/SZX5g/a5NI\nM71ubrDeI6psg/5B4PeksecbSd/m/5q33UQag1yWf6LNJJ2webOk9aTZMV8vUfYn2LZ3czrwWtKb\nfQJb3tBWXUKa4rSW1Nt9de65bCZ9Qz+f9LNxNXA+zf08f2v+fxnwf6STVw1NBc0fjmPy8gBpbHs6\nqec7sM+VpLP1r8plPEz6afn9Qlb75vdiPekn5YGkWTJDnXRq1B2S1heWcwvbvkX6UC6JiDsKdb0o\n1+1beTjiRtL4bCN+RDq/85CkgZ/If0N6zo+RpljWm2Y5UhaRzimsJH02yrzGzXgLqSc8cIxeBGyq\nse9aUiO7lDTktgj4YERcmrf/Xc7netJr+wNg37ztetJ7cV/+vE+rUZdxpF+YP8n1aagNiIiHI2Ll\nwELqga/JX1qQjp9LScfO3aQvmDNy2ltIbcMFkh4n/Wq7ML8WkIaDfkqaifVt4N8jotjheD3wHNKv\njPtIw8QtdUarpvrDoWbWTSR9GiAi3j7adbH22y4ueDCz1ihNNe0jzbt+AWlIrZVzK7YdcINu1t1m\nkMa+Z5OGDM6OiJ+MbpWsKh5yMTPrEh0TbdHMzMrp2CGXv9j7uFI/Hf44ppauw16by/16mdnfW7oO\nsyc9MfxOw+jtK/+9PWvu46XSqw1dh6c29JTO454Vta7jadx9Y8eXSr+iDZ+6lSp/bD0QG4ffaRh3\nblpdKv2ta5YNv1MDep9aUWo+/+aH7274wz5u131HIlhZS9xDNzPrEh3bQzczGzH9faNdg7Zwg25m\n1ld+CKsTuEE3sx3elph927dKGvQcK+F4tkRSWwFcFhG3VVGemVkp/d3RoLf9pKik95HikAu4Ni8C\nLpLUmbdtMrMdW/Q3vnSwKnrop5JiYm8VflXSv5GCeH2sVkJJC0m3AOO5M5/NvpP3rrWrmVn7dMlJ\n0SqmLfaT4oYPNodhwphGxKKIWBARC9yYm9mIcQ+9pncBP5V0F1vuTrIX8DRSCEszs44SnuUytIj4\nkaQDgMPZ+qTob3OweDOzztIlJ0UrmeWS7x50zbA7mpl1gjYOpUi6kHSLzIci4uC87uukG8hAuhnN\noxFxiKT5pNDGAzd8uSYiTstpDgO+AOxMupnIO4e5n6/noZuZtfmk6BdId1D60sCKiHjtwGNJnyTd\n8WnA0og4ZIh8zifdGek3pAb9aOCH9Qru2Ab9hSWDay146snSddhj9mPD71TH1HlPla5Dz+Ty5617\nZpQLJgUwZrfdSqXvf6RccC8ATZs4/E7DmHpnuWBSAM9YUy5o2wNLp5euw+Obdiqdx9KeKaXzmD1h\nQqn0E3crf2y2RRt76BFxde55b0OSgL8E/rReHpLmAFMj4pr895dIt9Ws26A7OJeZWV9vw4ukhZKu\nKywLmyjphcCqiLirsG4fSTdIuirfmBvS+cflhX2Ws+WcZE0d20M3MxsxTZwUjYhFpBtot+J1bLk5\nNaS7SO0VEY/kMfPvSDqoxbzdoJuZjcQEPEljgT8HDttSbmwCNuXH10taChxAmhk4r5B8Xl5Xl4dc\nzMxG5sKilwK3R8QfhlIk7SapJz/eF9gfuDsiHgTWSToij7u/Abh0uALcoJuZ9fc3vgxD0kXAr4ED\nJS2XdGredBJbD7cAvAi4WdKNwMXAaRGxJm97G3ABsARYyjAnRMFDLmZm7Z7l8roa6984xLpLgEtq\n7H8dcHAzZY94D13Sm0a6TDOzuvo2N750sNEYcjmn1obidKBfrb+r1m5mZu3VxiGX0VTVDS5urrUJ\nmF0rXXE60Kf2+qtyV2+YmTWqw6MoNqqqMfTZwCuAtYPWC/hVRWWambWmw3vejaqqQf8eMDkibhy8\nQdKVFZVpZtYaN+i1RcSpdbadXEWZZmatig4/2dkoT1s0M/MYerVmlLwSt2ykRIBdDlep9D1z5pSu\ng6aXj4inWbNK58G4ctH99Nij5evQhp/F42fvWjqPnlvuLpV+3/lPlK7D+tvLR6+ceH+5iKYA/b2T\nS6W/c0yHRFv0kIuZWZdwD93MrEu4h25m1iXcQzcz6xK9vaNdg7Zwg25m1iU99EpiuUh6uqSjJE0e\ntP7oKsozMyulS2K5tL1Bl/QOUiD2vwMWSzq+sPmjw6T9Q3Cunz3h4FxmNkJG5gYXlatiyOUtwGER\nsT7f+fpiSfMj4lOkWC41FYNzfWmug3OZ2Qjp8J53o6po0MdExHqAiLhX0pGkRn1vhmnQzcxGRYf3\nvBtVxRj6KkmHDPyRG/djgV2BZ1VQnplZOb29jS8drIoG/Q3AyuKKiOiNiDeQ7p9nZtZZIhpfOljb\nh1yKd7QeYtsv212emVlpHkM3M+sSbtCrtV/fk6XSTz+gfHzjvpIBAnvKB1tEU8tHW2TqjPJ5rH24\nVHJNKR/Zj11q3r2wcWtXl86i59F15etR0qQNK4ffaRhT1m4qncesR3culX7G+Aml69AWXXJStGMb\ndDOzEdNXMl53h3CDbmbmIRczsy7hBt3MrEt4DL02SYcDERG/lfRM4Gjg9oj4QRXlmZmVEf2dPb+8\nUW1v0CWdDbwSGCvpCuCPgJ8BZ0p6bkR8pN1lmpmV4iGXmv4COAQYT7pidF5ErJP0CeA3QM0GXdJC\nYCHAe6c8l+Mn7ltB9czMBumSWS5VXPrfGxF9EbEBWBoR6wAiYiNQ92swIhZFxIKIWODG3MxGTJfE\nQ6+ih/6UpIm5QT9sYKWkaQzToJuZjYoOb6gbVUWD/qKI2AQQsdWp43HAKRWUZ2ZWTocH3WpUFcG5\nhryeOCIeBspdP25mVgX30M3MuoSnLVarr+TNjTS2/M2RxkzdqVz6p+1Tug5MKBf8CEB7P7N8Hs+Y\nXip9/53Xlq4DPW04XKfvUjqLMQueVy6DRx4qXYdxbWiAZvWWD/B12y/LBX6bqJ7SdWiLLpnl0rEN\nupnZSIkuGXKpYtqimdn2pT8aX4Yh6UJJD0laXFj3IUkrJN2Yl2MK294vaYmkOyS9orD+6LxuiaQz\nG3kabtDNzKK/8WV4XyCFOxnsvIg4JC8/AMihUU4CDspp/ltSj6Qe4NOkq+6fCbwu71uXh1zMzNp4\nUjQirpY0v8Hdjwe+lmcH3iNpCXB43rYkIu4GkPS1vO/v62XmHrqZWW9fw4ukhZKuKywLGyzldEk3\n5yGZgbPJc4H7C/ssz+tqra9rRBp0SV8aiXLMzFrSxJBLMURJXhY1UML5wH6kOFcPAp+s4mlUEW3x\nssGrgJdImg4QEce1u0wzs1IqnoceEasGHkv6LPC9/OcKYM/CrvPyOuqsr6mKMfR5pHGeC4AgNegL\naOAbqRht8T1TDuU4B+gysxFQ9bRFSXMi4sH854nAwAyYy4CvSvo3YA9gf+BaUru5v6R9SA35ScDJ\nw5VTRYO+AHgncBbw3oi4UdLGiLhquIT5p8sigKt3f013XLplZp2vjT10SRcBRwK7SloOnA0cKekQ\nUif3XuCtABFxq6RvkDrBvcDbI6Iv53M68GOgB7gwIm4druwqYrn0A+dJ+mb+f1UV5ZiZtU17Z7m8\nbojVn6uz/0cY4j4ReWpjU3d5q6yhjYjlwGskvQpYV1U5Zmal+dL/xkTE94HvV12OmVmrfE9RM7Nu\n4Qa9Wpui3BT5eKoD3qAxbZjmP21m6Sw0sVykRIB4Yk2p9D3PeH7pOvQtu7l0Htptr9J5xMPLy2XQ\nhhkVmlQ+CufY3SaUzmNKlBuqmM640nVoiy4JztWxDbqZ2YhxD93MrEu4QTcz6w7R5yEXM7Pu4B56\n4yT9CSkk5OKIuHwkyjQza1S3TFusJNqipGsLj98C/BcwBTi70TtvmJmNmDbesWg0VRU+tzgXaSHw\nsog4B3g58PpaiYpxhr+/cWlFVTMzG6S/iaWDVTXkMiYHcB8DKCJWA0TEE5J6ayUqBue6YvZrO/ur\n0My6RvR2eEvdoKoa9GnA9aQQkDEQOlLS5LzOzKxzdEd7Xk2DHhHza2zqJ8UCNjPrGN1yUnREpy1G\nxAbgnpEs08xsWO6hm5l1B/fQKzZpTM1zpw3Z8ED5CTxT55YMHDR1xvD7DEO7ziudB1G++9Ez75nl\nMujbXLoOY2btUzqPdojNT5VL/+TG8nXoLR+/e/OD5esxa2K5z8ic/mml69AW7qGbmXWHKNd/7Bhu\n0M1sh9eGH7EdwQ26mZkbdDOz7uAeuplZl3CDXoOkPwJui4h1knYGzgQOBX4PfDQiHmt3mWZmZURf\nd1zAXkVwrguBDfnxp0hhAM7N6z5fQXlmZqVEf+NLJ6tiyGVMxB8mAS2IiEPz419IurFeQkkLSdEZ\nOWPqczl+YmfMOzaz7hb97qHXsljSm/LjmyQtAJB0AFD36pKIWBQRCyJigRtzMxsp3dJDr6JBfzPw\nYklLgWcCv5Z0N/DZvM3MrKNEqOGlk7V9yCWf9HyjpKnAPrmM5RGxqt1lmZm1Q6f3vBtV2bTFiFgH\n3FRV/mZm7dLfJbNcPA/dzHZ43XJStGMb9N4oN7zf39sBb9Dah0tnERMnl85DU2eVr8eTT5TLoA3R\nFmPDo6XzGLP700rnod3nl8tgyvTSdeD3N5TOQjvdVz4PlQs7u4HOCFvrBt3MrEtEZ3yvlOYG3cx2\neO6hm5l1iU6fjtgoN+hmtsPr8ywXM7Pu0C099CquFEXSOyTtWUXeZmbtFv1qeBmOpAslPSRpcWHd\nxyXdLulmSd+WND2vny9po6Qb8/KZQprDJN0iaYmk/5A0bOGVNOjAh4HfSPq5pLdJ2q2RRJIWSrpO\n0nWXbbi7oqqZmW0tovGlAV8Ajh607grg4Ih4NnAn8P7CtqURcUheTiusPx94C7B/XgbnuY2qGvS7\ngXmkhv0w4PeSfiTpFElTaiUqBuc6buK+FVXNzGxr7eyhR8TVwJpB6y4vRKG9htQ+1iRpDjA1Iq6J\niAC+BJwwXNlVNegREf35SZwK7AH8N+kbxl1vM+soff1jGl6KIwl5WdhkcX8D/LDw9z6SbpB0laQX\n5nVzgeWFfZbndXVVdVJ0q6+xiNgMXAZcJmliRWWambWkmQuLImIRsKiVciSdBfQCX8mrHgT2iohH\nJB0GfEfSQa3kDdU16K+ttSEiNtTaZmY2GvpHYJaLpDcCxwJH5WEUImITsCk/vj6HHT8AWMHWwzLz\n8rq6KhlyiYg7q8jXzKwKVcdDl3Q0cAZwXLFTK2k3ST358b6kk593R8SDwDpJR+TZLW8ALh2uHM9D\nN7MdXjtjuUi6CDgS2FXScuBs0qyW8cAVefbhNXlGy4uAf5K0GegHTouIgROqbyPNmNmZNOZeHHcf\nUsc26I+VrFrPuPIR63vmNjTbsradxpeugyZNK50HT20snUXfsptLpR+z+36l69COuxD031vueQDE\n6vvLZTB1Zvk6rC0febJnZvnjc+zYcu/JhA65QrOdQy4R8bohVn+uxr6XAJfU2HYdcHAzZXdsg25m\nNlL6+qua8Dey3KCb2Q6vS6LnukE3MxuJWS4jwQ26me3wuiU4V9sbdEk7AScBD0TETySdDDwfuA1Y\nlC8yMjPrGOVPt3eGKnron8/5TpR0CjAZ+BZwFHA4cEoFZZqZtSxwD72WZ0XEsyWNJV3ZtEdE9En6\nX+CmeglzTISFAG+bsoCjJ5a/oa+Z2XB6u2TIpYq5OmPysMsUYCIwMJF6PDCuXsJitEU35mY2UgI1\nvHSyKnronwNuB3qAs4BvSrobOAL4WgXlmZmV4jH0GiLiPElfz48fkPQl4KXAZyPi2naXZ2ZWVqf3\nvBtVybTFiHig8PhR4OIqyjEzawf30M3MukSfe+jVWrJTT6n0Bz1e9/xrQ3rvfmD4neroeaJ8UKwx\nEyeVziMeXV06D8aUO38e69cMv9Nwdq5598KGacLU0nnExsfLpb/r1tJ1YH352wr0P/pU6Tw2bCz3\nepZ/N9qjgTvLbRc6tkE3Mxsp/e6hm5l1BwfnMjPrEj4pambWJfrlIRczs67QN9oVaJNKGvR8s9M/\nB/YkvVZ3Al+NiHVVlGdmVka3zHJpeywXSe8APgNMAJ5HiuGyJ3CNpCOHSbtQ0nWSrvv1+rvaXTUz\nsyH1o4aXTlZFcK63AK+MiH8mXfJ/UEScBRwNnFcvYTE41x9P3r+CqpmZbSuaWDpZVWPoY0lDLeNJ\n8dCJiPsklb/ax8yszbplyKWKBv0C4LeSfgO8EDgXQNJuQBsuFzQzay9PW6whIj4l6SfAM4BPRsTt\nef1q4EXtLs/MrKw+99Bri4hbgTYErDAzq5576GZmXcINesXGlTyd3DOu/FvUM3d2qfSaVj46IE+W\nj9hIX2/5PGbuXip5PPJg6SqMefrepfNox2sR95abUquZu5avwyNrS+cxZnL5j/9uu68vlX7nB9rw\nGWmDLrmlaOc26GZmI8U9dDOzLuFL/83MuoTnoZuZdQkPuZiZdQk36GZmXaLTY7Q0qorgXC0rRlv8\npaMtmtkI6VfjSyerpEGXNE3SxyTdLmmNpEck3ZbXTa+Vrhht8QWOtmhmI6SviaWTVdVD/wawFjgy\nImZGxC7AS/K6b1RUpplZS/qJhpdOVlWDPj8izo2IlQMrImJlRJwLtOFyPzOz9ulvYhmOpAslPSRp\ncWHdTElXSLor/z8jr5ek/5C0RNLNkg4tpDkl73+XpFMaeR5VNejLJJ0h6Q/XzkuaLel9wP0VlWlm\n1pI23+DiC6Qb+hSdCfw0IvYHfpr/BnglsH9eFgLnQ/oCAM4G/gg4HDh74Eugnqoa9NcCuwBX5TH0\nNcCVwEzgNRWVaWbWknb20CPiara998PxwBfz4y8CJxTWfymSa4DpkuYArwCuiIg1EbEWuIJtvyS2\nUVX43LXA+/KyFUlvAj4/XB4P9pSbGbr5yZ5S6QF671lVKv24P2pDEKYVy0vnoXl7ls6jdB1mlgt0\nBtB//eXl63HAocPvNFwe02qe12/MpidL1yGeKB+0rW/d5tJ5PL5mUqn0ZYPwtUuvKq/I7IgYiFC3\nEhj4QMxl61GL5XldrfV1jca0xXNGoUwzs5qaGXIpTq/Oy8Kmyoqo7PaklfTQJd1caxNbvpnMzDpC\nM+MBEbEIWNRkEaskzYmIB/OQykN5/Qqg+BN6Xl63Ajhy0PorhyukqitFZ5PGgAYHbRbwq4rKNDNr\nyQhMR7wMOAX4WP7/0sL60yV9jXQC9LHc6P8Y+GjhROjLgfcPV0hVDfr3gMkRcePgDZKurKhMM7OW\ntLM5l3QRqXe9q6TlpNkqHwO+IelUYBnwl3n3HwDHAEuADcCbACJijaQPA7/N+/1TRAw+0bqNqk6K\nnlpn28lVlGlm1qp2BueKiNfV2HTUEPsG8PYa+VwIXNhM2Q7OZWY7vL4OvwK0UW7QzWyH1y3hc0d8\n2qKkH9bZ9ofpQDc8vmQkq2UjNzxwAAAMwElEQVRmO7Bo4l8nq2raYq2rNwQcUitdcTrQWfNP7uxX\nzsy6Rrf00KsacvktcBWpAR+s5GV2Zmbt1elRFBtVVYN+G/DWiNjmLhWSHJzLzDpKdzTn1TXoH6L2\n+PzfVVSmmVlLerukSa9qHvrFdTYPGwLSzGwkdfrJzkaNxrTFc2gg2uLGkqcp7nmk/FD9+MUPl0o/\nY5d7S9dBO48vnUesW186jzETJparw5g2TKiK8h+6uP7KNtSj3LEZax8tX4fNvaWz6Ftf/vXsGVfu\ntbizrzMaUp8UrcPBucxse+Ieen0OzmVm2w330OtzcC4z2270tWE4rxM4OJeZ7fA8D93MrEt4DN3M\nrEt0yxj6aNxTtKZicK6bH1862tUxsx1EP9Hw0skqadAlTZX0L5K+LOnkQdv+u1a6iFgUEQsiYsGz\np+xXRdXMzLbRLdEWq+qhf540RfES4CRJl0gauELmiIrKNDNrSV9Ew0snq2oMfb+IeHV+/B1JZwH/\nJ+m4isozM2tZpw+lNKqqBn28pDER6RrpiPiIpBXA1cDkiso0M2uJT4rW913gT4srIuILwHuApyoq\n08ysJd0yhl7VhUVn1Fj/I0kfraJMM7NWecildQ1FW3yUctHkHuqZUCo9wJxHdi6Vfuw160rXYfxu\n5X8MasxQN45qzrj1Py9Xh+lTSteB3r7yeYwbVzoLTS4XebJ3yYrSdYgN5aMtRvksePjRSaXSb5rQ\nGYMd0eEnOxvlaItmtsPrcw+9LkdbNLPthodc6nO0RTPbbnjIpQ5HWzSz7Yl76GZmXaLTpyM2yg26\nme3wOv2S/kZVFZxrd0nnS/q0pF0kfUjSLZK+IWlOnXR/iLZ4x+N3V1E1M7NtONpifV8Afg/cD/wM\n2AgcA/wc+EytRMVoiwdO2beiqpmZba1bGvTKpi1GxH8CSHpbRJyb1/+npJonTM3MRoNnudRX7Pl/\nadC2norKNDNrSaf3vBtVVYN+qaTJEbE+Ij4wsFLS04A7KirTzKwlnuVSR0R8sMb6JZK+X0WZZmat\n6ovOiClTVscG57ruyXIBjMZP2LNUeoC+TdNLpX/GPeUDQU1etal0HlNmPlk6j7GrHyuVfsKej5eu\nQ1v0lu+J9exWLmhb35ry7+n6e8uPXK58YGbpPB5mp1LpV3dING2Podfh4Fxmtj3xGHp9Ds5lZtuN\ndo6hSzoQ+Hph1b7AB4HpwFuA1Xn9P0bED3Ka9wOnAn3AOyLix62U7eBcZrbD62/jkEtE3AEcAiCp\nB1gBfBt4E3BeRHyiuL+kZwInAQcBewA/kXRARDR9AwAH5zKzHV6Fs1yOApZGxDKp5o1mjge+FhGb\ngHskLQEOB37dbGFVXSlqZrbd6Iv+hpdiiJK8LKyT9UnARYW/T5d0s6QLJc3I6+aSrqofsDyva5ob\ndDPb4fVHNLwUQ5TkZdFQeUraCTgO+GZedT6wH2k45kHgk+1+HiPWoEua1cA+f/jmW7Nh1UhUy8yM\naOJfE14J/C4iVgFExKqI6IuIfuCzpGEVSGPsxXnW8/K6plUVbXHmoGUX4FpJMyTVnPxa/OabOdGz\nG81sZDTTQ2/C6ygMtwyKNHsisDg/vgw4SdJ4SfsA+wPXtvI8qprl8jCwbNC6ucDvgCBN4zEz6wjt\nPikqaRLwMuCthdX/KukQUht478C2iLhV0jdIEWp7gbe3MsMFqmvQ30t6Mu+NiFsAJN0TEftUVJ6Z\nWcv6Wms/a4qIJ4BdBq376zr7fwT4SNlyq5q2+ElJXwfOk3Q/cDZ0yaVYZtZ1fOn/MCJiOfAaSccB\nVwATqyrLzKyMbrn0v/JZLhFxGfAS4KUAkt5UdZlmZs2INB2xoaWTjUi0xYjYyJYzug1FW7zj0eWl\nytT0mldlNWzThD1KpX+kf1L5OjxVPo/Z5V5KAHbtKzfG+ORd5fsOB45fVzqPVRvL/1Acr3KhVh+P\naaXr0F/+8GbZuDZEbBxX7rVY1lv+PW2Hdl76P5ocbdHMdni+wUV9jrZoZtsN3+CiPkdbNLPtRqeP\njTfK0RbNbIfnMXQzsy7hHrqZWZfolnnoHdWg57jCCwF6xk6np2fyKNfIzHYE3dJDryra4tGFx9Mk\nfS4Hdf+qpJrTFovRFt2Ym9lIaeYGF52sqitFP1p4/ElSMPc/A34L/E9FZZqZtaSi8LkjbiSGXBZE\nxCH58XmSThmBMs3MGtYtQy5VNeizJL2bdCHRVEmKLa+Yb3tnZh3FV4rW91lgSn78RWBXYLWk3YFt\nLjYyMxtN7qHXERHn1Fi/UtLPqijTzKxVnT423rBmwka2YwHua1M+C7shj06oQ6fk0Ql18PPoztdi\nR1mUX7C2Giba4gERMb4NZVwXEQu29zw6oQ6dkkcn1KEdeXRCHTolj06ow47E0RbNzLqEoy2amXWJ\n7Tna4qIuyaMT6tApeXRCHdqRRyfUoVPy6IQ67DAqGUM3M7OR54t8zMy6hBt0M7MusV026JKOlnSH\npCWSzmwh/YWSHpK0uMXy95T0M0m/l3SrpHe2kMcESddKuinnMeTFWA3k0yPpBknfazH9vZJukXSj\npOtazGO6pIsl3S7pNkl/3GT6A3P5A8s6Se9qMo+/z6/jYkkXSZrQ3LMASe/M6W9ttPyhjiVJMyVd\nIemu/P+MFvJ4Ta5Hv6Rhp+zVyOPj+T25WdK3JU1vMv2Hc9obJV0uaY9m61DY9h5JIWnXFp7HhySt\nKBwfx9TLY4c22hPhm12AHmApsC+wE3AT8Mwm83gRcCiwuMU6zAEOzY+nAHe2UAeRZgIBjAN+AxzR\nQl3eDXwV+F6Lz+VeYNeS78kXgTfnxzsB00u+vyuBvZtIMxe4B9g5//0N4I1NlnswsBiYSJos8BPg\naa0cS8C/Amfmx2cC57aQxzOAA4ErSQHuWqnHy4Gx+fG59epRI/3UwuN3AJ9ptg55/Z7Aj4Flwx1r\nNerxIeAfyhyjO8qyPfbQDweWRMTdEfEU8DXg+GYyiIirgTWtViAiHoyI3+XHjwO3kRqVZvKIiFif\n/xyXl6bOUEuaB7wKuKCZdO0kaRrpQ/g5gIh4KiIeLZHlUcDSiFjWZLqxwM6SxpIa5QeaTP8M4DcR\nsSEieoGrgD8fLlGNY+l40pcc+f8Tms0jIm6LiDsarHutPC7PzwXgGmBek+nXFf6cxDDHZ53P1XnA\nGcOlHyYPa8D22KDPBe4v/L2cJhvTdpI0H3guqYfdbNoeSTcCDwFXRESzefw76YNSJup+AJdLuj7f\nMapZ+wCrgc/noZ8LJE0qUZ+TgIuaSRARK4BPAPeRYu8/FhGXN1nuYuCFknaRNBE4htSzbMXsiHgw\nP15JutButP0N8MNmE0n6iKT7gdcDH2wh/fHAioi4qdm0g5yeh38uHG4Ia0e2PTboHUPSZOAS4F2D\nejMNiYi+SLHi5wGHSzq4ibKPBR6KiOubLXeQP4mIQ4FXAm+X9KIm048l/UQ+PyKeCzxBGmZomqSd\ngOOAbzaZbgapV7wPsAcwSdJfNZNHRNxGGpa4HPgRKSpoXzN51Mg3aPKXV7tJOgvoBb7SbNqIOCsi\n9sxpT2+y3InAP9LCF8Eg5wP7AYeQvrA/WTK/rrU9Nugr2LrnNC+vG1GSxpEa869ExLfK5JWHKH4G\nHD3cvgUvAI6TdC9p2OlPJf1vC2WvyP8/BHybNKTVjOXA8sKvi4tJDXwrXgn8LiJWNZnupcA9EbE6\nIjYD3wKe32zhEfG5iDgsIl5ECltxZ7N5ZKskzQHI/z/UYj6lSXojcCzw+vzl0qqvAK9uMs1+pC/Z\nm/JxOg/4nVIY7YZFxKrc+eknheZu9hjdYWyPDfpvgf0l7ZN7dCcBl41kBSSJNGZ8W0T8W4t57DYw\n60DSzsDLgNsbTR8R74+IeRExn/Qa/F9ENNUrlTRJ0pSBx6STaE3N/ImIlcD9kg7Mq44Cft9MHgWv\no8nhluw+4AhJE/N7cxTpvEZTJM3K/+9FGj//agt1gXQ8DtyZ6xTg0hbzKUXp3r5nAMdFxIYW0u9f\n+PN4mjg+ASLiloiYFRHz83G6nDSZYGWT9ZhT+PNEmjxGdyijfVa2lYU0vnknabbLWS2kv4j0020z\n6SA7tcn0f0L6GX0z6af5jcAxTebxbOCGnMdi4IMlXo8jaWGWC2mm0E15ubWV1zLncwhwXX4u3wFm\ntJDHJOARYFqLdTiH1OAsBr4MjG8hj5+TvoxuAo5q9VgCdgF+CtxFmi0zs4U8TsyPNwGrgB+3kMcS\n0vmmgWO05iyVGukvya/nzcB3gbnN1mHQ9nsZfpbLUPX4MnBLrsdlwJxWPyvdvvjSfzOzLrE9DrmY\nmdkQ3KCbmXUJN+hmZl3CDbqZWZdwg25m1iXcoJuZdQk36GZmXeL/A39+WaVg+Sc6AAAAAElFTkSu\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = 46898 // incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "8de29306-9e2d-4da6-a165-2b73d5b103fe"
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "3c32c392-14a3-4332-a7d1-909563d4fcaf"
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[53., 44., 41., 39., 37., 37., 35., 35., 35., 39., 42., 45., 48., 52.,\n",
              "         60., 75.],\n",
              "        [44., 38., 36., 34., 33., 32., 31., 32., 31., 33., 34., 37., 39., 42.,\n",
              "         49., 58.],\n",
              "        [42., 36., 32., 31., 30., 29., 28., 29., 29., 30., 32., 32., 35., 38.,\n",
              "         42., 55.],\n",
              "        [38., 33., 33., 31., 29., 28., 29., 29., 28., 28., 31., 31., 33., 36.,\n",
              "         40., 52.],\n",
              "        [39., 33., 32., 29., 28., 26., 26., 28., 27., 28., 28., 29., 31., 33.,\n",
              "         41., 50.],\n",
              "        [39., 33., 32., 29., 28., 26., 27., 26., 27., 27., 28., 29., 30., 32.,\n",
              "         39., 48.],\n",
              "        [38., 35., 32., 28., 27., 26., 25., 25., 26., 26., 28., 29., 30., 33.,\n",
              "         37., 45.],\n",
              "        [39., 35., 32., 29., 27., 26., 26., 26., 27., 26., 27., 27., 29., 33.,\n",
              "         37., 46.],\n",
              "        [40., 35., 30., 29., 27., 26., 28., 26., 27., 27., 28., 28., 30., 32.,\n",
              "         36., 45.],\n",
              "        [44., 36., 32., 31., 28., 28., 28., 27., 26., 26., 28., 29., 31., 33.,\n",
              "         38., 43.],\n",
              "        [43., 35., 31., 30., 29., 28., 27., 28., 26., 26., 29., 29., 30., 32.,\n",
              "         35., 42.],\n",
              "        [47., 36., 32., 29., 30., 28., 27., 27., 28., 27., 29., 29., 31., 33.,\n",
              "         37., 42.],\n",
              "        [48., 39., 35., 31., 31., 31., 28., 28., 28., 29., 29., 30., 31., 34.,\n",
              "         36., 46.],\n",
              "        [50., 42., 37., 35., 33., 33., 31., 30., 31., 30., 30., 32., 33., 34.,\n",
              "         38., 47.],\n",
              "        [61., 49., 42., 38., 37., 35., 34., 33., 33., 33., 32., 33., 35., 38.,\n",
              "         40., 50.],\n",
              "        [73., 60., 50., 44., 45., 42., 40., 41., 37., 39., 39., 40., 40., 43.,\n",
              "         46., 56.]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    }
  ]
}