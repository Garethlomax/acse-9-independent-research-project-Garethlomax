{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/Model_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "outputId": "4cecc29e-e076-4a0c-d356-ccaf5509ec1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "outputId": "331b3bee-9caa-4e94-9ff6-00c8b02a832e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=58a401f51797492246f6bc471d0c5df89175893b2192e3a8545eaca9f0fd0c50\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROspCb3F4Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "outputId": "cf6d8e67-775b-47d1-bf1f-f8edb3f35aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.066s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "outputId": "fcbee595-e738-4d5e-dcfc-581745ae9851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    print(\"GPUs:\", torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "outputId": "54c62629-8633-45ae-97f2-97a9366b6457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "#         truth -= self.avg[0]\n",
        "#         truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7, threshold = 0.5):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    plt.figure()\n",
        "    x[sample][0][0][threshold > x[sample][0][0]] = 0\n",
        "    plt.imshow(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader, loss_func):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        #loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "outputId": "26dad087-cb47-431a-8959-1278188d5981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 5).to(device)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZFoI1Sk0On",
        "colab_type": "code",
        "outputId": "83f2a751-04c0-4b3a-8af0-3ec101969442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "f = h5py.File('test_fixed_25.hdf5','r')\n",
        "print(f['predictor'].shape)\n",
        "f.close()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "(2452, 10, 5, 16, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "outputId": "0fe29730-9cdd-42ed-f246-03ca9185aeb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"now changed to fixed dataset\"\"\"\n",
        "\n",
        "avg = np.load(\"fixed_25_avg.npy\")\n",
        "std = np.load(\"fixed_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('test_fixed_25.hdf5', valid_frac = 0.1, dataset_length = 2452,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"second_test\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "outputId": "d6964dab-b98d-4145-a21b-ea42e7d32738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "outputId": "934efa96-15d7-41f5-9f92-e201fea802e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_image_save(test_model, train_loader, name + \"comparison\", sample = 200, threshold = 0)\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([2000, 1, 5, 16, 16])\n",
            "torch.Size([2000, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD1CAYAAABX2p5TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGkNJREFUeJzt3Xu0XHV5//H3hxCSSsIlBJBAEipQ\nf2orEU9BLa1R7hEIvxYFqkgtEqm61LVEpeqvUBWxttjaQksRMICKjdZgpNwiinhFAg1yESSyArmZ\nCIFAIFwCz++P/T24GWbOmcye25nv57XWWWdfvvu7n9nzzDN79t6zRxGBmZnlY6teB2BmZt3lwm9m\nlhkXfjOzzLjwm5llxoXfzCwzLvxmZplx4R/jJO0pKSRtncavlnRSC/3MkLRR0rj2RznieneVdKOk\nxySd0811V1W77a15ku6UNLvXceTKhb8LJC2XtCkV1rWS5kua1Il1RcQREXFJkzEdXFrugYiYFBHP\ndiKuEcwDHgS2i4gPd3ndXSHpBknvbkM/syWtbEdMvRYRr4qIG3odR65c+LvnqIiYBOwHDAGfrG2g\nQm7PyUzgrvA3CbPgT0d9IiL81+E/YDlwcGn8H4Er0/ANwFnAj4FNwN7A9sBFwBpgFfAZYFxqPw74\nJ4q95PuA9wEBbF3q792ldZ0C/BJ4DLiL4o3nMuC5tL6NwEeBPWv6mQYsAtYDy4BTSn2eCSwALk39\n3gkMjfD43wDcDGxI/9+Qps8HngGeTnEcXGfZ/YElwKPAWuALpXmvA34CPALcBswuzZsCfBlYDTwM\nXFGzTZalx7YImFaaF8CpwL2p3/MANbPtGzz2s4BngSfTYzw3Tf8/wOIUwz3A20rLzEnP1WPp+T8N\n2DY9X8+lfjam52gc8HHg16n9LcD0kbZ7KU8+k7bfRuA7wE7AV9O2vhnYs2a7fCA97gcpcnirNG8v\n4HvAQ2neV4EdavL/Y8AvgKeArSm9JkZ5jo+myK9HUsyvqOn3tNTvBuC/gIm9fr2Phb+eB5DDX02S\nT0+J/Ok0fgPwAPCq9IIYDywE/jO92HcBfg68J7U/Fbg79TMF+D4NCj/w1lQ4/hgQxZvKzNqY0vie\nNf3cCPw7MBGYBfwWeHOadyZFIZtDUXjOBn7W4LFPoSi8J6bHd0Ia3ynNnw98ptT+QOCR0vhPgRPT\n8CTgdWl491Ro5lB8cj0kje+c5v9PKgQ7pm36xjT9zRTFaT9gAvBvwI2l9QVwJbADMCM97sOb2fYj\nPP/PPydpfFtgBfCutE1ek2J6ZZq/BvjTNLwjsF8ang2srOn7I8DtwMvTc7wvRQEfbbvfQPHmtxfF\njsZdwK+Ag1P7S4Ev12yX76d+Z6S2w3m2d9r+E4CdKXLnX2ryf2nabr9X5zXR6Dn+A+Dx1Pd4ih2U\nZcA2pT5+TvEGOIViB+fUXr/ex8JfzwPI4S8l6EaKvZb7KQrq8AvgBuBTpba7UuwV/V5p2gnA99Pw\n98rJDRxK48J/LfDBEWKqW/jTC/RZYHJp/tnA/DR8JvDd0rxXApsarOdE4Oc1034K/FUank+p8NdZ\n/kbg74GpNdM/BlxWM+1a4CRgN4o94x3r9HcR8PnS+CSKTx17pvEADizNXwCc3sy2H+ExPP+cpPHj\ngB/WtPlP4Iw0/ADwHorzHuU2s3lx4b8HmNvCdr8B+ERp3jnA1aXxo4ClpfEgvQGm8fcC1zd4vMcA\n/1uTa3/dKP9GeI7/H7CgNL4VxY7M7FIf7yjN/zxwfjtes4P+l9vx5F46JiJ2iIiZEfHeiNhUmrei\nNDyTYu9mjaRHJD1CURR2SfOn1bS/f4R1Tqc4BLClpgHrI+KxmvXsXhr/TWn4CWBig+O30+rEWNvX\nSE6m2PO7W9LNko5M02cCbx3eRmk7HUhR9Ken+B8eLZ6I2EjxSWGkxzZ8In5Ltv1IZgIH1MT+duCl\naf5fUHySuV/SDyS9foS+Gj3HzWz3taXhTXXGay9AqH3s0+D5K7O+LmmVpEeBrwBTR1i2VqPnuPa5\nei7108xzZSPwiZb+EKXhFRR7/FMjYnOdtmsoXuzDZozQ7wqKj/KjrbPWamCKpMml4j+DYm9rS62m\nKHRlM4Brmlk4Iu4FTkgnvf8c+KaknSge22URcUrtMpJ2S/HvEBGPjBSPpG0pDo0089i2ZNu/4GHU\njK8AfhARh9RtHHEzMFfSeOD9FJ86ptfpZ7ivvYA7aqZX2u4NDB+mHO5rdRr+bIrtjyJivaRjgHNr\nlm2YbyM8x6uBPxpuJ0kphlby0Eq8x99nImINcB1wjqTtJG0laS9Jb0xNFgAfkLSHpB2B00fo7kLg\nNEmvTVcM7S1puBisBV7WIIYVFCf9zpY0UdKrKfbKvtLCQ7oK+ANJfylpa0nHURwaurKZhSW9Q9LO\naW9vuIg/l2I5StJhksalOGdL2iNtw6uBf5e0o6Txkv4sLXs58C5JsyRNoChaN0XE8ibC2ZJtX1a7\nra+k2CYnptjGS/pjSa+QtI2kt0vaPiKeoTjh+Vypn50kbV/q60Lg05L2Sc/xq1PRrLTdG/hI2p7T\ngQ9SnEMBmExxKHODpN0pzjs0bYTneAHwFkkHpTfBD1PsFP2kwmMwXPj71TuBbShOuD0MfJPiEAbA\nlyiOZd8G3Ap8q1EnEfENiqtKvkZxxccVFCfBoDhm/8l0qOG0OoufQHHcfzXFyeYzIuK7W/pAIuIh\n4EiKF+1DFCfojoyIB+u1l/SnkjaWJh0O3JmmfRE4PiI2pTenuRRXtPyWYs/3I/wup0+kOHZ/N7AO\n+FCK57sUx47/m2IPfi/g+CYfTtPbvsYXgWMlPSzpX9OnqEPTeldTHK74B4qTo8OxL0+HTU6lOAxE\nRNxN8cZ1X3repgFfoCiQ11G8SVxEcX5oi7Z7k75NcdXQUoqT5xel6X9PcbJ8Q5re7HYZ1ug5vgd4\nB8UJ+AcpzjscFRFPV3gMxu8uUzMza0hSAPtExLJex2LVeY/fzCwzPrlr1gY1h6fKjoiIH3Y1GLNR\n+FCPmVlmfKjHzCwzLvxmZplx4Tczy4wLv5lZZlz4zcwy48JvZpYZF34zs8y48JuZZcaF38wsMy78\nZmaZceE3M8uMC7+ZWWZc+M3MMuPCb2aWGRd+M7PMuPCbmWXGhd/MLDMu/GZmmXHhNzPLjAu/mVlm\nXPjNzDLjwm9mlhkXfjOzzLjwm5llxoXfzCwzLvxmZplx4Tczy4wLv5lZZlz4zcwy48JvZpYZF34z\ns8y48JuZZcaFf4yQtKekkLR1Gr9a0kkt9DND0kZJ49ofpVn3SLpB0rt7HcdY5MLfZpKWS9qUiuta\nSfMlTWr3eiLiiIi4pMl4Di4t90BETIqIZ9sdk1kjtXnYwvJnSvpKO2PKmQt/ZxwVEZOA/YAh4JPl\nmSp425sBw59irXtcfDooIlYBVwN/mD6WniXpx8ATwMskbS/pIklrJK2S9JnhQzCSxkn6J0kPSroP\neEu579qPuZJOkfRLSY9JukvSfpIuA2YA30mfQD5a55DRNEmLJK2XtEzSKaU+z5S0QNKlqd87JQ11\nfMPZQGmQhyHpZEkPAN+TNFvSyprllks6WNLhwMeB49Lyt5WazZT045Sf10ma2r1HNna58HeQpOnA\nHOB/06QTgXnAZOB+YD6wGdgbeA1wKDBczE8BjkzTh4BjR1jPW4EzgXcC2wFHAw9FxInAA6RPIBHx\n+TqLfx1YCUxL6/ispDeX5h+d2uwALALObfbxmwHU5iGwIM16I/AK4LBRlr8G+CzwXymP9y3N/kvg\nXcAuwDbAaW0OfyC58HfGFZIeAX4E/IAiaQHmR8SdEbEZmELxpvChiHg8ItYB/wwcn9q+DfiXiFgR\nEeuBs0dY37uBz0fEzVFYFhH3jxZkemP6E+BjEfFkRCwFLqR4Axn2o4i4Kp0TuAzYt05XZq04M+X+\npgp9fDkifpX6WADMalNsA83H1jrjmIj4bnmCJIAVpUkzgfHAmjQPijfi4TbTatqPVMinA79uIc5p\nwPqIeKxmPeXDOb8pDT8BTJS0dXrzMqtixehNRlWbn22/kGIQufB3V5SGVwBPAVMbFNE1FAV92IwR\n+l0B7NXEOmutBqZImlwq/jOAVSMsY9aKenlYnvY48JLhkXSua+dRlrcW+VBPj0TEGuA64BxJ20na\nStJekt6YmiwAPiBpD0k7AqeP0N2FwGmSXpuuGNpb0sw0by3wsgYxrAB+ApwtaaKkVwMnA75sztqt\nYR4mv6L4NPkWSeMproSbULP8nr4arj28EXvrnRQnpO4CHga+CeyW5n0JuBa4DbgV+FajTiLiG8BZ\nwNeAx4ArKM4hQHFu4JOSHpFU78TXCcCeFHv/C4Ezag9TmbXB83lInQsVImID8F6KnZhVFJ8Aylf5\nfCP9f0jSrR2OdeApwp+gzMxy4j1+M7PMuPCbmWXGhd/MLDMu/GZmmXHhNzPLTKUvcKWbJ30RGAdc\nGBGfq5k/AbgUeC3wEHBcRCwfrd9tNCEmsm2V0MwaepLHeTqe0khtOpHbzmvrpGbyeljLhT99s+48\n4BCK621vlrQoIu4qNTsZeDgi9pZ0PPAPwHGj9T2RbTlAB7UamtmIborrR5zfqdx2XlsnjZbXZVUO\n9ewPLIuI+yLiaYo7OM6taTMXGP6xkG8CB6l0YxqzPuXctoFWpfDvzgtvsrQyTavbJt2PZgOwU4V1\nmnWDc9sGWt/cpE3SPIp71TPxd/dqMhvTnNfWj6rs8a/ihXeP3IMX39Xx+TbpF5+2pzgR9iIRcUFE\nDEXE0PgX3JvJrOvaltvOa+tHVQr/zcA+kn5f0jYUPyCyqKbNIuCkNHws8L3wzYGs/zm3baC1fKgn\nIjZLej/FHSTHARdHxJ2SPgUsiYhFwEXAZZKWAev53a9LmfUt57YNur68O+d2mhK+7M065aa4nkdj\nfdevwHFeWydtSV77m7tmZplx4Tczy4wLv5lZZlz4zcwy48JvZpYZF34zs8y48JuZZcaF38wsMy78\nZmaZ6Zu7c1r/uHb10lHbHDZtVhciMRtdM/kKztky7/GbmWXGhd/MLDMu/GZmmXHhNzPLjAu/mVlm\nWi78kqZL+r6kuyTdKemDddrMlrRB0tL093fVwjXrPOe2Dboql3NuBj4cEbdKmgzcImlxRNxV0+6H\nEXFkhfWYdZtz2wZay3v8EbEmIm5Nw48BvwR2b1dgZr3i3LZB15Zj/JL2BF4D3FRn9usl3Sbpakmv\nasf6zLrFuW2DqPI3dyVNAv4b+FBEPFoz+1ZgZkRslDQHuALYp0E/84B5ABN5SdWwrI4rV93SVLvD\npr22w5GMDe3Ibee19aNKe/ySxlO8ML4aEd+qnR8Rj0bExjR8FTBe0tR6fUXEBRExFBFD45lQJSyz\nytqV285r60dVruoRcBHwy4j4QoM2L03tkLR/Wt9Dra7TrBuc2zboqhzq+RPgROB2ScN3Sfo4MAMg\nIs4HjgX+RtJmYBNwfEREhXWadYNz2wZay4U/In4EaJQ25wLntroOs15wbtug8zd3zcwy48JvZpYZ\nF34zs8y48JuZZcY/vZiRI3f3F7Ns8PgnFbec9/jNzDLjwm9mlhkXfjOzzLjwm5llxoXfzCwzLvxm\nZplx4Tczy4wLv5lZZlz4zcwy48JvZpaZyoVf0nJJt0taKmlJnfmS9K+Slkn6haT9qq7TrNOc1zbI\n2nWvnjdFxIMN5h1B8SPU+wAHAP+R/pv1O+e1DaRuHOqZC1wahZ8BO0jarQvrNesk57WNWe0o/AFc\nJ+kWSfPqzN8dWFEaX5mmmfUz57UNrHYc6jkwIlZJ2gVYLOnuiLhxSztJL655ABN5SRvCMqvEeW0D\nq/Ief0SsSv/XAQuB/WuarAKml8b3SNNq+7kgIoYiYmg8E6qGZVaJ89oGWaXCL2lbSZOHh4FDgTtq\nmi0C3pmugngdsCEi1lRZr1knOa9t0FU91LMrsFDScF9fi4hrJJ0KEBHnA1cBc4BlwBPAuyqu06zT\nnNc20CoV/oi4D9i3zvTzS8MBvK/KenL2ifuWjtrmrJf5p+fayXndedeuHj2v/ZOKneNv7pqZZcaF\n38wsMy78ZmaZceE3M8uMC7+ZWWZc+M3MMuPCb2aWGRd+M7PMuPCbmWWmXT/EYh3ib+XaIPK3cnvL\ne/xmZplx4Tczy4wLv5lZZlz4zcwy48JvZpaZlgu/pJdLWlr6e1TSh2razJa0odTm76qHbNZZzm0b\ndC1fzhkR9wCzACSNo/i90YV1mv4wIo5sdT1m3ebctkHXrkM9BwG/joj729SfWb9wbtvAadcXuI4H\nLm8w7/WSbgNWA6dFxJ1tWmdT1r3vDU21m/rnK0Zts/gV32mqL385ZaD0bW63SzM/gwjO60FSeY9f\n0jbA0cA36sy+FZgZEfsC/wZcMUI/8yQtkbTkGZ6qGpZZZe3Ibee19aN2HOo5Arg1ItbWzoiIRyNi\nYxq+ChgvaWq9TiLigogYioih8UxoQ1hmlVXObee19aN2FP4TaPBRWNJLJSkN75/W91Ab1mnWDc5t\nG0iVjvFL2hY4BHhPadqpABFxPnAs8DeSNgObgOMjIqqs06wbnNs2yCoV/oh4HNipZtr5peFzgXOr\nrMOsF5zbNsj8zV0zs8y48JuZZcaF38wsMy78ZmaZGfifXtzlvJ801/C80Zschr+5aIPH38jNj/f4\nzcwy48JvZpYZF34zs8y48JuZZcaF38wsMy78ZmaZceE3M8uMC7+ZWWZc+M3MMtNU4Zd0saR1ku4o\nTZsiabGke9P/HRsse1Jqc6+kk5pcH1tNnDjqn1kV3c5rs37R7B7/fODwmmmnA9dHxD7A9Wn8BSRN\nAc4ADgD2B85o9EIy64H5OK8tQ00V/oi4EVhfM3kucEkavgQ4ps6ihwGLI2J9RDwMLObFLzSznnBe\nW66qHOPfNSLWpOHfALvWabM7sKI0vjJNM+tXzmsbeG05uZt+a7TS741KmidpiaQlT/NUO8Iyq6Td\nef2M89r6RJXCv1bSbgDp/7o6bVYB00vje6RpLxIRF0TEUEQMbcOECmGZVdKxvB7vvLY+UaXwLwKG\nr2Y4Cfh2nTbXAodK2jGd/Do0TTPrV85rG3jNXs55OfBT4OWSVko6GfgccIike4GD0ziShiRdCBAR\n64FPAzenv0+laWY957y2XKk4jNlftt9qp3jdxDmjtnvuySe7EI0Nmpvieh6N9er2erfTlDhAB3V7\ntZaJLcnrvvzpxYhwUTcz6xDfssHMLDMu/GZmmXHhNzPLjAu/mVlmXPjNzDLjwm9mlhkXfjOzzLjw\nm5llxoXfzCwzLvxmZplx4Tczy4wLv5lZZlz4zcwy48JvZpaZUQu/pIslrZN0R2naP0q6W9IvJC2U\ntEODZZdLul3SUklL2hm4WVXObctVM3v884HDa6YtBv4wIl4N/Ar42xGWf1NEzIqIodZCNOuY+Ti3\nLUOjFv6IuBFYXzPtuojYnEZ/RvFj02ZjinPbctWOY/x/DVzdYF4A10m6RdK8NqzLrJuc2zaQKv30\noqRPAJuBrzZocmBErJK0C7BY0t1pL6teX/OAeQATeUlT67929dJR2xw2bVZTfZmVtSu3W8lrs05r\neY9f0l8BRwJvjwa/2B4Rq9L/dcBCYP9G/UXEBRExFBFD45nQalhmlbUzt53X1o9aKvySDgc+Chwd\nEU80aLOtpMnDw8ChwB312pr1C+e25aCZyzkvB34KvFzSSkknA+cCkyk+4i6VdH5qO03SVWnRXYEf\nSboN+DnwPxFxTUcehVkLnNuWq1GP8UfECXUmX9Sg7WpgThq+D9i3UnRmHeTctlz5m7tmZplx4Tcz\ny4wLv5lZZlz4zcwy48JvZpaZSt/c7TV/K9fMbMt5j9/MLDMu/GZmmXHhNzPLjAu/mVlmXPjNzDLj\nwm9mlhkXfjOzzLjwm5llxoXfzCwzzfwQy8WS1km6ozTtTEmr0g9VLJU0p8Gyh0u6R9IySae3M3Cz\nqpzblqtm9vjnA4fXmf7PETEr/V1VO1PSOOA84AjglcAJkl5ZJVizNpuPc9syNGrhj4gbgfUt9L0/\nsCwi7ouIp4GvA3Nb6MesI5zblqsqx/jfL+kX6ePyjnXm7w6sKI2vTNPM+p1z2wZaq4X/P4C9gFnA\nGuCcqoFImidpiaQlz/BU1e7MWtXW3HZeWz9qqfBHxNqIeDYingO+RPHRt9YqYHppfI80rVGfF0TE\nUEQMjWdCK2GZVdbu3HZeWz9qqfBL2q00+n+BO+o0uxnYR9LvS9oGOB5Y1Mr6zLrFuW05GPWHWCRd\nDswGpkpaCZwBzJY0CwhgOfCe1HYacGFEzImIzZLeD1wLjAMujog7O/IozFrg3LZcKSJ6HcOLbKcp\ncYAO6nUYNqBuiut5NNar2+t1XlsnbUle92Xhl/Rb4P7SpKnAgz0Kpx3GcvxjOXaoH//MiNi524HU\nyWsYzO07Vozl2OHF8Ted131Z+GtJWhIRQ72Oo1VjOf6xHDv0f/z9Ht9oxnL8Yzl2qBa/79VjZpYZ\nF34zs8yMlcJ/Qa8DqGgsxz+WY4f+j7/f4xvNWI5/LMcOFeIfE8f4zcysfcbKHr+ZmbVJ3xf+sX7f\nc0nLJd2e7u2+pNfxjKTB/emnSFos6d70v95Ny/pClfvrd5vzurvGcm53Iq/7uvAP0H3P35Tu7d7v\nl47N58X3pz8duD4i9gGuT+P9aj4t3F+/25zXPTGfsZvb82lzXvd14cf3Pe+qBvennwtckoYvAY7p\nalBboML99bvNed1lYzm3O5HX/V74B+G+5wFcJ+kWSfN6HUwLdo2INWn4N8CuvQymRaPdX7/bnNf9\nYazndst53e+FfxAcGBH7UXysf5+kP+t1QK2K4hKwsXYZWNt/O8KAAcprGJO5XSmv+73wb9E9/ftR\nRKxK/9cBC6l/f/d+tnb4VsXp/7oex7NFmry/frc5r/vDmM3tqnnd74V/TN/3XNK2kiYPDwOHUv/+\n7v1sEXBSGj4J+HYPY9liTd5fv9uc1/1hzOZ21bwe9X78vTQA9z3fFVgoCYpt/bWIuKa3ITXW4P70\nnwMWSDqZ4s6Sb+tdhCPbkvvr95LzuvvGcm53Iq/9zV0zs8z0+6EeMzNrMxd+M7PMuPCbmWXGhd/M\nLDMu/GZmmXHhNzPLjAu/mVlmXPjNzDLz/wEWKNKubO2KHQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfqlR/wdeir44XqjPGwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAAbuCAYAAAAIX+1GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceaV2m7ZdHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(model, train_loader, avg = 'macro'):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "#     print(x[0][0][0])\n",
        "    print(x[222,0,0].shape)\n",
        "    print(b[222].shape)\n",
        "#     print(b)\n",
        "    print(b[222].view(-1, 256).numpy().shape)\n",
        "    truth = set(list(b[222].view(256).numpy()))\n",
        "    pred = set(list(x[222,0,0].view(256).numpy()))\n",
        "    print(truth - pred)\n",
        "    scores = []\n",
        "    for i in range(len(b)):\n",
        "        score = f1_score(b[i].view(256).numpy(), x[i,0,0].view(256).numpy(), average=avg)\n",
        "        scores.append(score)\n",
        "        truth = set(list(b[i].view(256).numpy()))\n",
        "        pred = set(list(x[i,0,0].view(256).numpy()))\n",
        "        if len(truth - pred) > 0:\n",
        "            print(i)\n",
        "#     score = f1_score(b[222].numpy(), b[222].numpy(), average=avg)\n",
        "    return scores\n",
        "#     print(score)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKhzR5rvPov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(model, train_loader, name = 'default'):\n",
        "    \"\"\"Calculate TN, FN, TP, FP for multilabel classification\"\"\"\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    # reshape\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = x[:,0,0].view(-1,256).numpy()\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0 \n",
        "    fp = 0\n",
        "    \n",
        "    print(truth.shape)\n",
        "    print(pred.shape)\n",
        "    for i in range(len(b)):\n",
        "        for j in range(256):\n",
        "            # true positive\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 1):\n",
        "                tp += 1\n",
        "            # true negative\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 0):\n",
        "                tn += 1\n",
        "            \n",
        "            #false positive\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 1):\n",
        "                fp +=1\n",
        "            #false negative\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 0):\n",
        "                fn += 1\n",
        "    \n",
        "    print(\"tn:\" ,tn)\n",
        "    print(\"tp:\" , tp)\n",
        "    print(\"fn:\" , fn)\n",
        "    print(\"fp\" ,fp)\n",
        "    \n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp/ (tp + fn)\n",
        "    \n",
        "    f_1 = 2 * prec * rec / (prec + rec)\n",
        "    \n",
        "    print(\"prec:\" ,prec)\n",
        "    print(\"rec:\" , rec)\n",
        "    print(\"f1: \", f_1)\n",
        "    \n",
        "    f = open(name + \"metrics.csv\", 'w')\n",
        "    for i in [tn, tp, fn, fp, prec, rec, f_1]:\n",
        "        \n",
        "        f.write(str(i) + \"\\n\")\n",
        "\n",
        "    f.close()\n",
        "    \n",
        "                \n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6iVIEUNEPR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def area_under_curve_metrics(model, train_loader):\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "#     truth = b.contiguous().view(-1).numpy()\n",
        "#     pred = sig(x[:,0,0].contiguous().view(-1)).numpy()\n",
        "#     fpr, tpr, thresholds = roc_curve(truth, pred)\n",
        "#     plt.plot(fpr, tpr)\n",
        "#     plt.plot(fpr, fpr)\n",
        "    print(roc_auc_score(truth, pred))\n",
        "    print(average_precision_score(truth, pred))\n",
        "    \n",
        "        \n",
        "        \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZs5gRhzGYwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "74339954-0616-4787-84f8-573ea91b83cd"
      },
      "source": [
        "area_under_curve_metrics(test_model, train_loader)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "0.9016222699807742\n",
            "0.47130001720490283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPsmKD10Kg31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def curves(model, train_loader):\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "    plt.figure()\n",
        "    for i in range(16):\n",
        "        for j in range(16):\n",
        "            t = b[i,j].contiguous().view(-1).numpy()\n",
        "            p = sig(x[:,0,0,i,j].contiguous().view(-1)).numpy()\n",
        "            fpr, tpr, thresholds = roc_curve(t, p)\n",
        "            plt.plot(fpr, tpr)\n",
        "    plt.plot(fpr, fpr)\n",
        "    plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXD5JpCaLIzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "curves()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HLsqwEvahiy",
        "colab_type": "code",
        "outputId": "f0c8eee1-2b29-49c7-a1df-516044f81fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "    scores = f1(test_model, train_loader, avg = 'binary')\n",
        "    np.save(name + \"scores\", scores)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 16])\n",
            "(1, 256)\n",
            "set()\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "659\n",
            "1046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLTv_en0WN3",
        "colab_type": "code",
        "outputId": "06a56143-bfb5-4bf3-ca2f-a07feee61ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "metrics(test_model, train_loader, name = name)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 488583\n",
            "tp: 9492\n",
            "fn: 4796\n",
            "fp 9129\n",
            "prec: 0.509747059771226\n",
            "rec: 0.6643337066069429\n",
            "f1:  0.5768634719985415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiCx0wFnYxf",
        "colab_type": "code",
        "outputId": "d330d347-fd3e-4d4d-8829-46b7313a5156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5565801578011597"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe7BIv-liRj",
        "colab_type": "code",
        "outputId": "4fb90588-cc7b-44f2-afce-1f9a2a2f72ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import seaborn as sns\n",
        "plot = sns.distplot(scores)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFRflPCJ6VAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8h8Kkd7X86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0b54222-c9d6-45ae-aa23-4912dcf993eb"
      },
      "source": [
        "plot.set_title(\"Histogram of average F1 Score per Image prediction\")"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Histogram of average F1 Score per Image prediction')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEHaB5b8EQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszwtyDuVvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, b in train_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7oAiBWWX-2",
        "colab_type": "code",
        "outputId": "ee422e4d-7b3f-4b7a-8ccb-9d3cb2c736e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.count_nonzero(b) / (2000)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwUwgS2W3GC",
        "colab_type": "text"
      },
      "source": [
        "# testing flipping the input direction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nD-LmrX2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d, b in train_loader:\n",
        "    break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMHy4qTW7a9",
        "colab_type": "code",
        "outputId": "51b3ce93-4e95-4cde-9f09-dc8a70eb7299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# set up some axes\n",
        "a = d\n",
        "a = a.numpy()\n",
        "c = np.flip(a, 1)\n",
        "\n",
        "# c = torch.flip(a, (0,1))\n",
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(c[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAOICAYAAAAU5r/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1152x1152 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08NdPNhZoJG",
        "colab_type": "code",
        "outputId": "61b45a71-d42a-41f9-a147-c00d87e2b8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c.shape\n",
        "c = np.array(c)\n",
        "torch.tensor(c)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.2740, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026,  0.0377,  0.7753],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.4872,  1.4265],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.1818, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0205],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907, -1.1484],\n",
              "           [-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907,  0.2907],\n",
              "           [-0.7675, -0.7675, -0.7675,  ..., -0.1003,  0.6341,  0.6341],\n",
              "           ...,\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.4469,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]]],\n",
              "\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ...,  0.0089, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0147],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.6831, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY3GRCuQYkQZ",
        "colab_type": "code",
        "outputId": "1a1574fe-899d-49b2-eaeb-2cfbbab0433d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "e.shape"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-f73a2bc07fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBo6C1MAZdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = torch.tensor(c)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGYIr8YYKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(e.numpy()[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsDysmeW87k",
        "colab_type": "text"
      },
      "source": [
        "# after flipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQF2DnA6yB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d) // 3\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "losses = batch_loss_histogram(test_model, train_loader, loss_func = c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ic2GUlrX799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sirv8aO61kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbxmoet7IUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d).to(device)\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCoJCKbhiPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloIqpwpW6Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a,b in train_loader:\n",
        "    break\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbcXFd1pU6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = a.to(device)\n",
        "b = b.to(device)\n",
        "c(a[0][0][0],b[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIbwcFpW99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b[0]\n",
        "# sdaddasdasadad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "# truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = (46898  - incident_map)// incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8uk9UI6hGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UQbluAaTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)\n",
        "loss_default = nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg-X4tXNanfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = b[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_KNeqBapXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d[1 > d] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhnCUQDawzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsFFKJ7WaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(a[0][-1][0],b[0]))\n",
        "print(loss_default(a[0][-1][0], b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEo_Gr8PXhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.ones_like(a[0][-1][0])\n",
        "c *= -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8aP4eTX7cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizNoE4vXoGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(c,b[0]))\n",
        "print(loss_default(c, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyngsiPa5Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(d,b[0]))\n",
        "print(loss_default(d, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9AGiQKOePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = batch_loss_histogram(test_model, train_loader, loss_func)\n",
        "l2 = batch_loss_histogram(test_model, train_loader, loss_default)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-l3gnzjPGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(l1)\n",
        "plt.figure()\n",
        "sns.distplot(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}