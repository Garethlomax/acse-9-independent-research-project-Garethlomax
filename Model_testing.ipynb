{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/Model_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "outputId": "4cecc29e-e076-4a0c-d356-ccaf5509ec1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "outputId": "331b3bee-9caa-4e94-9ff6-00c8b02a832e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=58a401f51797492246f6bc471d0c5df89175893b2192e3a8545eaca9f0fd0c50\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dROspCb3F4Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfGC8lSoPcJ-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "outputId": "cf6d8e67-775b-47d1-bf1f-f8edb3f35aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.066s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "outputId": "fcbee595-e738-4d5e-dcfc-581745ae9851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    print(\"GPUs:\", torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "outputId": "54c62629-8633-45ae-97f2-97a9366b6457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "#         truth -= self.avg[0]\n",
        "#         truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7, threshold = 0.5):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    plt.figure()\n",
        "    x[sample][0][0][threshold > x[sample][0][0]] = 0\n",
        "    plt.imshow(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader, loss_func):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        #loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "outputId": "26dad087-cb47-431a-8959-1278188d5981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 5).to(device)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZFoI1Sk0On",
        "colab_type": "code",
        "outputId": "83f2a751-04c0-4b3a-8af0-3ec101969442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "f = h5py.File('test_fixed_25.hdf5','r')\n",
        "print(f['predictor'].shape)\n",
        "f.close()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "(2452, 10, 5, 16, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "outputId": "0fe29730-9cdd-42ed-f246-03ca9185aeb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"now changed to fixed dataset\"\"\"\n",
        "\n",
        "avg = np.load(\"fixed_25_avg.npy\")\n",
        "std = np.load(\"fixed_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('test_fixed_25.hdf5', valid_frac = 0.1, dataset_length = 2452,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"recent_test\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "outputId": "246de043-b591-4ab3-ae11-b79337d8baea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "outputId": "3f5c0acf-b896-4896-94d8-1acb3673e167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_image_save(test_model, train_loader, name + \"comparison\", sample = 200, threshold = 0)\n"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([2000, 1, 5, 16, 16])\n",
            "torch.Size([2000, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD3CAYAAAAT+Z8iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAEBCAYAAABxB7CHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL8AAAbzCAYAAAC0oPIrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqMF5pAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceaV2m7ZdHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(model, train_loader, avg = 'macro'):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "#     print(x[0][0][0])\n",
        "    print(x[222,0,0].shape)\n",
        "    print(b[222].shape)\n",
        "#     print(b)\n",
        "    print(b[222].view(-1, 256).numpy().shape)\n",
        "    truth = set(list(b[222].view(256).numpy()))\n",
        "    pred = set(list(x[222,0,0].view(256).numpy()))\n",
        "    print(truth - pred)\n",
        "    scores = []\n",
        "    for i in range(len(b)):\n",
        "        score = f1_score(b[i].view(256).numpy(), x[i,0,0].view(256).numpy(), average=avg)\n",
        "        scores.append(score)\n",
        "        truth = set(list(b[i].view(256).numpy()))\n",
        "        pred = set(list(x[i,0,0].view(256).numpy()))\n",
        "        if len(truth - pred) > 0:\n",
        "            print(i)\n",
        "#     score = f1_score(b[222].numpy(), b[222].numpy(), average=avg)\n",
        "    return scores\n",
        "#     print(score)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKhzR5rvPov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    \"\"\"Calculate TN, FN, TP, FP for multilabel classification\"\"\"\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    # reshape\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = x[:,0,0].view(-1,256).numpy()\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0 \n",
        "    fp = 0\n",
        "    \n",
        "    print(truth.shape)\n",
        "    print(pred.shape)\n",
        "    for i in range(len(b)):\n",
        "        for j in range(256):\n",
        "            # true positive\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 1):\n",
        "                tp += 1\n",
        "            # true negative\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 0):\n",
        "                tn += 1\n",
        "            \n",
        "            #false positive\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 1):\n",
        "                fp +=1\n",
        "            #false negative\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 0):\n",
        "                fn += 1\n",
        "\n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp/ (tp + fn)\n",
        "    \n",
        "    f_1 = 2 * prec * rec / (prec + rec)                \n",
        "                \n",
        "    if verbose:\n",
        "        print(\"tn:\" ,tn)\n",
        "        print(\"tp:\" , tp)\n",
        "        print(\"fn:\" , fn)\n",
        "        print(\"fp\" ,fp)\n",
        "        print(\"prec:\" ,prec)\n",
        "        print(\"rec:\" , rec)\n",
        "        print(\"f1: \", f_1)\n",
        "    \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'w')\n",
        "        for i in [tn, tp, fn, fp, prec, rec, f_1]:\n",
        "        \n",
        "            f.write(str(i) + \"\\n\")\n",
        "\n",
        "        f.close()\n",
        "    \n",
        "                \n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6iVIEUNEPR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def area_under_curve_metrics(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "#     truth = b.contiguous().view(-1).numpy()\n",
        "#     pred = sig(x[:,0,0].contiguous().view(-1)).numpy()\n",
        "#     fpr, tpr, thresholds = roc_curve(truth, pred)\n",
        "#     plt.plot(fpr, tpr)\n",
        "#     plt.plot(fpr, fpr)\n",
        "    r = roc_auc_score(truth, pred)\n",
        "    av = average_precision_score(truth, pred)\n",
        "    if verbose:\n",
        "        print(r)\n",
        "        print(av)\n",
        "    \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'a')\n",
        "        for i in [r,av]:\n",
        "        \n",
        "            f.write(str(i) + \"\\n\")\n",
        "\n",
        "        f.close()\n",
        "    \n",
        "        \n",
        "        \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDKnjygZerpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def brier_score(model, train_loader, name = 'default', verbose = True, save = True):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    diff = (x[:,0,0] - b)**2\n",
        "    brier = np.average(diff)\n",
        "    \n",
        "    if verbose:\n",
        "        print(brier)\n",
        "        \n",
        "    if save:\n",
        "        f = open(name + \"metrics.csv\", 'a')\n",
        "        f.write(str(brier) + \"\\n\")\n",
        "        f.close()\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgdaoLwaU8j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_metrics(model, train_loader, name = 'default'):\n",
        "    metrics(model, train_loader, name = name)\n",
        "    area_under_curve_metrics(model, train_loader, name = name)\n",
        "    brier_score(model, train_loader, name = name)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWepJCa5W_sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZs5gRhzGYwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "dabd4492-c330-4f69-d2b6-5d71fd172f56"
      },
      "source": [
        "\n",
        "\n",
        "full_metrics(test_model, train_loader, name)"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482628\n",
            "tp: 10080\n",
            "fn: 4208\n",
            "fp 15084\n",
            "prec: 0.4005722460658083\n",
            "rec: 0.7054871220604704\n",
            "f1:  0.511000709723208\n",
            "FINISHING ONE PASS\n",
            "0.914238896175356\n",
            "0.4631013734192746\n",
            "FINISHING ONE PASS\n",
            "0.0376796875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPsmKD10Kg31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def curves(model, train_loader):\n",
        "#     dataframe = pd.dataframe()\n",
        "    sig = nn.Sigmoid()\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "        \n",
        "    x = x.cpu()\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = sig(x[:,0,0].view(-1,256)).numpy()\n",
        "    plt.figure()\n",
        "#     for i in range(16):\n",
        "    for j in range(16):\n",
        "        t = b[:,j,j].contiguous().view(-1).numpy()\n",
        "        p = sig(x[:,0,0,j,j].contiguous().view(-1)).numpy()\n",
        "        fpr, tpr, thresholds = roc_curve(t, p)\n",
        "        plt.plot(fpr, tpr)\n",
        "    plt.plot(fpr, fpr)\n",
        "    plt.xlim(0, 1.1)\n",
        "    plt.ylim(0,1.1)\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXD5JpCaLIzd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "4df344c1-f8cf-4dc0-c5ed-b9cbd17b3ef4"
      },
      "source": [
        "curves(test_model, train_loader)\n",
        "# sns.set()\n",
        "# x = [1,2,3]\n",
        "# y = [4, 5,7]\n",
        "# ax = sns.lineplot(x, y, palette= 'red')\n",
        "# ax.plot([1,2], [11,22])"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEBCAYAAAB/rs7oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-OCXRDP3cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etMgGdJjf2Tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a1789879-23e6-49cc-9d98-8b4d02467688"
      },
      "source": [
        "brier_score(test_model, train_loader)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "0.0376796875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HLsqwEvahiy",
        "colab_type": "code",
        "outputId": "8c9945bd-b1da-4138-f560-e8ca59e5d16e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "    scores = f1(test_model, train_loader, avg = 'binary')\n",
        "    np.save(name + \"scores\", scores)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 16])\n",
            "(1, 256)\n",
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLTv_en0WN3",
        "colab_type": "code",
        "outputId": "7a143e18-cc24-456c-a7c5-eb091400858a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "metrics(test_model, train_loader, name = name)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 482628\n",
            "tp: 10080\n",
            "fn: 4208\n",
            "fp 15084\n",
            "prec: 0.4005722460658083\n",
            "rec: 0.7054871220604704\n",
            "f1:  0.511000709723208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiCx0wFnYxf",
        "colab_type": "code",
        "outputId": "0c66b19e-d7af-4cb9-cbf5-0816b0cbd12d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5003645562832529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe7BIv-liRj",
        "colab_type": "code",
        "outputId": "243d70ae-4d3c-4273-aa41-5cf4452ddcf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "import seaborn as sns\n",
        "plot = sns.distplot(scores)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEBCAYAAAB/rs7oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8h8Kkd7X86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "083c18d6-b6d1-467a-e4bb-c6ff79d96bba"
      },
      "source": [
        "plot.set_title(\"Histogram of average F1 Score per Image prediction\")"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Histogram of average F1 Score per Image prediction')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEHaB5b8EQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszwtyDuVvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a, b in train_loader:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK7oAiBWWX-2",
        "colab_type": "code",
        "outputId": "ee9a7574-ebf6-4da2-a661-dceda1566f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.count_nonzero(b) / (2000)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwUwgS2W3GC",
        "colab_type": "text"
      },
      "source": [
        "# testing flipping the input direction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nD-LmrX2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d, b in train_loader:\n",
        "    break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMHy4qTW7a9",
        "colab_type": "code",
        "outputId": "bd713e38-e303-4549-ef74-0cb618292f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "# set up some axes\n",
        "a = d\n",
        "a = a.numpy()\n",
        "c = np.flip(a, 1)\n",
        "\n",
        "# c = torch.flip(a, (0,1))\n",
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(c[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAONCAYAAABTCb6OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9LVVgwwuyQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x1152 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08NdPNhZoJG",
        "colab_type": "code",
        "outputId": "fa783757-0cb1-4905-bfae-96f39c81cb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c.shape\n",
        "c = np.array(c)\n",
        "torch.tensor(c)"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.2740, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026,  0.0377,  0.7753],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.4872,  1.4265],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.1818, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0205],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907, -1.1484],\n",
              "           [-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907,  0.2907],\n",
              "           [-0.7675, -0.7675, -0.7675,  ..., -0.1003,  0.6341,  0.6341],\n",
              "           ...,\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.4469,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]]],\n",
              "\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ...,  0.0089, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0147],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.6831, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY3GRCuQYkQZ",
        "colab_type": "code",
        "outputId": "d5adac63-fa12-41c6-f6b1-421e3f65eb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "e.shape"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-f73a2bc07fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBo6C1MAZdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = torch.tensor(c)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGYIr8YYKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(e.numpy()[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsDysmeW87k",
        "colab_type": "text"
      },
      "source": [
        "# after flipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQF2DnA6yB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d) // 3\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "losses = batch_loss_histogram(test_model, train_loader, loss_func = c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ic2GUlrX799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sirv8aO61kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbxmoet7IUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d).to(device)\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCoJCKbhiPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloIqpwpW6Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a,b in train_loader:\n",
        "    break\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbcXFd1pU6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = a.to(device)\n",
        "b = b.to(device)\n",
        "c(a[0][0][0],b[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIbwcFpW99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b[0]\n",
        "# sdaddasdasadad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "# truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = (46898  - incident_map)// incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8uk9UI6hGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UQbluAaTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)\n",
        "loss_default = nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg-X4tXNanfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = b[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_KNeqBapXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d[1 > d] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhnCUQDawzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsFFKJ7WaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(a[0][-1][0],b[0]))\n",
        "print(loss_default(a[0][-1][0], b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEo_Gr8PXhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.ones_like(a[0][-1][0])\n",
        "c *= -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8aP4eTX7cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizNoE4vXoGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(c,b[0]))\n",
        "print(loss_default(c, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyngsiPa5Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(d,b[0]))\n",
        "print(loss_default(d, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9AGiQKOePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = batch_loss_histogram(test_model, train_loader, loss_func)\n",
        "l2 = batch_loss_histogram(test_model, train_loader, loss_default)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-l3gnzjPGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(l1)\n",
        "plt.figure()\n",
        "sns.distplot(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}