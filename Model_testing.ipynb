{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/Model_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdjQiLORit87",
        "colab_type": "text"
      },
      "source": [
        "Notebook for testing and visualising the trained models, instead of just editing in and out of the other note books. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF1hCBBflpPE",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNCK1plivBa",
        "colab_type": "code",
        "outputId": "0a562842-470b-4024-d764-21217bd99fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pw1B9CRiq4b",
        "colab_type": "code",
        "outputId": "a445d8f7-84cc-4a5d-847a-6bef31356678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import f1_score, multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HjvzMDSmjvm",
        "colab_type": "code",
        "outputId": "c24ab3d1-42bb-493f-d1fa-12803ec07fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.395s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93GFSfjbmn9p",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng6nuRUemmId",
        "colab_type": "code",
        "outputId": "3e824ba2-2627-41fa-a8b3-e5254c9da0c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    print(\"GPUs:\", torch.cuda.device_count())\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n",
            "GPUs: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ygxsDfm13g",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYrqiJO2m3r7",
        "colab_type": "text"
      },
      "source": [
        "## LSTM CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QABn4VwLm1No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqL4TQZm9ux",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Full Unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_4SSRxnrvii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6r5pzTnEp1",
        "colab_type": "text"
      },
      "source": [
        "## lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6f9sKamnGsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3OsS3LnJST",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OliGMQernKxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhOY6M2nNkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaaxPlgInPbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(dataset, valid_frac = 0.1, dataset_length = 9000, avg = 0, std = 0, application_boolean = [0,0,0,0,0]):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_index = list(train_index)\n",
        "        \n",
        "        valid_index = list(valid_index)\n",
        "        \n",
        "        train_dataset = HDF5Dataset_with_avgs(dataset,train_index, avg, std, application_boolean)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset_with_avgs(dataset,valid_index, avg, std, application_boolean)\n",
        "        \n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgXbH9ufnRUQ",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeG22ZLUnSwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FtVqEhenUxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-ycpijnWaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnJNW6pcnYVS",
        "colab_type": "text"
      },
      "source": [
        "# training functions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id-1ba_mnaMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88roEYKncdq",
        "colab_type": "code",
        "outputId": "ce3c7988-dccf-499d-d931-2573e9c66dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "\n",
        "    # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss / i # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, loss_func = nn.MSELoss()):\n",
        "    \n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    tot_loss = 0\n",
        "    i = 0\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad(): # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x = x.to(device) # send to cuda. need to change = sign as to(device)\n",
        "            y = y.to(device) # produces a copy on thd gpu not moves it. \n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction[:,0,0], y)\n",
        "            \n",
        "            tot_loss += loss.item()\n",
        "            i += 1\n",
        "            \n",
        "            print(\"MSE_VALIDATION_LOSS:\", tot_loss / i)\n",
        "            \n",
        "    \n",
        "    \n",
        "    return tot_loss / i # returns total loss averaged across the dataset. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        \n",
        "        \n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Qh0HFanfZd",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxMIqmhTng9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "#         print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "#         print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "#         truth -= self.avg[0]\n",
        "#         truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxJN-sRn2Vx",
        "colab_type": "text"
      },
      "source": [
        "## save fig def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxHgHdoYn3qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_image_save(model, train_loader, name, sample = 7, threshold = 0.5):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "\n",
        "    x = x.cpu()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(1,2)\n",
        "    print(x.shape)\n",
        "    print(b.shape)\n",
        "    axes[0].imshow(x[sample][0][0])\n",
        "    axes[1].imshow(b[sample])\n",
        "    \n",
        "    axes[1].set_title(\"truth\")\n",
        "    axes[0].set_title(\"Prediction\")\n",
        "    fig.suptitle(\"Prediction of:\" + name)\n",
        "    fig.savefig(name + \"sample\"+ str(sample) + \"comparison.pdf\")\n",
        "#     print(b[7])\n",
        "#     print(x[7][0][0])\n",
        "    plt.figure()\n",
        "    x[sample][0][0][threshold > x[sample][0][0]] = 0\n",
        "    plt.imshow(x[sample][0][0])\n",
        "    fig, axes = plt.subplots(10,1,figsize=(32,32))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(a[sample][i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3LLyGvaoTug",
        "colab_type": "text"
      },
      "source": [
        "## batch loss histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6Zf6jzoVwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_loss_histogram(model, train_loader, loss_func):\n",
        "    \n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "        x = x.cpu()\n",
        "#     print(x.shape)\n",
        "    # now over each one in x - we do\n",
        "        #loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = []\n",
        "        for i in range(len(x)):\n",
        "            loss.append(loss_func(x[i,:,0],b[i:i+1]).item())\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44bWfVt3njrM",
        "colab_type": "text"
      },
      "source": [
        "#wrapper\n",
        "\n",
        "not put in "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1EsNMannU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7deYNPMonjoJ",
        "colab_type": "text"
      },
      "source": [
        "# code imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUOBUBX2nvPV",
        "colab_type": "code",
        "outputId": "422708cf-767f-4acc-f8b7-d56272b3418b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "test_model = LSTMencdec_onestep(structure, 1, kernel_size = 5).to(device)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmZFoI1Sk0On",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "aa7450c9-80a4-4770-97a6-d3abcac50bef"
      },
      "source": [
        "%cd /content/drive/My Drive/masters_project/data/\n",
        "\n",
        "f = h5py.File('test_fixed_25.hdf5','r')\n",
        "print(f['predictor'].shape)\n",
        "f.close()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "(2452, 10, 5, 16, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIFSO4IMoDo9",
        "colab_type": "text"
      },
      "source": [
        "## code loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-COzcpoJe0",
        "colab_type": "code",
        "outputId": "58a12b6b-4b61-4e96-ef3b-e43f8bd93ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\"\"\"now changed to fixed dataset\"\"\"\n",
        "\n",
        "avg = np.load(\"fixed_25_avg.npy\")\n",
        "std = np.load(\"fixed_25_std.npy\")\n",
        "# changed below\n",
        "apbln = [0,1,0,0,1] # think this is correct\n",
        "index_map = np.arange(0,52109,1)\n",
        "train, valid = initialise_dataset_HDF5_full('test_fixed_25.hdf5', valid_frac = 0.1, dataset_length = 2452,avg = avg, std = std, application_boolean=apbln)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n",
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiIZuUAQoNM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train, batch_size = 2000, shuffle = False) # implement moving MNIST data input\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMjpjcSxFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = \"valid_test94\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7B7f3ui0h_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmgFsBi6oE2l",
        "colab_type": "code",
        "outputId": "29f66021-ea61-46c7-d7c3-e83b5815a5f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model = nn.DataParallel(LSTMencdec_onestep(structure, 5, kernel_size = 3)).to(device) # added data parrallel\n",
        "\n",
        "test_model.load_state_dict(torch.load(name + \".pth\"))\n",
        "test_model.eval()\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12 0\n",
            "24 24\n",
            "0 12\n",
            "0 6\n",
            "0 5\n",
            "enc_shape, dec_shape, enc_copy_out, dec_copy_in:\n",
            "[12 24]\n",
            "[24 12  6  5]\n",
            "[False, True, False, False]\n",
            "[True, False, False, False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): LSTMencdec_onestep(\n",
              "    (encoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): LSTMmain(\n",
              "      (unit_list): ModuleList(\n",
              "        (0): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(24, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "        (3): LSTMunit(\n",
              "          (conv_dict): ModuleDict(\n",
              "            (Wxi): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxf): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxc): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Wxo): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "            (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p7lJB7uoIO7",
        "colab_type": "text"
      },
      "source": [
        "loading in averaging "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m-yzx0WopWO",
        "colab_type": "code",
        "outputId": "71b097ae-c781-4c46-dfb4-c186f421dba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_image_save(test_model, train_loader, name + \"comparison\", sample = 200, threshold = 0)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([2000, 1, 5, 16, 16])\n",
            "torch.Size([2000, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD1CAYAAABX2p5TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Tczy4wLv5lZZlz4zcwy48JvZpaZ/w8wkEUjeDrkkwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn0A5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAAbuCAYAAAAIX+1GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 2304x2304 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceaV2m7ZdHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def f1(model, train_loader, avg = 'macro'):\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "#     print(x[0][0][0])\n",
        "    print(x[222,0,0].shape)\n",
        "    print(b[222].shape)\n",
        "#     print(b)\n",
        "    print(b[222].view(-1, 256).numpy().shape)\n",
        "    truth = set(list(b[222].view(256).numpy()))\n",
        "    pred = set(list(x[222,0,0].view(256).numpy()))\n",
        "    print(truth - pred)\n",
        "    scores = []\n",
        "    for i in range(len(b)):\n",
        "        score = f1_score(b[i].view(256).numpy(), x[i,0,0].view(256).numpy(), average=avg)\n",
        "        scores.append(score)\n",
        "        truth = set(list(b[i].view(256).numpy()))\n",
        "        pred = set(list(x[i,0,0].view(256).numpy()))\n",
        "        if len(truth - pred) > 0:\n",
        "            print(i)\n",
        "#     score = f1_score(b[222].numpy(), b[222].numpy(), average=avg)\n",
        "    return scores\n",
        "#     print(score)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     print(x[sample][0][0])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSKhzR5rvPov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(model, train_loader):\n",
        "    \"\"\"Calculate TN, FN, TP, FP for multilabel classification\"\"\"\n",
        "    model.eval()\n",
        "    # calculate x and prediction \n",
        "    for a, b in train_loader:\n",
        "        # a in input, b is truth\n",
        "        break # train loader cannot be indexed\n",
        "        \n",
        "        \n",
        "    with torch.no_grad():\n",
        "        x = model(a.cuda())\n",
        "    \n",
        "    \n",
        "    x = x.cpu()\n",
        "    x[x>0] = 1\n",
        "    x[0>x] = 0\n",
        "    \n",
        "    # reshape\n",
        "    truth = b.view(-1,256).numpy()\n",
        "    pred = x[:,0,0].view(-1,256).numpy()\n",
        "    tn = 0\n",
        "    tp = 0\n",
        "    fn = 0 \n",
        "    fp = 0\n",
        "    \n",
        "    print(truth.shape)\n",
        "    print(pred.shape)\n",
        "    for i in range(len(b)):\n",
        "        for j in range(256):\n",
        "            # true positive\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 1):\n",
        "                tp += 1\n",
        "            # true negative\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 0):\n",
        "                tn += 1\n",
        "            \n",
        "            #false positive\n",
        "            if (truth[i][j] == 0) and (pred[i][j] == 1):\n",
        "                fp +=1\n",
        "            #false negative\n",
        "            if (truth[i][j] == 1) and (pred[i][j] == 0):\n",
        "                fn += 1\n",
        "    \n",
        "    print(\"tn:\" ,tn)\n",
        "    print(\"tp:\" , tp)\n",
        "    print(\"fn:\" , fn)\n",
        "    print(\"fp\" ,fp)\n",
        "    \n",
        "    prec = tp / (tp + fp)\n",
        "    rec = tp/ (tp + fn)\n",
        "    \n",
        "    f_1 = 2 * prec * rec / (prec + rec)\n",
        "    \n",
        "    print(\"prec:\" ,prec)\n",
        "    print(\"rec:\" , rec)\n",
        "    print(\"f1: \", f_1)\n",
        "    \n",
        "                \n",
        "            \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HLsqwEvahiy",
        "colab_type": "code",
        "outputId": "113a5d83-4a0a-4890-bc4d-960beb1f007b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "    scores = f1(test_model, train_loader, avg = 'binary')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "torch.Size([16, 16])\n",
            "torch.Size([16, 16])\n",
            "(1, 256)\n",
            "set()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFLTv_en0WN3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "02666af2-fd22-478b-e2fb-b5c1b8a5d130"
      },
      "source": [
        "metrics(test_model, train_loader)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINISHING ONE PASS\n",
            "(2000, 256)\n",
            "(2000, 256)\n",
            "tn: 481727\n",
            "tp: 10129\n",
            "fn: 4159\n",
            "fp 15985\n",
            "prec: 0.38787623496974805\n",
            "rec: 0.7089165733482643\n",
            "f1:  0.5014108212464731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdiCx0wFnYxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f6e3fa82-f26c-488d-eb85-13589fa47775"
      },
      "source": [
        "np.average(scores)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4890611957747059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfe7BIv-liRj",
        "colab_type": "code",
        "outputId": "667f7182-bf9d-41df-c0f1-3af5405fe1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(scores)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd4275fc7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4XOWZ9/HvPaPee7OqZcmWe5G7\nwSZAggktlGBYasKyIQkpy2ZT3+wu2d30ThIghGzY0FtiFpteTLUty03ukouKJav3Oprn/UMDcYxk\njaSRzpT7c11zMZo5mvkdpLl99Jzn3I8YY1BKKeVfbFYHUEop5Xla3JVSyg9pcVdKKT+kxV0ppfyQ\nFnellPJDWtyVUsoPaXFXSik/pMVdKaX8kBZ3pZTyQ0FWvXFSUpLJzc216u2VUson7dixo9EYkzza\ndpYV99zcXEpKSqx6e6WU8kkicsKd7XRYRiml/JAWd6WU8kNa3JVSyg9pcVdKKT80anEXkSwReV1E\n9ovIPhH58jDbrBORNhHZ5bp9d3LiKqWUcoc7s2UcwF3GmFIRiQZ2iMjLxpj9Z2z3ljHmEs9HVEop\nNVajHrkbY2qNMaWu+x3AAWDaZAdTSik1fmMacxeRXGARsHWYp1eKyG4R2Swic0b4/ttFpEREShoa\nGsYcVimllHvcLu4iEgU8DXzFGNN+xtOlQI4xZgHwa+Avw72GMeZ+Y0yxMaY4OXnUC6yUUkqNk1tX\nqIpIMEOF/WFjzDNnPn96sTfGbBKR34pIkjGm0XNRlbLOI1srR93m+uXZU5BEKfe4M1tGgD8AB4wx\nPxthmzTXdojIMtfrNnkyqFJKKfe5c+S+GrgR2Csiu1yPfQvIBjDG3AtcDdwhIg6gB9hgjDGTkFcp\npZQbRi3uxpi3ARllm3uAezwVSiml1MRY1hVSKW/hzni6Ur5G2w8opZQf0uKulFJ+SIu7Ukr5IS3u\nSinlh7S4K+UGYwx9A4NWx1DKbTpbRqkRGGPYcaKFkhMtNHT00TMwSFpMGAuy4liUHUdMWLDVEZUa\nkRZ3pYbR1efg2Z017K9tJyM2jPmZsUSFBXHkVCcv7qtjy+EGrl2aRWFqtNVRlRqWFnelztDRO8Dv\n3qygo9fBxfPSWZWfiG2ouwbnz0qlvr2Xx7ZX8ad3j3N+UQrnzUzB1X1DKa+hY+5KnWbQaXh0WxVd\nfQ5uP2c6a2YkfVjYP5ASE8bn1uazICuOVw7U88qBeovSKjUyLe5KnWZTWS3Hm7r41KJMshIiRtwu\nJMjGNUsyWZITz+uH6nn/qPbJU95Fi7tSLvtPtvFeRROr8xNZmBU36vYiwhULpzErLZrndp/kpX11\nU5BSKfdocVcKGBh08vzeWlJjQrlobrrb32e3CRuWZpMRF85dT+6mqrl7ElMq5T4t7koB75Y30tI9\nwCfnZWC3je3kaEiQjeuWDS3U8cVHd9LvcE5GRKXGRGfLqIDX0TvA64cbKEqLZkZK1LheIyEyhB9d\nNZ87Hi7lRy8c5DuXzB52O13RSU0VPXJXAe+VA6cYHDSsn+f+cMxw1s9L58YVOTzw9jG26glWZTEt\n7iqgnWztYceJFpbmJZAUFTrh1/vmxbPITojgX5/eQ0+/titQ1tHirgLaA28dA+CcgiSPvF5ESBA/\nvGo+J5q6+clLhzzymkqNhxZ3FbBauvp5dFslCzLjiI8I8djrrsxP5IYV2Tz4zjFKK1s89rpKjYWe\nUFV+7WwnMF89cIqegUHOKUz2+Pt+Y30Rr+yv5zvPlrHxi6sJsutxlJpaWtxVQOp3OHm3oolZadGk\nxYR55DXP/IfkvFkpPLqtkq8+vouV+Z4Z9lHKXXo4oQLS7urWoaP2As8ftX9gbkYMM1KiePnAKTp6\nBybtfZQajhZ3FXCMMbx/tIm0mDByE0fuHzNRIsJl8zMYGDS8UKatCdTU0uKuAk5VSw+1bb0sn54w\n6a16k6JDWTMjiZ1VrVS3aGsCNXW0uKuA8/7RJkKDbG41B/OEtYXJRIbY2bS3DmPMlLynUlrcVUDp\n7HOwt6aNRdnxhAbZp+Q9w4LtXDA7leNNXRyobZ+S91RKi7sKKKUnWhh0GpbnJUzp+xbnJJASHcrm\nsjocTm0spiafFncVMIwxlJxoIScxglQPTX90l90mXDQ3jaaufnac0Aub1OTT4q4CRlVLD42dfSzJ\njrfk/WemRpOdEMHrB+sZGNSjdzW5tLirgFF6ooVguzBvWqwl7y8iXDg7lfZeB9uONVuSQQUOLe4q\nIAwMOtlT08rcjFhCg6fmROpw8pOjyE+O5I3DDfQ5tGukmjxa3FVA2H+ynd4BJ4tzrBmSOd2Fs9Po\n6nPw/lE9eleTZ9TiLiJZIvK6iOwXkX0i8uVhthER+ZWIlIvIHhFZPDlxlRqf0soW4iKCyUuKtDoK\n2QkRFKRE8XZ5o469q0njzpG7A7jLGDMbWAF8QUTOXENsPVDgut0O/M6jKZWagLaeAcrrO1mcHY9t\nkq9Idde6mSl09Tko0ZkzapKMWtyNMbXGmFLX/Q7gADDtjM0uBx4yQ94H4kRkYmuWKeUhOytbMMCi\nKboi1R25iRFkJ0Tw1uEGBp161aryvDGNuYtILrAI2HrGU9OAqtO+ruaj/wAoNeWMMZRWtpCbGEmi\nB5bR8xQRYV1hMq09A+yubrU6jvJDbhd3EYkCnga+YowZ1zXUInK7iJSISElDQ8N4XkKpMals7qax\ns58lOd5z1P6Bma5e8m8ebsCpPWeUh7lV3EUkmKHC/rAx5plhNqkBsk77OtP12N8xxtxvjCk2xhQn\nJ09eH22lPlBa2UKI3cZci+a2n42IcE5BEg0dfZTXd1odR/kZd2bLCPAH4IAx5mcjbLYRuMk1a2YF\n0GaMqfVgTqXGrKd/kD3VbcydFjNlTcLGal5mLNGhQbxT3mh1FOVn3FlmbzVwI7BXRHa5HvsWkA1g\njLkX2ARcDJQD3cCtno+q1Ni8tL+OPoeTxRa1G3BHkM3G8umJvHLgFKfae6e8543yX6MWd2PM28BZ\n54+ZoSbVX/BUKKU84cmSauIjgsn1grntZ7MsL4E3DtXzXkUTVyzSeQjKM/QKVeWXalp7eKeikUVe\nNLd9JFGhQSzMimNnVQvdfQ6r4yg/ocVd+aVnS6sxBq8ekjndqvwkBgYNOyr1oiblGVrcld8xxvDU\njmpWTE8gITLE6jhuSYsNIychgm3HmnUpPuURWtyV39lxooXjTd1cvSRr9I29yLK8BJq6+nmvosnq\nKMoPaHFXfufJkmoiQuysn5tmdZQxmTstlvBgOw9vq7Q6ivIDWtyVX+nud/D83lo+OS+dyFB3Zvp6\nj2C7jcXZcbxYVkdDR5/VcZSP0+Ku/MqL++ro7HNw9ZJMq6OMy9K8BBxOw5M7qkbfWKmz0OKu/MpT\nO6rJTohgaW6C1VHGJSU6jOV5CTyxvUpPrKoJ0eKu/EZ1SzfvVjRx1eJMbDbvntt+NtcUZ3G8qVt7\nvasJ0eKu/MYzpTUYA1cu9u2rPNfPTSMixM5TJdVWR1E+TIu78gsfzG1flZ9IVkKE1XEmJDI0iE/O\nS+f5vbV09+sVq2p8tLgrv7D9eAuVzd0+eyL1TFcvyaSzz8ELZXVWR1E+yrfmiik1gidLqogMsXOR\nj81tH8nS3ASyEyJ4akc1Vy7+6D9Yj2x1by789cuzPR1N+Qg9clc+r6vPNbd9fjoRIf5xvGKzCVct\nzuTdiiaqW7qtjqN8kBZ35fNeKKuju3+Qa4p9q93AaD7lav/73G5d90aNnRZ35fOe2lFNTmIExTm+\n0QHSXdmJESzKjuOvuz6yYqVSo9LirnxaVXM37x1t4urFmYiX920fjysWTuNgXQcH68a1Jr0KYFrc\nlU97urQaEbjST2bJnOmT89Ox24S/7jppdRTlY7S4K5/ldBqeLh2a2z4tLtzqOJMiKSqUNTOS2Ljr\nJE6ntiNQ7tPirnzWtuPNVDX3cI2P9W0fq8sXZlDT2kOprtKkxkCLu/JZT+2oJio0iE/M8Y+57SP5\n+Jw0woJtbNytQzPKfVrclU/q6nOwaW8tl8xPJzzEbnWcSRUVGsS6whReKKvToRnlNi3uyidt2ltL\nd/+g37QbGM36eWnUd/Tp0IxymxZ35ZOe2lFNXlIkS/xsbvtIPjYrhRC7jU17tdeMco9/XKut/M7Z\neqc0d/Wz9VgzX/vETL+c2z6c6LBgzi1MYnNZLd/5ZJHVcZQP0CN35XNKK1sQ/nZ5fqBYPzed2rZe\ndle3Wh1F+QAt7sqnOI2htLKF/JQoMvx0bvtILihKJdgubNY2wMoNWtyVTznW2EVr9wCLswNjrP10\nsRHBrJ6RxKa9tbq+qhqVjrmrKeduL/LhlJ5oITTIxpyMGA8m8h0Xz03nX5/ew8nWXqbFB9ZfLmps\n9Mhd+Yy+gUHKTrYxPzOOYHtg/upeODsVu00oO9lmdRTl5QLzE6J80t6aNgYGDUuy46yOYpn4yBBW\n5SdSVtOmQzPqrLS4K59RWtlCUlSozy+APVHr56bT1NVPXXuv1VGUF9PirnxCU2cfx5u6WZIdFzBz\n20fy8TmpCFBWoz3e1chGLe4i8qCI1ItI2QjPrxORNhHZ5bp91/MxVaArrWxFgIUBOEvmTElRoeQl\nReq4uzord47c/we4aJRt3jLGLHTd7p54LKX+xmkMOytbmJESRWx4sNVxvMKcabE0dPRxSodm1AhG\nLe7GmC1A8xRkUWpYRxu6aO0ZYHGA9JFxx5z0oamg+2t1aEYNz1Nj7itFZLeIbBaROR56TaWAoROp\nYcE2ZqcH5tz24cSEB5MVH84BLe5qBJ4o7qVAjjFmAfBr4C8jbSgit4tIiYiUNDQ0eOCtlb/rHRhk\nX4DPbR9JUXoM1S09tPUMWB1FeaEJf1qMMe3GmE7X/U1AsIgkjbDt/caYYmNMcXJy8kTfWgWAv81t\n1yGZM33wl4wevavhTLi4i0iauOamicgy12s2TfR1lYKhdgPJUaFk6qX2H5EcHUpSVIgWdzWsUXvL\niMijwDogSUSqgX8DggGMMfcCVwN3iIgD6AE2GL10TnlAY2cfJ5q7+cSctICf2z4cEaEoPYZ3yhvp\n6R/0++UG1diMWtyNMdeN8vw9wD0eS6SUywd92xdlDd9uYCINyPzF7PQY3jrSyOFTHSwY4f+TCkx6\nhkp5paG57a0UpEYRo3PbR5SVEEFUaJBOiVQfocVdeaWKhk7aegKzb/tY2ESYlRbN4VMdOAadVsdR\nXkSLu/JKpSeG5rYX6dz2Uc3OiKHP4eRoY5fVUZQX0eKuvM7Q3PZ2FujcdrfkJ0cRYrfp0Iz6O/rJ\nUV5nb3UbDqdhibYbcEuw3UZBahQHattx6kQ15aLFXXmdHZUtpESHMi3AFsCeiNnpMXT0Oqhp6bE6\nivISWtyVV2no6KOyuZvF2fE6t30MZqZFYxNtJKb+Rou78iqllS3YBBYG8FJ64xEREkRuUqQWd/Uh\nLe7Ka3zQt70gJZqYMJ3bPlaz02No6OijsaPP6ijKC2hxV16jvL6T9l6H9m0fpyLt8a5Oo8VdeY3S\nyhbCg+0UpUVbHcUnxUeEkBEbpsVdAVrclZfo6R9k/8l2FmTFEqRz28etKCOGquZuOnq1x3ug00+R\n8gp7alqH5rZnJ1gdxafNTo/BAAfrOqyOoiymxV15hdITLaTGhJIRF2Z1FJ+WFhNGfEQw+0/q0Eyg\n0+KuLHeqvZeqlh6d2+4BIsLs9BgqGjrpcwxaHUdZSIu7slzJ8WbsIizSDpAeUZQeg8NpOHKq0+oo\nykJa3JWlHINOdla1UpQeTVToqGvHKDfkJEYSHmzXWTMBTou7stSBug66+wcpztUTqZ5itwlF6dEc\nqutgQHu8Bywt7spSJcebiQ0PZkZKlNVR/EpRegw9A4NsP9ZsdRRlES3uyjIt3f2U13eyJCcem55I\n9aiClGiCbMJL+09ZHUVZRIu7ssyOEy0A2rd9EoQE2ZiREsXL+09htMd7QNLirizhNIYdJ1qYkRJF\nfESI1XH80uz0GGpae9inc94DkhZ3ZYny+qEFsPVE6uSZlR6DTdChmQClxV1ZouR4MxEh2iRsMkWF\nBrEkJ56X9tVZHUVZQCcWqynX2efgQG0HK6YnaJOwSZYcHcb247X86tUjJEWFDrvN9cuzpziVmgr6\nyVJTbmdlC4PG6JDMFJibMdTjvaymzeIkaqppcVdTyuk0bD/eTHZCBKkx2iRsssVFhJAVH07ZSS3u\ngUaLu5pSb5c30tjZz/I8PWqfKnOnxXKytZfmrn6ro6gppMVdTamH3jtBZIidedNirY4SMOZmDP2/\n1qGZwKInVNWUqW7p5rWDpzinIFlPpJ7FI1srPfp68ZEhZMaHs7emjXMLkz362sp76SdMTZmHXUVL\nh2Sm3tyMWGpae3RoJoBocVdTondgkMe3V3FBUSpxekXqlJvrGgbbq0MzAUOLu5oSf9lZQ3NXP7es\nyrU6SkBKiByaNbOnutXqKGqKjFrcReRBEakXkbIRnhcR+ZWIlIvIHhFZ7PmYypc5nYbfv3WUORkx\nrMxPtDpOwJqfGUdtWy/1Hb1WR1FTwJ0j9/8BLjrL8+uBAtftduB3E4+l/Mnrh+qpaOji9nOn6xqp\nFpqXGYsAe6p1aCYQjFrcjTFbgLN1/L8ceMgMeR+IE5F0TwVUvu/+LUfJiA3j4nn6a2GlmLBg8pIj\n2V3Vqm2AA4AnxtynAVWnfV3teuwjROR2ESkRkZKGhgYPvLXydrurWtl6rJnPrMkjWKc/Wm5BZhxN\nXf2cbNWhGX83pZ82Y8z9xphiY0xxcrLOtw0E97xeTnRYENcuzbI6imJoSqRdhN16YtXveaK41wCn\nf3IzXY+pAFdW08bL+09x25rpRIcFWx1HAeEhdgrTotld3YpTh2b8mieK+0bgJtesmRVAmzGm1gOv\nq3zcL145TExYELeuybU6ijrNoqw4OnodVNR3Wh1FTaJR2w+IyKPAOiBJRKqBfwOCAYwx9wKbgIuB\ncqAbuHWywirfsbuqlVcO1HPXhYXE6FG7V5mVFk14sJ3SyhYKUnWxFH81anE3xlw3yvMG+ILHEim/\n8PNXDhMXEcwtq3OtjqLOEGS3MT8zltLKFnoHBq2OoyaJTl9QHvfWkQbeONTA59bm61i7l1qUHc/A\noGGf9nn3W1rclUc5Bp3c/dx+chIjuFWP2r1WVnw4iZEhlFbqrBl/pcVdedTDWys5Ut/Jty8uIjTI\nbnUcNQIRYVF2PMcau6hs6rY6jpoEWtyVx7R09fOzlw+zekYiF85OtTqOGsXi7DgEeHJH1ajbKt+j\nxV15zH88t4+uPgffvWSO9pDxAXERIRSmRvNkSTWOQafVcZSHaXFXHvFCWR1/2XWSOz9WwMw0nV7n\nK4pz46lr72XLEW0H4m+0uKsJa+7q5zt/2cucjBg+f16+1XHUGMxKiyEpKoTHtunQjL/RNVTHwJ21\nLa9fnj0FSTzL3TU7h9s3YwzffGYPbT0D/Pm25doczMfYbcJVSzJ54K1j1Lf3khITZnUk5SH6SVQT\n8rs3K3hx3ym+ftEsZqXFWB1HjcO1xVkMOg1P7qi2OoryIC3uaty2HG7gJy8e4tIFGXx2TZ7VcdQ4\nTU+OYlV+Io9srWTQqc3E/IUWdzUu5fUdfOmxnRSmRvPDq+bp7Bgfd+OKHGpae3j9YL3VUZSHaHFX\nY1bd0s0ND2wj2G7j/huLiQjRUze+7oLZqaTGhPK/75+wOoryEC3uakwaOvq48Q/b6O538NBnlpGd\nGGF1JOUBwXYb1y3L5s3DDZxo6rI6jvIAPeRSbvvt6+U8+M4x2noG+MzqPHZWtrLzjN4kvjhbSA25\nblk2v36tnIe3VvKti4usjqMmSI/clVsaOvq4b8tROvscfGZ1HjmJkVZHUh6WGhPGJ+ak8vj2Krr7\nHVbHUROkxV2N6mhjJ/dtqcDhNNy2ZroWdj926+o82noGeKZUV8r0dVrc1VltP9bMg28fIzIkiM+d\nO52MuHCrI6lJVJwTz7xpsfzxnWM4dVqkT9PiroY1MOjk2Z3VPLurhhkpUdyxLp/EqFCrY6lJJiJ8\nZk0uFQ1d2m/Gx+kJVfURjR19PLq9ktq2XtYWJnNBUSp2m85j91dntp9wOJ1Ehwbx35sOcLK1F9AT\n5b5Ii7sP83SvG2MM2443s2lvLcF2GzevzNUOjwEoyGZj+fREXjlwilPtvaRqvxmfpMMyCoDW7n7+\n9N5x/rrrJLmJkdq6N8Aty0sgyCa8Xd5odRQ1TnrkHuAGnYZ3Kxp5af8pjDFcOj+dFdMTtZ1AgIsK\nDWJJTjwlx1u4sEhX1fJFeuQeoIwxvLSvjvW/3ML/7aklNzGCr5xfyMr8JC3sCoA1M5JwGsO7FU1W\nR1HjoEfuAej9o0388IWD7KxsZXpSJNcty2ZuRowWdfV3EqNCmTstlq3HmujoHSA6LNjqSGoM9Mg9\ngJTVtHHTg9vYcP/71Lb28oMr5/HSV89l3rRYLexqWOcUJNHncLq9oIvyHnrkHgCONnTy05cP8/ye\nWuIigvn2xUXcuDKHsGC71dGUl8uMjyA/OZIH3j7Gzaty9XfGh2hx92NtPQN885k9PFFSTWiQjS99\nbAa3nTudGP3zWo3BeTNTeODtYzxRUsVNK3OtjqPcpMXdDzkGnbxV3sgbh4YWXrhxRQ5fOG8GydF6\nhakau7ykSIpz4rn3jQo2LM0mJEhHc32BFnc/c6Kpi6d2VNPU1c/cjBh+d8MSshK057oaPxHhzvML\nuPnBbTxTWs2GZXq1qi/Q4u6lxnoCyxjD2+WNvLivjtjwYG5ZlUtharQWduUR5xYkMT8zlt++UcFV\nSzIJtuvRu7fTn5AfcDidPLa9is1ldcxKi+HOjxVQmKpXlyrPERG+9LECKpu7eaa02uo4yg1a3H2c\nw+nksW1V7K1p4xOzU/mH5dk6o0FNivOLUliQFcevXi2nzzFodRw1CreKu4hcJCKHRKRcRL4xzPO3\niEiDiOxy3W7zfFR1pkGn4dFtVeyvbeeS+emsnZmi89XVpBER7rqwkJrWHp7YXmV1HDWKUYu7iNiB\n3wDrgdnAdSIye5hNHzfGLHTdHvBwTjWMTWW1HKht59L56azKT7I6jgoA5xQksTQ3nl+/Vk7vgB69\nezN3TqguA8qNMUcBROQx4HJg/2QGU2e3s7KF9yqaWJ2fyEovKux6JaN/ExHu+vhMNtz/Pg+9d5zb\nz823OpIagTvDMtOA0/8Gq3Y9dqarRGSPiDwlIlkeSaeGVdvWw1921ZCXFMlFc9OtjqMCzIrpiawt\nTOae18pp7e63Oo4agadOqD4H5Bpj5gMvA38abiMRuV1ESkSkpKFBl/AaD8egk8e3VxEebGfD0ixd\nIUlZ4hvrZ9HR5+C3b1RYHUWNwJ1hmRrg9CPxTNdjHzLGnN4T9AHgR8O9kDHmfuB+gOLiYl19dxze\nONxAfUcfN6/M0S59asoMN9y2KCueP7x9jNiwYOIjQ3QpPi/jzpH7dqBARPJEJATYAGw8fQMROX1s\n4DLggOciqg/UtvXwxqF6FmbFMTMtxuo4KsBdODsVAV4+cMrqKGoYoxZ3Y4wD+CLwIkNF+wljzD4R\nuVtELnNt9iUR2Sciu4EvAbdMVuBA5TSGZ0prCA+2c8k8HWdX1osND2ZNQRK7qlo50dRldRx1BrfG\n3I0xm4wxhcaYfGPMf7ke+64xZqPr/jeNMXOMMQuMMecZYw5OZuhAtLOylZrWHj45P4OIUO0aobzD\n2sJkYsKCeG7PSQadOtLqTfQKVR8wMOjk5f11ZMaHsyAz1uo4Sn0oNMjO+nnpnGzt5XG9sMmraHH3\nAe+UN9Le62D93HS9AlV5nfnTYslNjODHLx7UqZFeRIu7l+vsc/Dm4QaK0qLJS4q0Oo5SHyEiXLog\ng/ZeB9/fpCOy3kKLu5d77WA9A4NOPjE3zeooSo0oPTac287J4/GSKt6raBr9G9Sk0zNzXqyxo49t\nx5oozk0gJTpsXK+h7QDUVPnK+YVs3lvHt57dy+Yvn6PdSS2mR+5e7MX9dQTZbZw/K8XqKEqNKjzE\nzn9/ah7HGrv45atHrI4T8LS4e6kTTV3sO9nOuQVJeiWq8hlrCpK4tjiL+96sYMeJZqvjBDQt7l7I\nGMPmsjqiw4JYMyPZ6jhKjcn/u3Q20+LD+erju+nsc1gdJ2BpcfdCL5TVUdnczYVFqbrSvPI5UaFB\n/PzTC6lu6eZ7z2lncKto5fAy/Q4nP3zhICnRoSzOibc6jlLjUpybwOfW5vN4SRV/3VUz+jcoj9PZ\nMqOo7+jlF68c4e0jjXT1OQi221icHceK6YkETcIK8I9sPcHxpm5uXpmDTS9YUj7sqxcWsv14M994\nei+z02Mo0EXbp5QW9xEYY/j9W0f55StH6HM4uaAolZNtPbT1DLCprI53jzZx6fwMitI9152xvXeA\nX71Wzqr8RAr1g6B8zHDTbs+flcr+2g6u//1WPr8un1vX5FmQLDDpsMwIfvbyYf5700FW5ify8j+v\n5d4bl7BhaTb/dG4+n1mdR1iQnT+/f4Ltxzw3I+BnLx2mpbufb11cpG0GlF+ICQ9mw9IsGjv7eKKk\nSpuLTSEt7sO4780Kfv1aORuWZvH7m4o/ctn/jJQoPrc2n4LUKJ7dVcObhye+qlRZTRsPvXecG5bn\nMHeaNgdT/iM/OYpL5qdzoK6D/3xeT7BOFS3uZ9i4+yTf33yQS+an81+fmjfiEXRIkI0bVuQwPzOW\nF/fV8U5547jf0+k0/L+/lpEQGcK/fHzmuF9HKW+1Mj+J1fmJ/PGd4/zxnWNWxwkIOuZ+mqrmbr79\nzF6W5MTz82sXjro+aZDNxqeLsxh0Gp7fW0tkqH1cS409tr2KnZWt/PSaBcRG6AVLyj+tn5dOZGgQ\n//HcfiJDgvj00qxht3OnZYYu6Tc6PXJ3GXQa/vmJXRjgF9cuJNjNmTA2ET5dnEVeUiRP7ajmjUP1\nY3rfg3Xt3P1/+1iVn8iVi6eNI7lSvsEmwq+uW8Q5BUl8/Zk9PFNabXUkv6bF3eV3b5Sz/XgL37ti\nDlkJEWP63mC7jRtX5JAWE8Y//e8Ot4doOnoHuOPPpUSHBfOLDQv1JKrye2HBdn5/UzGr8hP5lyd3\n8/DWE1ZH8lta3IFDdR388tXz1yiCAAAKNklEQVQjXDI/nSsWju/oOSzYzq2r88hLiuSzf9o+attT\nx6CTrz25h8rmbu65btG4uz4q5WvCgu08cNNS1hYm8+1ny/jJi4cwRmfReFrAF/dBp+Ffn95DdFgw\n/3HZnAkdPUeGBvHn25aTFR/BzQ9u48G3jw37S9vWPcAtf9zOC/vq+Ob6WSyfnjiRXVDK54SHDB3B\nb1iaxT2vl3Pnozu1D42HBfwJ1T++c4zdVa38csNCEqNCJ/x6SVGhPP5PK/nak7u5+//289aRBm5d\nnUdxbjx9A062HGngl68coaqlmx9dPZ9PFw9/Ukkpfxdkt/H9K+eRnRjBT148xP7adu69YYnVsfxG\nQBf3yqZufvrSYT42K4XLFmR47HUTIkN44OZiHnrvBN/ffIDXDzUQbBcGnQangbSYMB75xxUszU3w\n2Hsq5YtEhM+vm8HCrDi+9OguLv3125xflMqq/ERtvzFBAVvcjTF889k92G3Cf14x1+MnM0WEm1fl\nck1xJtuPt/BeRROhQTbWzUxmfmbcqNMslQokq/KT2PTlNXzrmb1s2ltLWU0bVyyaRlqMnosar4At\n7k+WVPNOeRP/ecVcMuLCJ+19IkKCWFuYzNpC7cuu1Ghz2M+bmUJCZAjP7a7lnteOsCwvkQuKUogI\nCdhSNW4B+X/sVHsv33t+P8vyErh+mV4MoZS3EBEWZsVTmBLNKwfr2Xasid1VrVwwO5VluQn6F+8Y\nBNxsGafT8PWn99DvcPKDK+dh018WpbxORGgQly3I4IsfKyAjLozndp/k168d4WBdu06bdFPAHbn/\n/q2jvHGoge9dPofpyVFWx1FKnUVaTBifWZ3HgdoONpfV8tB7J8hNjKQoI4bF2bqYzdkE1JF7aWUL\nP37xEOvnpnHDihyr4yil3CAizM6I4SsXFHLZggwaO/u48rfvcvtDJZTXd1gdz2sFzJH7qfZe7nxk\nJ2mxYfzgqvl6qb9SPsZuE1ZMT2RRdhydvQ7u23KUj/98C9csyeIrFxaQHjt5EyN8UUAcubd1D3DT\nH7bR2t3PvTcsITZcOy8q5atCg+zceX4Bb35tHbesyuPZnTWs+/EbfH/zAdq6B6yO5zX8vrj39A9y\n20PbOdrYyX03FutCGEr5icSoUL576WxevWstn5yXzv1bjnLOj17jN6+X096rRd6vi3ttWw+fvu89\nSk608PNrF7KmIMnqSEopD8tKiOBn1y5k05fOYUlOPD9+8RCrv/8aP9h8kJrWHqvjWcZvx9y3H2/m\n8w+X0t3n4Pc3FnPB7FSrI33IncUIlFJjU5Qewx9vXUZZTRu/e7OC+7ZUcP+WCtbNTOHqJZmcNzOF\n8BC71TGnjFvFXUQuAn4J2IEHjDE/OOP5UOAhYAnQBFxrjDnu2ajuqWvr5UcvHOSZnTVkJ0Tw8G3L\nKUyNtiKKUsoCc6fF8pvrF1Pd0s3j26t4bHsVrx2sJzzYzrqZyZxTkMzqGYlkJ0T49cSKUYu7iNiB\n3wAXAtXAdhHZaIw5faXbzwItxpgZIrIB+CFw7WQEHo5j0Mm7FU08XVrNC2V1GODz6/L5/HkziAr1\n2z9OlFJnkRkfwV0fn8mXzy9g27Fmnt9by6sH6tlcVgdAbHgws9KiKUqPYVZaNIVp0WTGhZMYFeoX\nV8K6U/mWAeXGmKMAIvIYcDlwenG/HPh31/2ngHtERMwkXErW1NnHnpo2qlt6qGruZk91K7ur2ugZ\nGCQmLIhrijO5/Zx8shPHtpqSUsq/nDn8OScjltnpMTR29nO0sZPa1l4GBp08WVJFV//gh9sF2YTU\nmDDSYsNIiQ4lNjyYmPDgof+GBX14PzosmNAgGyFBNkLsrv+e9vWZnMbQ73DS53ASFmSf9PWS3Snu\n04Cq076uBpaPtI0xxiEibUAi4N56c2PwbkUTdz66E4AQu41Z6dFcuzSLFdMTWDczhbDgwBlTU0qN\njYiQHB1KcvTQ2g3XL8/G6TRUt/Rw+FQHte291LX1UNvWS11bL0fqO2nvGaCtZ4A+h9NjOe5Yl8/X\nL5rlsdcbzpSOWYjI7cDtri87ReTQRF/zCPDcRF/EfUmM8g/WP0xRkAkYdR98gO6Dd7BsHzz1OfsH\ni/bhGz+Eb4z/2926vN6d4l4DnL5cUKbrseG2qRaRICCWoROrf8cYcz9wvzvBvJGIlBhjiq3OMRG6\nD95B98E7+MM+jMSdee7bgQIRyROREGADsPGMbTYCN7vuXw28Nhnj7Uoppdwz6pG7awz9i8CLDE2F\nfNAYs09E7gZKjDEbgT8A/ysi5UAzQ/8AKKWUsohbY+7GmE3ApjMe++5p93uBazwbzSv57JDSaXQf\nvIPug3fwh30YlujoiVJK+R+/7i2jlFKBSov7METkIhE5JCLlIvKRGUsiEioij7ue3yoiuVOf8uzc\n2Id/FpH9IrJHRF4VEa9bvWS0fThtu6tExIiI1816cGcfROTTrp/FPhF5ZKozjsaN36VsEXldRHa6\nfp8utiLnSETkQRGpF5GyEZ4XEfmVa//2iMjiqc44KYwxejvtxtBJ4wpgOhAC7AZmn7HN54F7Xfc3\nAI9bnXsc+3AeEOG6f4cv7oNru2hgC/A+UGx17nH8HAqAnUC86+sUq3OPYx/uB+5w3Z8NHLc69xn5\nzgUWA2UjPH8xsBkQYAWw1erMnrjpkftHfdhuwRjTD3zQbuF0lwN/ct1/CjhfvKsD0aj7YIx53RjT\n7fryfYauX/Am7vwcAL7HUC+j3qkM5yZ39uEfgd8YY1oAjDH1U5xxNO7sgwFiXPdjgZNTmG9Uxpgt\nDM3iG8nlwENmyPtAnIikT026yaPF/aOGa7cwbaRtjDEO4IN2C97CnX043WcZOnLxJqPug+vP5yxj\nzPNTGWwM3Pk5FAKFIvKOiLzv6sDqTdzZh38HbhCRaoZm1d05NdE8ZqyfF5+gLRMDnIjcABQDa63O\nMhYiYgN+BtxicZSJCmJoaGYdQ389bRGRecaYVktTjc11wP8YY34qIisZuuZlrjHGc81Y1JjpkftH\njaXdAmdrt2Ahd/YBEbkA+DZwmTGmb4qyuWu0fYgG5gJviMhxhsZKN3rZSVV3fg7VwEZjzIAx5hhw\nmKFi7y3c2YfPAk8AGGPeA8IY6tniK9z6vPgaLe4f5Q/tFkbdBxFZBNzHUGH3tnFeGGUfjDFtxpgk\nY0yuMSaXofMGlxljSqyJOyx3fpf+wtBROyKSxNAwzdGpDDkKd/ahEjgfQESKGCruDVOacmI2Aje5\nZs2sANqMMbVWh5owq8/oeuONobPnhxmaJfBt12N3M1Q8YOiX90mgHNgGTLc68zj24RXgFLDLddto\ndeax7sMZ276Bl82WcfPnIAwNL+0H9gIbrM48jn2YDbzD0EyaXcDHrc58Rv5HgVpggKG/lD4LfA74\n3Gk/g9+49m+vN/4ejeemV6gqpZQf0mEZpZTyQ1rclVLKD2lxV0opP6TFXSml/JAWd6WU8kNa3JVS\nyg9pcVdKKT+kxV0ppfzQ/wcQWRTnzgQLoAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oszwtyDuVvX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwUwgS2W3GC",
        "colab_type": "text"
      },
      "source": [
        "# testing flipping the input direction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0nD-LmrX2UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for d, b in train_loader:\n",
        "    break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnMHy4qTW7a9",
        "colab_type": "code",
        "outputId": "d7639319-e259-4ae7-81ea-e90715d63e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "# set up some axes\n",
        "a = d\n",
        "a = a.numpy()\n",
        "c = np.flip(a, 1)\n",
        "\n",
        "# c = torch.flip(a, (0,1))\n",
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(c[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAOICAYAAAAU5r/0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1152x1152 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j08NdPNhZoJG",
        "colab_type": "code",
        "outputId": "cfa58827-22c2-42be-ae21-848bfbba0234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c.shape\n",
        "c = np.array(c)\n",
        "torch.tensor(c)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.2740, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026,  0.0377,  0.7753],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.4872,  1.4265],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.1818, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0205],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0026, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0026, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523, -1.1484],\n",
              "           [-0.4455, -0.4455, -0.4455,  ..., -0.2581,  0.0523,  0.0523],\n",
              "           [-0.8384, -0.8384, -0.8384,  ..., -0.3979,  0.2495,  0.2495],\n",
              "           ...,\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151],\n",
              "           [-0.8101, -0.8101, -0.8101,  ..., -0.4301, -0.0151, -0.0151]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907, -1.1484],\n",
              "           [-0.4565, -0.4565, -0.4565,  ...,  0.0250,  0.2907,  0.2907],\n",
              "           [-0.7675, -0.7675, -0.7675,  ..., -0.1003,  0.6341,  0.6341],\n",
              "           ...,\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397],\n",
              "           [-0.6733, -0.6733, -0.6733,  ..., -0.4722, -0.1397, -0.1397]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ...,  0.4469,  0.0032, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           [ 1.5191,  1.5191,  1.5191,  ...,  0.1586, -0.5665, -0.5665],\n",
              "           ...,\n",
              "           [ 0.2575,  0.2575,  0.2575,  ...,  0.4042,  1.2045,  1.2045],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414],\n",
              "           [-0.1673, -0.1673, -0.1673,  ...,  0.1355,  0.9414,  0.9414]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           [ 1.4846,  1.4846,  1.4846,  ...,  0.4336, -0.4000, -0.4000],\n",
              "           ...,\n",
              "           [ 0.2753,  0.2753,  0.2753,  ...,  0.7617,  1.3650,  1.3650],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070],\n",
              "           [-0.1019, -0.1019, -0.1019,  ...,  0.2275,  1.3070,  1.3070]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [ 0.2275,  1.3070,  1.3070,  ..., -0.2959, -0.2959, -0.2959],\n",
              "           [-0.3838,  0.5363,  0.5363,  ..., -0.4330, -0.4330, -0.4330],\n",
              "           ...,\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621],\n",
              "           [-1.1266, -1.0642, -1.0642,  ..., -0.9621, -0.9621, -0.9621]]]],\n",
              "\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ...,  0.0089, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.4585, -0.4585, -0.4585,  ...,  0.3195,  0.3195,  0.1252],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           ...,\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019],\n",
              "           [-0.5172, -0.5172, -0.5172,  ..., -0.2093, -0.2093,  0.2019]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.3803, -0.3803, -0.3803,  ...,  0.4900,  0.4900,  0.2559],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           ...,\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756],\n",
              "           [-0.6307, -0.6307, -0.6307,  ..., -0.6965, -0.6965, -0.3756]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0026]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.2581, -0.2581, -0.2581,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.3979, -0.3979, -0.3979,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423],\n",
              "           [-0.4301, -0.4301, -0.4301,  ...,  0.3666,  0.3666,  0.3423]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.0550, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0026,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084,  0.0147],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0250,  0.0250,  0.0250,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           [-0.1003, -0.1003, -0.1003,  ..., -1.1484, -1.1484, -1.1484],\n",
              "           ...,\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959],\n",
              "           [-0.4722, -0.4722, -0.4722,  ...,  0.2824,  0.2824,  0.1959]]]],\n",
              "\n",
              "\n",
              "\n",
              "        [[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         ...,\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  1.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084,  0.6831, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0861, -0.7097, -0.7097,  ..., -0.1776, -0.1776, -0.1776],\n",
              "           [-1.0858, -0.2305, -0.2305,  ...,  0.2892,  0.2892,  0.2892],\n",
              "           ...,\n",
              "           [-1.1484, -0.2069, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538],\n",
              "           [-1.1484, -1.1484, -0.2069,  ...,  0.9538,  0.9538,  0.9538]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]],\n",
              "\n",
              "\n",
              "         [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           ...,\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084],\n",
              "           [-0.0084, -0.0084, -0.0084,  ..., -0.0084, -0.0084, -0.0084]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           ...,\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
              "\n",
              "          [[-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0985, -0.5059, -0.5059,  ...,  0.2545,  0.2545,  0.2545],\n",
              "           [-1.0935, -0.3595, -0.3595,  ...,  0.7540,  0.7540,  0.7540],\n",
              "           ...,\n",
              "           [-1.1484, -0.1574, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681],\n",
              "           [-1.1484, -1.1484, -0.1574,  ...,  1.1681,  1.1681,  1.1681]]]]],\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY3GRCuQYkQZ",
        "colab_type": "code",
        "outputId": "8c40011f-a37f-4675-baa0-eb3e5b8e71c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "e.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-f73a2bc07fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBo6C1MAZdGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = torch.tensor(c)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSGYIr8YYKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(10,2, figsize = (16,16))\n",
        "#     print(x.shape)\n",
        "#     print(b.shape)\n",
        "#     axes[0].imshow(x[sample][0][0])\n",
        "#     axes[1].imshow(b[sample])\n",
        "    \n",
        "\n",
        "for i in range(10):\n",
        "    axes[i,0].imshow(a[0][i][0])\n",
        "    axes[i,1].imshow(e.numpy()[0][i][0])\n",
        "    \n",
        "plt.show()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwsDysmeW87k",
        "colab_type": "text"
      },
      "source": [
        "# after flipping\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXQF2DnA6yB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d) // 3\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "losses = batch_loss_histogram(test_model, train_loader, loss_func = c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ic2GUlrX799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sirv8aO61kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pbxmoet7IUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cPmIoZ3JNl",
        "colab_type": "text"
      },
      "source": [
        "## making histograms to check kernel size effect "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiEBDQBR3VKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import seaborn as sns\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d).to(device)\n",
        "c = nn.BCEWithLogitsLoss(pos_weight=weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHCoJCKbhiPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloIqpwpW6Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a,b in train_loader:\n",
        "    break\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbcXFd1pU6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = a.to(device)\n",
        "b = b.to(device)\n",
        "c(a[0][0][0],b[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoIbwcFpW99P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# b[0]\n",
        "# sdaddasdasadad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfVeZgua3NNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = batch_loss_histogram(test_model, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYvtfNvMrMQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sns.distplot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEulvwY35_DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change in all - train_index  = list\n",
        "\n",
        "\n",
        "\n",
        "# truth = train[:][1]\n",
        "truth.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAHdhEAmCAYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.application_boolean\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C96Eneh2CFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans = train[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX4AYML9CG7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX25bMZtCS3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans[1]\n",
        "plt.imshow(ans[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqHhfnU1A8Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = truth.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-lOg4RmBHkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRR7kZg8BJc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[t>0] = 1\n",
        "t[t<0] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGdBvH9PE2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ndgb4yy_4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incident_map = np.sum(t, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6BoXcdbFD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = sns.heatmap(incident_map).set_title(\"Total Number of UCDP Events in Training Set of 46898\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UlhsFIUIdbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyplot_fig = heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"heatmap_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgD8oDV2LRJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3LIRp9NI0DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors = (46898  - incident_map)// incident_map\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8uk9UI6hGaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84UQbluAaTTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multiplicative_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDO2uxF3LUSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"weights_bce\", multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6MygGdJzni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_heatmap = sns.heatmap(multiplicative_factors)\n",
        "pyplot_fig = second_heatmap.get_figure()\n",
        "pyplot_fig.savefig(\"multiplicative_factors_min_event_25_occurances.pdf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF2Coy8QM7DA",
        "colab_type": "text"
      },
      "source": [
        "# applying weight function to lossy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htD4jvXUN-gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = torch.tensor(multiplicative_factors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjIF2EHZODYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func  = nn.BCEWithLogitsLoss(pos_weight= weights)\n",
        "loss_default = nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg-X4tXNanfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = b[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_KNeqBapXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d[1 > d] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHhnCUQDawzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMsFFKJ7WaKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(a[0][-1][0],b[0]))\n",
        "print(loss_default(a[0][-1][0], b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEo_Gr8PXhS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = torch.ones_like(a[0][-1][0])\n",
        "c *= -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8aP4eTX7cD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizNoE4vXoGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(c,b[0]))\n",
        "print(loss_default(c, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beyngsiPa5Vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss_func(d,b[0]))\n",
        "print(loss_default(d, b[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9AGiQKOePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l1 = batch_loss_histogram(test_model, train_loader, loss_func)\n",
        "l2 = batch_loss_histogram(test_model, train_loader, loss_default)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-l3gnzjPGyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(l1)\n",
        "plt.figure()\n",
        "sns.distplot(l2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}