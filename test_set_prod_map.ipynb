{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_set_prod_map.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Garethlomax/acse-9-independent-research-project-Garethlomax/blob/map_ensembling/test_set_prod_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaxapAEijZMn",
        "colab_type": "text"
      },
      "source": [
        "# google imports\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fkFy9e4jU-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RasmStkKk0vX",
        "colab_type": "text"
      },
      "source": [
        "## normal imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOxvkMnok0Vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#import seaborn as sns\n",
        "import h5py\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4PEiJl-lHlb",
        "colab_type": "text"
      },
      "source": [
        "# Functions from model_variant_3.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_dMQ1N_k5Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def date_to_int_list(date):\n",
        "    # date is in format yyyy-mm-dd\n",
        "\n",
        "    y = int(date[0:4])\n",
        "    m = int(date[5:7])\n",
        "    d = int(date[8:10])\n",
        "\n",
        "#    print(\"date is:\")\n",
        "#    print(y, \" \",m, \" \", d)\n",
        "    return [y,m,d]\n",
        "\n",
        "def monotonic_date(date, baseline = [1989, 1, 1]):\n",
        "\n",
        "    date = date_to_int_list(date)\n",
        "#    print(type(date[0]))\n",
        "#    print(type(baseline[0]))\n",
        "    # turns date since baseline start date into a monotonic function based on\n",
        "    # year and months in line with pgm unit of analysis\n",
        "    return date[1] - baseline[1] + ((date[0] - baseline[0]) * 12)\n",
        "\n",
        "\n",
        "\n",
        "def construct_layer(dataframe, key, prio_key = \"gid\", debug = False):\n",
        "    # returns 360 720 grid layer for a given parameter\n",
        "    # should be given for one parameter per year.\n",
        "    array = np.zeros(360*720)\n",
        "    prio_grid = dataframe[prio_key]\n",
        "    for i in range(len(prio_grid)):\n",
        "        j = prio_grid.iloc[i] - 1\n",
        "        \"\"\"change the below to only be in the case it isnt nan\"\"\" # will this cause problems?\n",
        "        array[j] += dataframe[key].iloc[i]\n",
        "    array.resize(360,720)\n",
        "\n",
        "    return array\n",
        "\n",
        "def construct_combined_sequence(dataframe_prio, dataframe_ucdp, key_list_prio, key_list_ucdp, string_start, start = [1989, 1,1], stop = [2014,1,1]):\n",
        "    \"\"\"commented out the stop and start\"\"\"\n",
        "    #stop  = '2014-01-01'\n",
        "    # need to adapt ged and other year / month vs ged database for this.\n",
        "    # bool prio to add multiples of 12 to each year usin prio grid.\n",
        "    num_month = monotonic_date(stop, start)\n",
        "    print(num_month)\n",
        "    comb_channel_len = len(key_list_prio) + len(key_list_ucdp)\n",
        "    print(comb_channel_len)\n",
        "    array = np.zeros((num_month, comb_channel_len, 360,720))\n",
        "    \"\"\"commented out here aswell\"\"\"\n",
        "    stop = date_to_int_list(stop)\n",
        "\n",
        "    month = 0\n",
        "    extract_month = monotonic_date(string_start) ## changed.\n",
        "    for i in range(start[0], stop[0]):\n",
        "        for j in range(12): # for each month\n",
        "            # now fill in selected channels as requried.\n",
        "\n",
        "            array[month][:len(key_list_ucdp)] = construct_channels(dataframe_ucdp[dataframe_ucdp.mon_month == extract_month], key_list = key_list_ucdp, prio_key = \"priogrid_gid\")\n",
        "            array[month][len(key_list_ucdp):] = construct_channels(dataframe_prio[dataframe_prio.year == i], key_list = key_list_prio, prio_key = 'gid')\n",
        "            print(extract_month)\n",
        "\n",
        "            month += 1\n",
        "            extract_month +=1\n",
        "    del month\n",
        "    return array\n",
        "\n",
        "\n",
        "\n",
        "def construct_channels(dataframe, key_list, prio_key = \"gid\"):\n",
        "    # usually used for prio\n",
        "    array = np.zeros((len(key_list), 360, 720))\n",
        "    for i, keys in enumerate(key_list):\n",
        "        array[i] = construct_layer(dataframe, key = keys, prio_key = prio_key)\n",
        "    return array\n",
        "\n",
        "\n",
        "\"\"\"check how we are dealing with cases that go up to and including the final step\"\"\"\n",
        "\n",
        "def construct_sequence(dataframe, key_list, prio_key = 'gid', start = [1989, 1, 1], stop = [2014,1,1], prio = False):\n",
        "    stop  = '2014-01-01'\n",
        "    # need to adapt ged and other year / month vs ged database for this.\n",
        "    # bool prio to add multiples of 12 to each year usin prio grid.\n",
        "    num_month = monotonic_date(stop, start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if prio == False:\n",
        "        # i.e if doing ucdp\n",
        "        # presumes adapted ucdp\n",
        "        # seq length, channels, height, width\n",
        "        array = np.zeros((num_month, len(key_list), 360, 720))\n",
        "\n",
        "        for month in range(num_month):\n",
        "            array[month] = construct_channels(dataframe[dataframe.mon_month == month], key_list = key_list, prio_key = \"priogrid_gid\")\n",
        "\n",
        "            # now for\n",
        "#            array[i] =\n",
        "    elif prio:\n",
        "        stop = [2014,1,1]\n",
        "        array = np.zeros((num_month, len(key_list), 360, 720))\n",
        "        # now for the prio grid data.\n",
        "        # need to make up remainder of start year,\n",
        "        # then multiples of 12 for each year\n",
        "        # then remainder of end year.\n",
        "\n",
        "        \"\"\"this presumes start dates @ start of year no more no less\"\"\"\n",
        "        # need to plus one at the end\n",
        "        month = 0\n",
        "        for i in range(start[0], stop[0]):\n",
        "            for j in range(12): # for each month\n",
        "                array[month] = construct_channels(dataframe[dataframe.year == i], key_list = key_list, prio_key = 'gid')\n",
        "                print(month)\n",
        "\n",
        "                month += 1\n",
        "        del month\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#        start_months = 13 - start[1] # i.e if its 1989,1,1 then there are 12 months left.\n",
        "#        years = stop[0] - start[0]  - 1 # -1 as due to method of making up start months. i.e\n",
        "#        # want [2013,1,1] [2014,1,1] to be dependant of start and stop months and have no year * 12 months additions\n",
        "#        finish_months = stop[1] # stop months\n",
        "#        months_interim = np.arange(start[0], stop[0]+1, 1)\n",
        "#\n",
        "#        ######\n",
        "#\n",
        "#        # now the start month multiples\n",
        "#        for i in range(start_months):\n",
        "#            array[i] = construct_channels(dataframe[dataframe.year == start[0]], key_list = key_list, prio_key = \"gid\")\n",
        "#        # double check prio yearly - try to get monthly values out.\n",
        "#\n",
        "#        for i in range(years):\n",
        "#            for j in range(i *12, (i+1)* 12):\n",
        "#                array[j]\n",
        "\n",
        "\n",
        "    return array\n",
        "\n",
        "\n",
        "def date_column(dataframe, baseline = [1989,1,1]):\n",
        "    # puts new column on dataframe, no need to return.\n",
        "    # date start just as dummy atm\n",
        "#    dataframe = dataframe[\"date_start\"]\n",
        "    vals = dataframe[\"date_start\"].values\n",
        "    new_col = np.array([monotonic_date(string_date) for string_date in vals])\n",
        "    dataframe[\"mon_month\"] = new_col\n",
        "\n",
        "def h5py_conversion(data_array, filename, key_list_ucdp, key_list_prio):\n",
        "    # this is for saving the default 360:720 file to chop out of.\n",
        "    # lazy loading saves the day\n",
        "    # all day\n",
        "    # every day\n",
        "    f = h5py.File(\"{}.hdf5\".format(filename), \"w\")\n",
        "\n",
        "    f.create_dataset(\"data_combined\", data = data_array)\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "    csv = open(name + \"_config.csv\", 'w')\n",
        "    csv.write(\"Included data UCDP:\\n\")\n",
        "    for key in key_list_ucdp:\n",
        "        csv.write(key + \"\\n\")\n",
        "\n",
        "    csv.write(\"Included data PRIO:\\n\")\n",
        "    for key in key_list_prio:\n",
        "        csv.write(key + \"\\n\")\n",
        "    csv.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M1AieJClWQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raster_test(input_data, chunk_size = 16):\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            print(input_data[:,i:i+chunk_size,j:j + chunk_size])\n",
        "            print(\".\")\n",
        "\n",
        "#    plt.imshow(input_data)\n",
        "\n",
        "def raster_selection(input_data, chunk_size = 16):\n",
        "    # here input_data is sequence step.\n",
        "    # data should be of dimensions seq, channels, height, width.\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    # this is not efficient.\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            input_data[0][i:i+chunk_size,j:j + chunk_size]\n",
        "\n",
        "    plt.imshow(input_data)\n",
        "\n",
        "\n",
        "def random_pixel_bounds(i, j, chunk_size = 16):\n",
        "    # returns the bounds of the image to select with a random pixel size.\n",
        "\n",
        "    height = random.randint(0, chunk_size-1)\n",
        "    width = random.randint(0, chunk_size-1)\n",
        "    # this randomly generates a of the image for where the pixel may be located\n",
        "    # randomly in the cut out image.\n",
        "    i_lower = i - height\n",
        "    i_upper = i + (chunk_size - height)\n",
        "\n",
        "    j_lower = j - width\n",
        "    j_upper = j + (chunk_size - width)\n",
        "\n",
        "    return [i_lower, i_upper, j_lower, j_upper]\n",
        "\n",
        "def random_selection(image, i, j, chunk_size = 16):\n",
        "\n",
        "    i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size = chunk_size)\n",
        "\n",
        "    print(image[i_lower:i_upper,j_lower:j_upper])\n",
        "\n",
        "# def random_grid_selection(image, sequence_step, chunk_size= 16, draws = 5, debug = True):\n",
        "#     if debug:\n",
        "#         print(\"Image shape is:\" , image.shape)\n",
        "\n",
        "\n",
        "#     # decide if this is going to be h5py loaded.\n",
        "\n",
        "#     # decide what sequence step is going to be like and how to return it\n",
        "\n",
        "#     # image is seq, channels, height, width\n",
        "\n",
        "#     # here we are using a seq length of 10. - could use 12 but atm we go for 10.\n",
        "\n",
        "#     # sequence step is the step at which the TRUTH is being extracted. the predictor sequence\n",
        "#     # is extracted from the 10 preceding steps. be careful to send in from i > 11\n",
        "#     assert sequence_step > 10, (\"This function selects the datapoints from this test set that contain\"\n",
        "#                                 \"a conflict event and then selects predictor data from the 10 preceding steps\"\n",
        "#                                 \" as a result i > 10 must be true\")\n",
        "#     # for sequence step, 0th layer - i.e fatalities\n",
        "#     y, x = np.where(image[sequence_step][0] >= 1)\n",
        "\n",
        "#     if debug:\n",
        "#         print(x.shape)\n",
        "\n",
        "#     truth_list = []\n",
        "#     predictor_list = []\n",
        "\n",
        "#     # re arange for loops for speed?\n",
        "#     for i,j in zip(y, x): # now over sites where fatalities have occured\n",
        "#         for _ in range(draws):\n",
        "#             i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size=chunk_size)\n",
        "\n",
        "#             # now need to work out how to store these. how to stack ontop ect.\n",
        "#             truth = image[sequence_step][0,i_lower:i_upper,j_lower:j_upper]\n",
        "#             # check these dimensions\n",
        "#             \"\"\"FIXED BELOW \"\"\"\n",
        "#             predictors = image[sequence_step-10:sequence_step, :,i_lower:i_upper,j_lower:j_upper]\n",
        "\n",
        "#             truth_list.append(truth)\n",
        "#             predictor_list.append(predictors)\n",
        "\n",
        "#     # finally we combine the previous arrays.\n",
        "\n",
        "#     return np.stack(predictor_list, axis= 0), np.stack(truth_list, axis = 0)\n",
        "\n",
        "\n",
        "def full_dataset_numpy(image, chunk_size = 16, draws = 5, debug = False):\n",
        "    # image is seq, channels, height, width\n",
        "    predictor_list = []\n",
        "    truth_list = []\n",
        "    for i in range(11, len(image)):\n",
        "        t1, t2 = random_grid_selection(image, i)\n",
        "        predictor_list.append(t1)\n",
        "        truth_list.append(t2)\n",
        "\n",
        "    truth_np = np.concatenate(truth_list, axis = 0)\n",
        "    predictor_np = np.concatenate(predictor_list, axis =0)\n",
        "    return predictor_np, truth_np\n",
        "\n",
        "def quick_dataset(data, name):\n",
        "    f = h5py.File(name + \".hdf5\", \"w\")\n",
        "    f.create_dataset(\"main\", data = data)\n",
        "#    f.create_dataset(\"truth\", data = truth)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "def debug_func1(dataframe, month):\n",
        "    a = dataframe[dataframe.mon_month == month]\n",
        "    print(len(a[a.best >0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQRln4TSlcjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def raster_test(input_data, chunk_size = 16):\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            print(input_data[:,i:i+chunk_size,j:j + chunk_size])\n",
        "            print(\".\")\n",
        "\n",
        "#    plt.imshow(input_data)\n",
        "\n",
        "def raster_selection(input_data, chunk_size = 16):\n",
        "    # here input_data is sequence step.\n",
        "    # data should be of dimensions seq, channels, height, width.\n",
        "    # to overcome edge sizes can make selection large if we just reject the training data for outside africa\n",
        "    # although we do not necessarily need to do this\n",
        "    # i.e expand box and allow less sampled box to sampel others more frequently.\n",
        "\n",
        "    # step size is always 1\n",
        "    # assuming image is a cutout of globe\n",
        "    # this is for single step, single channel as a test.\n",
        "    step = 1\n",
        "    height = input_data.shape[-2]\n",
        "    width = input_data.shape[-1]\n",
        "    # this is not efficient.\n",
        "    for i in range(height - chunk_size + 1):\n",
        "        for j in range(width - chunk_size+1):\n",
        "            input_data[0][i:i+chunk_size,j:j + chunk_size]\n",
        "\n",
        "    plt.imshow(input_data)\n",
        "\n",
        "\n",
        "def random_pixel_bounds(i, j, chunk_size = 16):\n",
        "    # returns the bounds of the image to select with a random pixel size.\n",
        "\n",
        "    height = random.randint(0, chunk_size-1)\n",
        "    width = random.randint(0, chunk_size-1)\n",
        "    # this randomly generates a of the image for where the pixel may be located\n",
        "    # randomly in the cut out image.\n",
        "    i_lower = i - height\n",
        "    i_upper = i + (chunk_size - height)\n",
        "\n",
        "    j_lower = j - width\n",
        "    j_upper = j + (chunk_size - width)\n",
        "\n",
        "    return [i_lower, i_upper, j_lower, j_upper]\n",
        "\n",
        "def random_selection(image, i, j, chunk_size = 16):\n",
        "\n",
        "    i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size = chunk_size)\n",
        "\n",
        "    print(image[i_lower:i_upper,j_lower:j_upper])\n",
        "\n",
        "def random_grid_selection(image, sequence_step, chunk_size= 16, draws = 5, cluster = False, min_events = 0, debug = True):\n",
        "    \"\"\"This version of random grid selection was modified on 14/11/2019\n",
        "    inputting this manually as the github tracking of collab notebooks appears\n",
        "    to be a little haywire\n",
        "\n",
        "    THIS VERSION PREDICTS IN ADVANCE AND DOESNT SPECIFY THAT AN EVENT WILL HAPPEN \n",
        "    IN ADVANCE.\n",
        "\n",
        "    NOW RETURNS TOP LEFT COORDS\n",
        "    MONTH IDENTIFIER\n",
        "    EVENT POINTS ? \n",
        "    \"\"\"\n",
        "    if debug:\n",
        "        print(\"Image shape is:\" , image.shape)\n",
        "\n",
        "\n",
        "    # decide if this is going to be h5py loaded.\n",
        "\n",
        "    # decide what sequence step is going to be like and how to return it\n",
        "\n",
        "    # image is seq, channels, height, width\n",
        "\n",
        "    # here we are using a seq length of 10. - could use 12 but atm we go for 10.\n",
        "\n",
        "    # sequence step is the step at which the TRUTH is being extracted. the predictor sequence\n",
        "    # is extracted from the 10 preceding steps. be careful to send in from i > 11\n",
        "    assert sequence_step > 10, (\"This function selects the datapoints from this test set that contain\"\n",
        "                                \"a conflict event and then selects predictor data from the 10 preceding steps\"\n",
        "                                \" as a result i > 10 must be true\")\n",
        "    # for sequence step, 0th layer - i.e fatalities\n",
        "    y, x = np.where(image[sequence_step][0] >= 1)\n",
        "#     print(y)\n",
        "#     print(x)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \"\"\"INCLUDE CLUSTERING IN HERE?????\"\"\"\n",
        "    if cluster:\n",
        "        clf = LocalOutlierFactor(n_neighbors=5, contamination= 0.05)\n",
        "        pred = clf.fit_predict(X)\n",
        "        # correct this but answer is this shape\n",
        "        np.hstack((a.reshape((-1,1)), b.reshape((-1,1))))\n",
        "        \n",
        "    \n",
        "    if debug:\n",
        "        print(x.shape)\n",
        "\n",
        "    truth_list = []\n",
        "    predictor_list = []\n",
        "    corner_list_i = []\n",
        "    corner_list_j = []\n",
        "    sequence_step_list = []\n",
        "    event_points_i = []\n",
        "    event_points_j = []\n",
        "\n",
        "    # re arange for loops for speed?\n",
        "    for i,j in zip(y, x): # now over sites where fatalities have occured\n",
        "        for _ in range(draws):\n",
        "            i_lower, i_upper, j_lower, j_upper = random_pixel_bounds(i, j, chunk_size=chunk_size)\n",
        "\n",
        "            # now need to work out how to store these. how to stack ontop ect.\n",
        "            truth = image[sequence_step+1][0,i_lower:i_upper,j_lower:j_upper]\n",
        "            # check these dimensions\n",
        "            \"\"\"fixed here\"\"\"\n",
        "            predictors = image[sequence_step-9:sequence_step+1, :,i_lower:i_upper,j_lower:j_upper]\n",
        "            \n",
        "            # if statement here to decide whether will be appended to full list\n",
        "            # i.e if it reaches the cutoff for acceptable level of conflict.\n",
        "#             print(np.count_nonzero(predictors[:,0]))\n",
        "#             print(sequence_step)\n",
        "#             print(predictors.shape)\n",
        "            if (np.count_nonzero(predictors[:,0]) >= min_events):\n",
        "                truth_list.append(truth)\n",
        "                predictor_list.append(predictors)\n",
        "                # APPEND THE I AND J LOCATIONS \n",
        "                # I LOWER AND J LOWER IS TOP LEFT CORNER\n",
        "                corner_list_i.append(i_lower)\n",
        "                corner_list_j.append(j_lower)\n",
        "                event_points_i.append(i)\n",
        "                event_points_j.append(j)\n",
        "                sequence_step_list.append(sequence_step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # finally we combine the previous arrays.\n",
        "    \n",
        "    return np.stack(predictor_list, axis= 0), np.stack(truth_list, axis = 0), corner_list_i, corner_list_j, event_points_i, event_points_j, sequence_step_list\n",
        "\n",
        "\n",
        "def full_dataset_numpy(image, chunk_size = 16, draws = 5, min_events = 0, debug = False):\n",
        "    # image is seq, channels, height, width\n",
        "    predictor_list = []\n",
        "    truth_list = []\n",
        "    for i in range(11, len(image)):\n",
        "        t1, t2 = random_grid_selection(image, i, min_events = min_events)\n",
        "        predictor_list.append(t1)\n",
        "        truth_list.append(t2)\n",
        "\n",
        "    truth_np = np.concatenate(truth_list, axis = 0)\n",
        "    predictor_np = np.concatenate(predictor_list, axis =0)\n",
        "    return predictor_np, truth_np\n",
        "\n",
        "def quick_dataset(data, name):\n",
        "    f = h5py.File(name + \".hdf5\", \"w\")\n",
        "    f.create_dataset(\"main\", data = data)\n",
        "#    f.create_dataset(\"truth\", data = truth)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "def debug_func1(dataframe, month):\n",
        "    a = dataframe[dataframe.mon_month == month]\n",
        "    print(len(a[a.best >0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdBXEc_pleuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_event_column(dataframe):\n",
        "    \"\"\" as it is hard to encode categorical information about battles when\n",
        "    battle deaths = 0, we then encode a binary - 0, 1 layer pertaining to\n",
        "    whether an event took place\"\"\"\n",
        "    new_col = np.ones(len(dataframe))\n",
        "    dataframe[\"binary_event\"] = new_col\n",
        "\n",
        "def nan_to_one(dataframe, key):\n",
        "    \"\"\"takes column from dataframe\"\"\"\n",
        "    dataframe[key] = dataframe[key].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QZV_Dtnlpvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pop_hyd_impute(data):\n",
        "    \"\"\"has data for 1990,1995,2000,2005.\n",
        "\n",
        "    for 1989 do 1990\n",
        "    1990 - 1994\n",
        "    1995 - 1999\n",
        "    2000 - 2004\n",
        "    2005 - 2014\n",
        "    \"\"\"\n",
        "    for i in range(1989, 1990):\n",
        "        data.loc[data.year == i, 'pop_hyd_sum'] = np.array(data.loc[data.year == 1990, 'pop_hyd_sum'])\n",
        "    for i in range(1991, 2000):\n",
        "        data.loc[data.year == i, 'pop_hyd_sum'] = np.array(data.loc[data.year == 1990, 'pop_hyd_sum'])\n",
        "    for i in range(2000,2005):\n",
        "        data.loc[data.year == i, 'pop_hyd_sum'] = np.array(data.loc[data.year == 2000, 'pop_hyd_sum'])\n",
        "    for i in range(2006,2015):\n",
        "        data.loc[data.year == i, 'pop_hyd_sum'] = np.array(data.loc[data.year == 2005, 'pop_hyd_sum'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rYJW9LpnwnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def full_dataset_h5py(image, filename, key_list_prio, key_list_ucdp, chunk_size = 16, draws = 5, min_events = 25, debug = False):\n",
        "    \"\"\" dataset is too large to combine in 12gb of ram - need to combine in h5py\n",
        "    array. i.e lazy saving as well as lazy loading\n",
        "    # TODO: REFORMAT\n",
        "    \"\"\"\n",
        "    \n",
        "    with h5py.File(filename + \".hdf5\", 'w') as f:\n",
        "        for i in range(11, len(image)-1):\n",
        "            print(i)\n",
        "            t1, t2, ci, cj, ei, ej, m = random_grid_selection(image, i, min_events = min_events)\n",
        "            if i == 11:\n",
        "                # creat h5py file at first step.\n",
        "                f.create_dataset('predictor', data= t1, maxshape=(None,None, None, None,None)) # compression=\"gzip\", chunks=True, taken out\n",
        "                f.create_dataset(\"truth\", data= t2, maxshape=(None,None,None))\n",
        "                f.create_dataset(\"corner_i\", data= ci, maxshape=(None,))\n",
        "                f.create_dataset(\"corner_j\", data= cj, maxshape=(None,))\n",
        "                f.create_dataset(\"event_i\", data= ei, maxshape=(None,))\n",
        "                f.create_dataset(\"event_j\", data= ej, maxshape=(None,))\n",
        "                f.create_dataset(\"month\", data= m, maxshape=(None,))\n",
        "        \n",
        "            else:\n",
        "                f[\"predictor\"].resize((f[\"predictor\"].shape[0] + t1.shape[0]), axis = 0) # expand dataset\n",
        "                f[\"truth\"].resize((f[\"truth\"].shape[0] + t2.shape[0]), axis = 0)\n",
        "                f[\"corner_i\"].resize((f[\"corner_i\"].shape[0] + ci.shape[0]), axis = 0)\n",
        "                f[\"corner_j\"].resize((f[\"corner_j\"].shape[0] + cj.shape[0]), axis = 0)\n",
        "                f[\"event_i\"].resize((f[\"event_i\"].shape[0] + ei.shape[0]), axis = 0)\n",
        "                f[\"event_j\"].resize((f[\"event_j\"].shape[0] + ej.shape[0]), axis = 0)\n",
        "                f[\"month\"].resize((f[\"month\"].shape[0] + m.shape[0]), axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                f[\"predictor\"][-t1.shape[0]:] = t1 # place new data in expanded dataset\n",
        "                f[\"truth\"][-t2.shape[0]:] = t2\n",
        "                f[\"corner_i\"][-ci.shape[0]:] = ci\n",
        "                f[\"corner_j\"][-cj.shape[0]:] = cj\n",
        "                f[\"event_i\"][-ei.shape[0]:] = ei\n",
        "                f[\"event_j\"][-ej.shape[0]:] = ej\n",
        "                f[\"month\"][-m.shape[0]:] = m\n",
        "\n",
        "                \n",
        "        \n",
        "        f[\"predictor\"].attrs.create(\"key_prio\", np.string_(key_list_prio))\n",
        "        f[\"predictor\"].attrs.create(\"key_ucdp\", np.string_(key_list_ucdp))\n",
        "        \n",
        "        \n",
        "    \n",
        "#     f.close()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-yKaq1ho21k",
        "colab_type": "text"
      },
      "source": [
        "## pipeline funcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wce8GPIEo1bi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Dec  1 15:50:51 2019\n",
        "\n",
        "@author: Gareth Lomax\n",
        "\"\"\"\n",
        "\n",
        "def find_avg_lazy_load(data, div = 10000):\n",
        "    \"\"\"Extracts average and std from produced hdf5 datasets\n",
        "    Uses hdf5 lazy loading to subdivide produced image sequence datasets and extract\n",
        "    an overall average, when the produced datasets are too large to naively average\n",
        "    and find the standard deviation for.\n",
        "    Parameters\n",
        "    ----------\n",
        "    data: hdf5 file\n",
        "        Takes loaded hdf5 file. File should have two datasets, accessible via\n",
        "        the keys: 'predictor' and 'truth'. Full_dataset_h5py produces suitable\n",
        "        files.\n",
        "    div: int\n",
        "        The number of datasamples to average over at a time.\n",
        "    Returns\n",
        "    -------\n",
        "    avg: list of int\n",
        "        List of averages for each channel in the input image sequence dataset\n",
        "    std: List of int\n",
        "        List of standard deviations for each channel in the inout image sequence\n",
        "        dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    predictor = data[\"predictor\"]\n",
        "    channel_num = predictor.shape[2]\n",
        "    avg = np.zeros(channel_num)\n",
        "    std = np.zeros_like(avg)\n",
        "\n",
        "\n",
        "    for i in range(channel_num):\n",
        "        # batching as cant fit into ram\n",
        "\n",
        "        batch_avg = 0\n",
        "        batch_std = 0\n",
        "        for j in range(int(len(predictor)/div)):\n",
        "\n",
        "            batch_avg += np.sum(predictor[j * div: (j+1)*div,:,i])\n",
        "\n",
        "            batch_std += np.sum(predictor[j * div: (j+1)*div,:,i] * predictor[j * div: (j+1)*div,:,i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        batch_avg += np.sum(predictor[int(len(predictor)/div)*div: len(predictor),:,i])\n",
        "        batch_std += np.sum(predictor[int(len(predictor)/div)*div: len(predictor),:,i]*predictor[int(len(predictor)/div)*div: len(predictor),:,i])\n",
        "\n",
        "        sing_chan_shape = np.array(predictor.shape)\n",
        "        sing_chan_shape[2] = 1\n",
        "        batch_avg /= np.prod(sing_chan_shape)\n",
        "        batch_std /= np.prod(sing_chan_shape)\n",
        "\n",
        "        batch_std = np.sqrt(batch_std - batch_avg**2)\n",
        "\n",
        "        avg[i] = batch_avg\n",
        "        std[i] = batch_std\n",
        "\n",
        "\n",
        "    return avg, std\n",
        "\n",
        "def make_dataset(filename_root):\n",
        "    \"\"\"\n",
        "    Filename root should be root of filename to be appended to dataset\n",
        "    \"\"\"\n",
        "    # loading in data\n",
        "    data_prio = pd.read_csv(\"data/PRIO-GRID Yearly Variables for 1946-2014 - 2019-07-26.csv\")\n",
        "    data_ucdp = pd.read_csv(\"data/ged191.csv\")\n",
        "\n",
        "    # define north west south east\n",
        "    north = 37.32\n",
        "    south = -34.5115\n",
        "    west = -17.3113\n",
        "    east = 51.2752\n",
        "\n",
        "    # select data in africa\n",
        "    data_ucdp = data_ucdp[(data_ucdp.latitude >= south) & (data_ucdp.latitude <= north) & (data_ucdp.longitude >= west) & (east >= data_ucdp.longitude)]\n",
        "\n",
        "    # monotonic date and nan to one\n",
        "    date_column(data_ucdp)\n",
        "    binary_event_column(data_ucdp)\n",
        "    nan_to_one(data_prio, \"petroleum_y\")\n",
        "    nan_to_one(data_prio, \"drug_y\")\n",
        "    pop_hyd_impute(data_prio)\n",
        "    nan_to_one(data_prio, \"pop_hyd_sum\")\n",
        "    nan_to_one(data_prio, \"capdist\")\n",
        "\n",
        "    #  load in premade test set\n",
        "    test_set = np.load(\"map_new_test.npy\")\n",
        "\n",
        "    # make binary\n",
        "    test_set[:,0][test_set[:,0] > 0] = 1\n",
        "\n",
        "    #check that only contains 0, 1\n",
        "    print(np.unique(test_set[:,0]))\n",
        "\n",
        "    # make the dataset\n",
        "    full_dataset_h5py(test_set, filename_root,key_list_prio, key_list_ucdp)\n",
        "\n",
        "    with h5py.File(filename_root + \".hdf5\", 'r') as f:\n",
        "\n",
        "        avg, std = find_avg_lazy_load(f)\n",
        "\n",
        "        np.save(filename_root + \"_avg\", avg)\n",
        "        np.save(filename_root + \"_avg\", std)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x75KIv3Mo58T",
        "colab_type": "text"
      },
      "source": [
        "# calling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOG03Ivco45U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_dataset(\"map_test_dataset\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}