{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msc-acse/acse-9-independent-research-project-Garethlomax/blob/full_data_run/full_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq1onVYuKcDt",
        "colab_type": "text"
      },
      "source": [
        "#imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgHHu2lwKF0S",
        "colab_type": "code",
        "outputId": "7ac72c32-95fa-44bb-ae04-bc5ba7c93b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2k695HbKic0",
        "colab_type": "code",
        "outputId": "31e1a867-08cd-4664-9df2-76d10b074ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/MovingMNIST-master\n",
        "\n",
        "# all torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "# importing moving mnist test set.\n",
        "from MovingMNIST import MovingMNIST\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorch-summary-master\n",
        "from torchsummary import summary\n",
        "\n",
        "# %cd /content/drive/My \\Drive/masters_project/python_modules/pytorch_modelsize-master\n",
        "\n",
        "%cd /content/drive/My \\Drive/masters_project/python_modules/pytorchvis-master\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "%cd /content/drive/My\\ Drive/masters_project/python_modules/pytorch-ssim-master\n",
        "import pytorch_ssim # cite this \n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/python_modules/MovingMNIST-master\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "[Errno 2] No such file or directory: '/content/drive/My Drive/masters_project/python_modules/pytorchvis-master'\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-summary-master\n",
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.1.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.16.4)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3521 sha256=41f5790a737f5d783c2fc2f32e380a32dcce52a0f553bcb97d460bcded87169b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n",
            "/content/drive/My Drive/masters_project/python_modules/pytorch-ssim-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0y9nWYUKmsy",
        "colab_type": "code",
        "outputId": "081010f4-0ebc-4608-9727-013249770bad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "h5py.run_tests()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".....................................................x...................................................................x....................................s...s......ss.......................................................................................................ssssss...................................................................x....x.........................x......x.................................................ssss..................\n",
            "----------------------------------------------------------------------\n",
            "Ran 457 tests in 1.548s\n",
            "\n",
            "OK (skipped=14, expected failures=6)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=457 errors=0 failures=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHqL0wCqKr6O",
        "colab_type": "text"
      },
      "source": [
        "## snippet to investigate ram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8VGlxIDKq8t",
        "colab_type": "code",
        "outputId": "1b06c47c-d3e5-4d32-d254-4f45f3e626aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=6722d1afbf79e7715e0af143d4481e6829faf16d398f107f2b5580cc73866493\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 309.7 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbWslp2UKzwn",
        "colab_type": "code",
        "outputId": "9a2bf8f7-9873-48fd-a1c7-7424f195a895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9Fp3Nk3K03D",
        "colab_type": "text"
      },
      "source": [
        "## cuda imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poqrux9tK2ap",
        "colab_type": "code",
        "outputId": "de653d61-5bab-4082-d485-6c0038009f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    \n",
        "    \n",
        "import random\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "  \n",
        "set_seed(42)\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--S3DiL3KcVy",
        "colab_type": "text"
      },
      "source": [
        "# LSTM CELL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj3Icdv2K8z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: CUDIFY EVERYTHING\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSTMunit(nn.Module):\n",
        "    def __init__(self, input_channel_no, hidden_channels_no, kernel_size, stride = 1):\n",
        "        super(LSTMunit, self).__init__()\n",
        "        \"\"\"base unit for an overall convLSTM structure. convLSTM exists in keras but\n",
        "        not pytorch. LSTMunit repersents one cell in an overall convLSTM encoder decoder format\n",
        "        the structure of convLSTMs lend themselves well to compartmentalising the LSTM\n",
        "        cells. \n",
        "    \n",
        "        Each cell takes an input the data at the current timestep Xt, and a hidden\n",
        "        representation from the previous timestep Ht-1\n",
        "    \n",
        "        Each cell outputs Ht\n",
        "        \"\"\"\n",
        "    \n",
        "    \n",
        "        self.input_channels = input_channel_no\n",
        "    \n",
        "        self.output_channels = hidden_channels_no\n",
        "    \n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "        self.padding = (int((self.kernel_size - 1) / 2 ), int((self.kernel_size - 1) / 2 ))#to ensure output image same dims as input\n",
        "        # as in conv nowcasting - see references \n",
        "        self.stride = stride # for same reasons as above\n",
        "        \n",
        "        # need convolutions, cells, tanh, sigmoid?\n",
        "        # need input size for the lstm - on size of layers.\n",
        "        # cannot do this because of the modules not being registered when stored in a list\n",
        "        # can if we convert it to a parameter dict\n",
        "    \n",
        "        # list of names of filter to put in dictionary.\n",
        "        # some of these are not convolutions\n",
        "        \"\"\"TODO: CHANGE THIS LAYOUT OF CONVOLUTIONAL LAYERS. \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.filter_name_list = ['Wxi', 'Wxf', 'Wxc', 'Wxo','Whi', 'Whf', 'Whc', 'Who']\n",
        "        \n",
        "        \"\"\" TODO : DEAL WITH BIAS HERE. \"\"\" \n",
        "        \"\"\" TODO: CAN INCLUDE BIAS IN ONE OF THE CONVOLUTIONS BUT NOT ALL OF THEM - OR COULD INCLUDE IN ALL? \"\"\"\n",
        "\n",
        "        # list of concolution instances for each lstm cell step\n",
        "       #  nn.Conv2d(1, 48, kernel_size=3, stride=1, padding=0),\n",
        "        self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False).cuda() for i in range(4)]\n",
        "#         self.conv_list = [nn.Conv2d(self.input_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = False) for i in range(4)]\n",
        "\n",
        "#         self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True)).double() for i in range(4)]\n",
        "\n",
        "        self.conv_list = self.conv_list + [(nn.Conv2d(self.output_channels, self.output_channels, kernel_size =  self.kernel_size, stride = self.stride, padding = self.padding, bias = True).cuda()).double() for i in range(4)]\n",
        "#         self.conv_list = nn.ModuleList(self.conv_list)\n",
        "        # stores nicely in dictionary for compact readability.\n",
        "        # most ML code is uncommented and utterly unreadable. Here we try to avoid this\n",
        "        self.conv_dict = nn.ModuleDict(zip(self.filter_name_list, self.conv_list))\n",
        "    \n",
        "        # may be able to combine all the filters and combine all the things to be convolved - as long as there is no cross layer convolution\n",
        "        # technically the filter will be the same? - check this later.\n",
        "    \n",
        "        # set up W_co, W_cf, W_co as variables.\n",
        "        \"\"\" TODO: decide whether this should be put into function. \"\"\"\n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: put correct dimensions of tensor in shape\"\"\"\n",
        "        \n",
        "        # of dimensions seq length, hidden layers, height, width\n",
        "        \"\"\"TODO: DEFINE THESE SYMBOLS. \"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN CONSTRUCTOR.\"\"\"\n",
        "        shape = [1, self.output_channels, 16, 16]\n",
        "        \n",
        "        self.Wco = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wcf = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        self.Wci = nn.Parameter((torch.zeros(shape).double()).cuda(), requires_grad = True)\n",
        "        \n",
        "        \n",
        "#         self.Wco = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wcf = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wci = nn.Parameter((torch.zeros(shape).double()), requires_grad = True)\n",
        "#         self.Wco.name = \"test\"\n",
        "#         self.Wco = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wcf = torch.zeros(shape, requires_grad = True).double()\n",
        "#         self.Wci = torch.zeros(shape, requires_grad = True).double()\n",
        "\n",
        "        # activation functions.\n",
        "        self.tanh = torch.tanh\n",
        "        self.sig  = torch.sigmoid\n",
        "\n",
        "#     (1, 6, kernel_size=5, padding=2, stride=1).double()\n",
        "    def forward(self, x, h, c):\n",
        "        \"\"\" put the various nets in here - instanciate the other convolutions.\"\"\"\n",
        "        \"\"\"TODO: SORT BIAS OUT HERE\"\"\"\n",
        "        \"\"\"TODO: PUT THIS IN SELECTOR FUNCTION? SO ONLY PUT IN WXI ECT TO MAKE EASIER TO DEBUG?\"\"\"\n",
        "#         print(\"size of x is:\")\n",
        "#         print(x.shape)\n",
        "        # ERROR IS IN LINE 20\n",
        "        #print(self.conv_dict['Wxi'](x).shape)\n",
        "#         print(\"X:\")\n",
        "#         print(x.is_cuda)\n",
        "#         print(\"H:\")\n",
        "#         print(h.is_cuda)\n",
        "#         print(\"C\")\n",
        "#         print(c.is_cuda)\n",
        "        \n",
        "        i_t = self.sig(self.conv_dict['Wxi'](x) + self.conv_dict['Whi'](h) + self.Wci * c)\n",
        "        f_t = self.sig(self.conv_dict['Wxf'](x) + self.conv_dict['Whf'](h) + self.Wcf * c)\n",
        "        c_t = f_t * c + i_t * self.tanh(self.conv_dict['Wxc'](x) + self.conv_dict['Whc'](h))\n",
        "        o_t = self.sig(self.conv_dict['Wxo'](x) + self.conv_dict['Who'](h) + self.Wco * c_t)\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "        \n",
        "        return h_t, c_t\n",
        "    \n",
        "    def copy_in(self):\n",
        "        \"\"\"dummy function to copy in the internals of the output in the various architectures i.e encoder decoder format\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dR0F-kKKcYD",
        "colab_type": "text"
      },
      "source": [
        "# lstm full unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jt5nXmELFjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"TODO: IMPORTANT \n",
        "WHEN COPYING STATES OVER, INITIAL STATE OF DECODER IS BOTH LAST H AND LAST C \n",
        "FROM THE LSTM BEING COPIED FROM.\n",
        "\n",
        "WE ALSO NEED TO INCLUDE THE ABILITY TO OUTPUT THE LAST H AND C AT EACH TIMESTEP\n",
        "AS INPUT.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" SEQUENCE, BATCH SIZE, LAYERS, HEIGHT, WIDTH\"\"\"\n",
        "\n",
        "class LSTMmain(nn.Module):\n",
        "    \n",
        "    \n",
        "    \"\"\" collection of units to form encoder/ decoder branches - decide which are which\n",
        "    need funcitonality to copy in and copy out outputs.\n",
        "    \n",
        "    \n",
        "    layer output is array of booleans selectively outputing for each layer i.e \n",
        "    for three layer can have output on second and third but not first with \n",
        "    layer_output = [0,1,1]\"\"\"\n",
        "    \n",
        "    \"\"\"TODO: DECIDE ON OUTPUT OF HIDDEN CHANNEL LIST \"\"\"\n",
        "    def __init__(self, shape, input_channel_no, hidden_channel_no, kernel_size, layer_output, test_input, copy_bool = False, debug = False, save_outputs = True, decoder = False, second_debug = False):\n",
        "        super(LSTMmain, self).__init__()\n",
        "        \n",
        "        \"\"\"TODO: USE THIS AS BASIS FOR ENCODER DECODER.\"\"\"\n",
        "        \"\"\"TODO: SPECIFY SHAPE OF INPUT VECTOR\"\"\"\n",
        "        \n",
        "        \"\"\"TODO: FIGURE OUT HOW TO IMPLEMENT ENCODER DECODER ARCHITECUTRE\"\"\"\n",
        "        self.copy_bool = copy_bool\n",
        "        \n",
        "        self.test_input = test_input\n",
        "        \n",
        "        self.debug = debug\n",
        "        self.second_debug = second_debug\n",
        "        self.save_all_outputs = save_outputs\n",
        "        \n",
        "        self.shape = shape\n",
        "        \n",
        "        \"\"\"specify dimensions of shape - as in channel length ect. figure out once put it in a dataloader\"\"\"\n",
        "        \n",
        "        self.layers = len(test_input) #number of layers in the encoder. \n",
        "        \n",
        "        self.seq_length = shape[1]\n",
        "        \n",
        "        self.enc_len = len(shape)\n",
        "        \n",
        "        self.input_chans = input_channel_no\n",
        "        \n",
        "        self.hidden_chans = hidden_channel_no\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.layer_output = layer_output\n",
        "        \n",
        "        # initialise the different conv cells. \n",
        "#         self.unit_list = [LSTMunit(input_channel_no, hidden_channel_no, kernel_size) for i in range(self.enc_len)]\n",
        "        self.dummy_list = [input_channel_no] + list(self.test_input) # allows test input to be an array\n",
        "        if self.debug:\n",
        "            print(\"dummy_list:\")\n",
        "            print(self.dummy_list)\n",
        "            \n",
        "#         self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        self.unit_list = nn.ModuleList([(LSTMunit(self.dummy_list[i], self.dummy_list[i+1], kernel_size).double()).cuda() for i in range(len(self.test_input))])\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"number of units:\")\n",
        "            print(len(self.unit_list))\n",
        "#             print(\"number of \")\n",
        "\n",
        "#         self.unit_list = nn.ModuleList(self.unit_list)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, copy_in = False, copy_out = [False, False, False]):\n",
        "#     def forward(self, x):\n",
        "#         copy_in = False\n",
        "#         copy_out = [False, False, False]\n",
        "\n",
        "        \n",
        "#         print(\"IS X CUDA?\")\n",
        "#         print(x.is_cuda)\n",
        "        \"\"\"loop over layers, then over hidden states\n",
        "        \n",
        "        copy_in is either False or is [[h,c],[h,c]] ect.\n",
        "        \n",
        "        THIS IN NOW CHANGED TO COPY IN \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        internal_outputs = []\n",
        "        \"\"\"TODO: HOW MANY OUTPUTS TO SAVE\"\"\"\n",
        "        \"\"\" S \"\"\"\n",
        "        \n",
        "        \"\"\" TODO: PUT INITIAL ZERO THROUGH THE SYSTEM TO DEFINE H AND C\"\"\"\n",
        "        \n",
        "        layer_output = [] # empty list to save each h and c for each step. \n",
        "        \"\"\"TODO: DECIDE WHETHER THE ABOVE SHOULD BE ARRAY OR NOT\"\"\"\n",
        "        \n",
        "        # x is 5th dimensional tensor.\n",
        "        # x is of size batch, sequence, layers, height, width\n",
        "        \n",
        "        \"\"\"TODO: INITIALISE THESE WITH VECTORS.\"\"\"\n",
        "        # these need to be of dimensions (batchsizze, hidden_dim, heigh, width)\n",
        "        \n",
        "        size = x.shape\n",
        "        \n",
        "        # need to re arrange the outputs. \n",
        "        \n",
        "        \n",
        "        \"\"\"TODO: SORT OUT H SIZING. \"\"\"\n",
        "        \n",
        "        batch_size = size[0]\n",
        "        # change this. h should be of dimensions hidden size, hidden size.\n",
        "        h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "        h_shape[1] = self.hidden_chans\n",
        "        if self.debug:\n",
        "            print(\"h_shape:\")\n",
        "            print(h_shape)\n",
        "        \n",
        "        # size should be (seq, batch_size, layers, height, weight)\n",
        "        \n",
        "        \n",
        "        empty_start_vectors = []\n",
        "        \n",
        "        \n",
        "        #### new method of copying vectors. copy_bool, assigned during object \n",
        "        # construction now deals iwth copying in values.\n",
        "        # copy in is still used to supply the tensor values. \n",
        "    \n",
        "        k = 0 # to count through our input state list.\n",
        "        for i in range(self.layers):\n",
        "            if self.copy_bool[i]: # if copy bool is true for this layer\n",
        "                # check purpose of h_shape in below code.\n",
        "                empty_start_vectors.append(copy_in[k])\n",
        "                # copies in state for that layer\n",
        "                \"\"\"TODO: CHECK IF THIS NEEDS TO BE DETATCHED OR NOT\"\"\"\n",
        "                k += 1 # iterate through input list.\n",
        "            \n",
        "            else: # i.e if false\n",
        "                assert self.copy_bool[i] == False, \"copy_bool arent bools\"\n",
        "                \n",
        "                h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "                h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "                empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "        del k # clear up k so no spare variables flying about.\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             \"\"\"CHANGED: NOW HAS COPY IN COPY OUT BASED ON [[0,0][H,C]] FORMAT\"\"\"\n",
        "#             if copy_in == False: # i.e if no copying in occurs then proceed as normal\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "# #                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "# #             elif copy_in[i] == [0,0]:\n",
        "#             elif isinstance(copy_in[i], list):\n",
        "\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 # if no copying in in alternate format\n",
        "#                 h_shape = list(x.shape[:1] + x.shape[2:]) # seq is second, we miss it with fancy indexing\n",
        "#                 h_shape[1] = self.dummy_list[i+1] # check indexing. \n",
        "#                 empty_start_vectors.append([(torch.zeros(h_shape).double()).cuda(), (torch.zeros(h_shape).double()).cuda()])\n",
        "                \n",
        "#             else: # copy in the provided vectors\n",
        "#                 assert (len(copy_in) == self.layers), \"Length disparity between layers, copy in format\"\n",
        "\n",
        "#                 \"\"\"TODO: DECIDE WHETHER TO CHANGE THIS TO AN ASSERT BASED OFF TYPE OF TENSOR.\"\"\"\n",
        "#                 empty_start_vectors.append(copy_in[i])\n",
        "                \n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "#         empty_start_vectors = [[torch.zeros(h_shape), torch.zeros(h_shape)] for i in range(self.layers)]\n",
        "        \n",
        "        \n",
        "        \n",
        "        if self.debug:\n",
        "            for i in empty_start_vectors:\n",
        "                print(i[0].shape)\n",
        "            print(\" \\n \\n \\n\")\n",
        "        \n",
        "#         for i in range(self.layers):\n",
        "#             empty_start_vectors.append([torch.tensor()])\n",
        "        \n",
        "        total_outputs = []\n",
        "        \n",
        "        \n",
        "        for i in range(self.layers):\n",
        "            \n",
        "            \n",
        "            layer_output = []\n",
        "            if self.debug:\n",
        "                print(\"layer iteration:\")\n",
        "                print(i)\n",
        "            # for each in layer\n",
        "\n",
        "            \"\"\"AS WE PUT IN ZEROS EACH TIME THIS MAKES OUR LSTM STATELESS\"\"\"\n",
        "            # initialise with zero or noisy vectors \n",
        "            # at start of each layer put noisy vector in \n",
        "            # look at tricks paper to find more effective ideas of how to put this in\n",
        "            # do we have to initialise with 0 tensors after we go to the second layer\n",
        "            # or does the h carry over???\n",
        "            \"\"\"TODO: REVIEW THIS CHANGE\"\"\"\n",
        "            \n",
        "            # copy in for each layer. \n",
        "            # this is used for encoder decoder architectures.\n",
        "            # default is to put in empty vectors. \n",
        "            \n",
        "            \"\"\"TODO: REVIEW THIS SECTION\"\"\"\n",
        "            \"\"\"CHANGED: TO ALWAYS CHOOSE H AND C\"\"\"\n",
        "#             if copy_in == False:\n",
        "#                 h, c = empty_start_vectors[i]\n",
        "#             else: h, c = copy_in[i]\n",
        "\n",
        "            h, c = empty_start_vectors[i] \n",
        "                \n",
        "            if self.debug:\n",
        "                print(\"new h shape\")\n",
        "                print(h.shape)\n",
        "                \n",
        "            \"\"\"TODO: DO WE HAVE TO PUT BLANK VECTORS IN AT EACH TIMESTEP?\"\"\"\n",
        "            \n",
        "            # need to initialise zero states for c and h. \n",
        "            for j in range(self.seq_length):\n",
        "                if self.debug:\n",
        "                    print(\"inner loop iteration:\")\n",
        "                    print(j)\n",
        "                if self.debug:\n",
        "                    print(\"x dtype is:\" , x.dtype)\n",
        "                # for each step in the sequence\n",
        "                # put x through \n",
        "                # i.e put through each x value at a given time.\n",
        "                \n",
        "                \"\"\"TODO: PUT H IN FROM PREVIOUS LAYER, BUT C SHOULD BE ZEROS AT START\"\"\"\n",
        "                \n",
        "                if self.debug:\n",
        "                    print(\"inner loop size:\")\n",
        "                    print(x[:,j].shape)\n",
        "                    print(\"h size:\")\n",
        "                    print(h.shape)\n",
        "                    \n",
        "                h, c = self.unit_list[i](x[:,j], h, c)\n",
        "                \n",
        "                # this is record for each output in given layer.\n",
        "                # this depends whether copying out it enabld \n",
        "#                 i\n",
        "                layer_output.append([h, c])\n",
        "                \n",
        "            \"\"\"TODO: IMPLEMENT THIS\"\"\"\n",
        "#             if self.save_all_outputs[i]:\n",
        "#                 total_outputs.append(layer_outputs[:,0]) # saves h from each of the layer outputs\n",
        "                \n",
        "            # output \n",
        "            \"\"\"OUTSIDE OF SEQ LOOP\"\"\"\n",
        "            \"\"\"TODO: CHANGE TO NEW OUTPUT METHOD.\"\"\"\n",
        "            if copy_out[i] == True:\n",
        "                # if we want to copy out the contents of this layer:\n",
        "                internal_outputs.append(layer_output[-1])\n",
        "                # saves last state and memory which can be subsequently unrolled.\n",
        "                # when used in an encoder decoder format.\n",
        "            \"\"\"removed else statement\"\"\"\n",
        "#             else:\n",
        "#                 internal_outputs.append([0,0])\n",
        "                # saves null variable so we can check whats being sent out.\n",
        "            \n",
        "            \n",
        "            h_output = [i[0] for i in layer_output] #layer_output[:,0] # take h from each timestep.\n",
        "            if self.debug:\n",
        "                print(\"h_output is of size:\")\n",
        "                print(h_output[0].shape)\n",
        "                \n",
        "                      \n",
        "            \"\"\"TODO: REVIEW IF 1 IS THE CORRECT AXIS TO CONCATENATE THE VECTORS ALONG\"\"\"\n",
        "            # we now use h as the predictor input to the other layers.\n",
        "            \"\"\"TODO: STACK TENSORS ALONG NEW AXIS. \"\"\"\n",
        "            \n",
        "            \n",
        "            x = torch.stack(h_output,0)\n",
        "            x = torch.transpose(x, 0, 1)\n",
        "            if self.second_debug:\n",
        "                print(\"x shape in LSTM main:\" , x.shape)\n",
        "            if self.debug:\n",
        "                print(\"x reshaped dimensions:\")\n",
        "                print(x.shape)\n",
        "        \n",
        "#         x = torch.zeros(x.shape)\n",
        "#         x.requires_grad = True\n",
        "        return x , internal_outputs # return new h in tensor form. do we need to cudify this stuff\n",
        "\n",
        "    def initialise(self):\n",
        "        \"\"\"put through zeros to start everything\"\"\"\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSvkO-LjLL_h",
        "colab_type": "text"
      },
      "source": [
        "# lstm enc dec onestep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NYS0hX5LI0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test2 = LSTMmain(shape, 1, 3, 5, [1], test_input = [1,2], debug = False).double()\n",
        "\n",
        "\n",
        "\n",
        "class LSTMencdec_onestep(nn.Module):\n",
        "    \"\"\"structure is overall architecture of \"\"\"\n",
        "    def __init__(self, structure, input_channels, kernel_size = 5, debug = True):\n",
        "        super(LSTMencdec_onestep, self).__init__()\n",
        "#         assert isinstance(structure, np.array), \"structure should be a 2d numpy array\"\n",
        "        assert len(structure.shape) == 2, \"structure should be a 2d numpy array with two rows\"\n",
        "        self.debug = debug\n",
        "        \n",
        "        \"\"\"TODO: MAKE KERNEL SIZE A LIST SO CAN SPECIFY AT EACH JUNCTURE.\"\"\"\n",
        "        shape = [1,10,3,16,16]\n",
        "        \n",
        "        self.structure = structure\n",
        "        \"\"\"STRUCTURE IS AN ARRAY - CANNOT USE [] + [] LIST CONCATENATION - WAS ADDING ONE ONTO THE ARRAY THING.\"\"\"\n",
        "        self.input_channels = input_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        \"\"\"TODO: ASSERT THAT DATATYPE IS INT.\"\"\"\n",
        "        \n",
        "        self.enc_shape, self.dec_shape, self.enc_copy_out, self.dec_copy_in = self.input_test()\n",
        "        \n",
        "        if self.debug:\n",
        "            print(\"enc_shape, dec_shape, enc_copy_out, dec_copy_in:\")\n",
        "            print(self.enc_shape)\n",
        "            print(self.dec_shape)\n",
        "            print(self.enc_copy_out)\n",
        "            print(self.dec_copy_in)\n",
        "            \n",
        "        \n",
        "        \n",
        "#         self.sig = nn.Sigmoid()\n",
        "        \n",
        "         # why does this have +1 at third input and decoder hasnt?????? \n",
        "        \n",
        "        self.encoder = LSTMmain(shape, self.input_channels, len(self.enc_shape)+1, self.kernel_size, layer_output = self.enc_copy_out, test_input = self.enc_shape, copy_bool = [False for k in range(len(self.enc_shape))]  ).cuda()\n",
        "        # now one step in sequence\n",
        "        shape = [1,1,1,64,64]\n",
        "\n",
        "        self.decoder = LSTMmain(shape, self.enc_shape[-1], len(self.dec_shape), self.kernel_size, layer_output = 1, test_input = self.dec_shape, copy_bool = self.dec_copy_in,  second_debug = False).cuda()\n",
        "        \n",
        "        \n",
        "        \n",
        "        # initialise encoder and decoder network\n",
        "    \n",
        "    def input_test(self):\n",
        "        \"\"\"check input structure to make sure there is overlap between encoder \n",
        "        and decoder.\n",
        "        \"\"\"\n",
        "        copy_grid = []\n",
        "        # finds dimensions of the encoder\n",
        "        enc_layer = self.structure[0]\n",
        "        enc_shape = enc_layer[enc_layer!=0]\n",
        "        dec_layer = self.structure[1]\n",
        "        dec_shape = dec_layer[dec_layer!=0]\n",
        "#         \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        #set up boolean grid of where the overlaps are.\n",
        "        for i in range(len(enc_layer)):\n",
        "            if self.debug:\n",
        "                print(enc_layer[i], dec_layer[i])\n",
        "            if (enc_layer[i] != 0) and (dec_layer[i] != 0):\n",
        "                copy_grid.append(True)\n",
        "            else:\n",
        "                copy_grid.append(False)\n",
        "                \n",
        "                \n",
        "        enc_overlap = copy_grid[:len(enc_layer)-1]\n",
        "        \n",
        "        num_dec_zeros = len(dec_layer[dec_layer==0]) # will this break if no zeros?\n",
        "        \n",
        "        dec_overlap = copy_grid[num_dec_zeros:]\n",
        "        \n",
        "        return enc_shape, dec_shape, enc_overlap, dec_overlap\n",
        "        \n",
        "#         dec_overlap = copy_grid[]                \n",
        "        \n",
        "                \n",
        "                \n",
        "#         [[1,2,3,0],\n",
        "#          [0,2,3,1]]\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x, out_states = self.encoder(x, copy_in = False, copy_out = self.enc_copy_out)\n",
        "        \n",
        "#         print(\"length of out_states:\", len(out_states))\n",
        "#         print(\"contents out outstates are as follows:\")\n",
        "#         for i in out_states:\n",
        "#             print(\"----------------------------------\")\n",
        "#             print(\"first object type:\", type(i[0]))\n",
        "# #             print(\"length of object:\", len(i[0]))\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        dummy_input = torch.zeros(x.shape)\n",
        "        # technically a conditional loader - put x in there \n",
        "        # puts in the last one as input - should make shorter. \n",
        "        # presume coming out in the correct order - next try reversing to see if that helps \n",
        "        x = x[:,-1:,:,:,:]\n",
        "#         print(\"x shape encoder:\", x.shape)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        \n",
        "        res, _ = self.decoder(x, copy_in = out_states, copy_out = [False, False, False,False, False])\n",
        "        print(\"FINISHING ONE PASS\")\n",
        "#         res = self.sig(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbyJ0SfNLVVT",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8YZKbahLin2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        \n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qppFhApXLlIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iKjRtJ-PL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset_HDF5_full(valid_frac = 0.1, dataset_length = 9000):\n",
        "    \"\"\"\n",
        "    Returns datasets for training and validation. \n",
        "    \n",
        "    Loads in datasets segmenting for validation fractions.\n",
        "   \n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    if valid_frac != 0:\n",
        "        \n",
        "        dummy = np.array(range(dataset_length)) # clean this up - not really needed\n",
        "        \n",
        "        train_index, valid_index = validation_split(dummy, n_splits = 1, valid_fraction = 0.1, random_state = 0)\n",
        "        \n",
        "        train_dataset = HDF5Dataset(\"train_set.hdf5\", index_map = train_index)\n",
        "        \n",
        "        valid_dataset = HDF5Dataset(\"test_set.hdf5\", index_map = valid_index)\n",
        "        \n",
        "        return train_dataset, valid_dataset\n",
        "        \n",
        "    else:\n",
        "        print(\"not a valid fraction for validation\") # turn this into an assert.\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4SHaDXKcap",
        "colab_type": "text"
      },
      "source": [
        "# shuffling functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khitUnTHLu0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation_split(data, n_splits = 1, valid_fraction = 0.1, random_state = 0):\n",
        "    \"\"\"\n",
        "    Function to produce a validation set from test set.\n",
        "    THIS SHUFFLES THE SAMPLES. __NOT__ THE SEQUENCES.\n",
        "    \"\"\"\n",
        "    dummy_array = np.zeros(len(data))\n",
        "    split = StratifiedShuffleSplit(n_splits, test_size = valid_fraction, random_state = 0)\n",
        "    generator = split.split(torch.tensor(dummy_array), torch.tensor(dummy_array))\n",
        "    return [(a,b) for a, b in generator][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPCRgC06L0ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unsqueeze_data(data):\n",
        "    \"\"\"\n",
        "    Takes in moving MNIST object - must then account for \n",
        "    \"\"\"\n",
        "    \n",
        "    # split moving mnist data into predictor and ground truth.\n",
        "    predictor = data[:][0].unsqueeze(2)\n",
        "    predictor = predictor.double()\n",
        "        \n",
        "    truth = data[:][1].unsqueeze(2)# this should be the moving mnist sent in\n",
        "    truth = truth.double()\n",
        "    \n",
        "    return predictor, truth\n",
        "    # the data should now be unsqueezed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d6g-caEL4-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialise_dataset(data):\n",
        "    # unsqueeze data, adding a channel dimension for later convolution. \n",
        "    # this also gets rid of the annoying tuple format\n",
        "    predictor, truth = unsqueeze_data(data)\n",
        "    \n",
        "    train_index, valid_index = validation_split(data)\n",
        "    \n",
        "    train_predictor = predictor[train_index]\n",
        "    valid_predictor = predictor[valid_index]\n",
        "    \n",
        "    train_truth = truth[train_index]\n",
        "    valid_truth = truth[valid_index]\n",
        "    \n",
        "    train_dataset = SequenceDataset(train_predictor, train_truth)\n",
        "    valid_dataset = SequenceDataset(valid_predictor, valid_truth)\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BL54iWHMUtj",
        "colab_type": "text"
      },
      "source": [
        "#TRAINING FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtQQVIoxMWnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def comb_loss_func(pred, y):\n",
        "    \"\"\"hopefully should work like kl and bce for VAE\"\"\"\n",
        "    mse = nn.MSELoss()\n",
        "    ssim = pytorch_ssim.SSIM()\n",
        "    mse_loss = mse(pred, y[:,:1,:,:,:])\n",
        "    ssim_loss = -ssim(pred[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    return mse_loss + ssim_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R8deKvmMblY",
        "colab_type": "code",
        "outputId": "4b345f6d-4348-41cb-86c6-fc3c3cf4bd37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/masters_project/data/models\n",
        "def train_enc_dec(model, optimizer, dataloader, loss_func = nn.MSELoss()):\n",
        "    \"\"\"\n",
        "    training function \n",
        "    \n",
        "    by default mseloss\n",
        "    \n",
        "    could try brier score.\n",
        "    \n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    model.train() # enables training for model. \n",
        "    tot_loss = 0\n",
        "    for x, y in dataloader:\n",
        "#         print(\"training\")\n",
        "        x = x.to(device) # send to cuda.\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() # zeros saved gradients in the optimizer.\n",
        "        # prevents multiple stacking of gradients\n",
        "        # this is important to do before we evaluate the model as the \n",
        "        # model is currenly in model.train() mode\n",
        "        \n",
        "        prediction = model(x) #x should be properly formatted - of size\n",
        "        \"\"\"THIS DOESNT DEAL WITH SEQUENCE LENGTH VARIANCE OF PREDICTION OR Y\"\"\"\n",
        "        \n",
        "#         print(\"the size of prediction is:\", prediction.shape)\n",
        "        #last image sequence.\n",
        "    \n",
        "        \"\"\"ACTUAL FUNCTION THATS BEEN COMMENTED OUT.\"\"\"\n",
        "#         loss = loss_func(prediction, y[:,:1,:,:,:])\n",
        "        \"\"\"CHANGED BECAUSE \"\"\"\n",
        "        print(prediction.shape)\n",
        "        print(y.shape)\n",
        "        loss = loss_func(prediction[:,0,0], y)\n",
        "        \n",
        "\n",
        "#         loss = comb_loss_func(prediction, y)\n",
        "#         print(prediction.shape)\n",
        "#         print(y[:,:1,:,:,:].shape)\n",
        "        \"\"\"commented out \"\"\"\n",
        "#         loss = - loss_func(prediction[:,0,:,:,:], y[:,0,:,:,:])\n",
        "    \n",
        "# ssim_out = -ssim_loss(train[0][0][-1:],  x[0])\n",
        "# ssim_value = - ssim_out.data\n",
        "    \n",
        "    \n",
        "        \n",
        "        loss.backward() # differentiates to find minimum.\n",
        "#         printm()\n",
        "\n",
        "        ##\n",
        "        # implement the interpreteable stuff here.\n",
        "        # as it is very unlikely we predict every pixel correctly we will not \n",
        "        # use accuracy. \n",
        "        # technically this is a regression problem, not a classification.\n",
        "        \n",
        "        \n",
        "        optimizer.step() # steps forward the optimizer.\n",
        "        # uses loss.backward() to give gradient. \n",
        "        # loss is negative.\n",
        "#         del x # make sure the garbage is collected.\n",
        "#         del y\n",
        "        \"\"\"commented it out\"\"\"\n",
        "        tot_loss += loss.item() # .data.item() \n",
        "        print(\"BATCH:\")\n",
        "        print(i)\n",
        "        i += 1\n",
        "#         if i == 20:\n",
        "#             break\n",
        "        print(\"MSE_LOSS:\", tot_loss / i)\n",
        "    return model, tot_loss # trainloss, trainaccuracy \n",
        "\n",
        "def validate(model, dataloader, criterion = nn.MSELoss()):\n",
        "    \"\"\"as for train_enc_dec but without training - and acting upon validation\n",
        "    data set\n",
        "    \"\"\"\n",
        "    model.eval() # puts out of train mode so we do not mess up our gradients\n",
        "    for x, y in dataloader:\n",
        "        with torch.no_grad: # no longer have to specify tensors \n",
        "            # as volatile = True. as of modern pytorch use torch.no_grad.\n",
        "            \n",
        "            x.to(device) # send to cuda.\n",
        "            y.to(device)\n",
        "            prediction = model(x)\n",
        "            \n",
        "            loss = loss_func(prediction, y)\n",
        "            \n",
        "            \n",
        "    return validloss, validaccuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_main(model, params, train, valid, epochs = 30, batch_size = 1):\n",
        "    # make sure model is ported to cuda\n",
        "    # make sure seed has been specified if testing comparative approaches\n",
        "    \n",
        "#     if model.is_cuda == False:\n",
        "#         model.to(device)\n",
        "    \n",
        "    # initialise optimizer on model parameters \n",
        "    # chann\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, amsgrad= True)\n",
        "    loss_func = nn.MSELoss()\n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "    validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        \n",
        "        torch.save(optimizer.state_dict(), F\"Adam_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), F\"Test_new_ams_changed\"+str(epoch)+\".pth\")\n",
        "#         validate(model, validation_loader)\n",
        "        \n",
        "    return model, optimizer\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88zV9JwqN3c7",
        "colab_type": "text"
      },
      "source": [
        "# hdf5 with avgs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAdu0-tVN51S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HDF5Dataset_with_avgs(Dataset):\n",
        "    \"\"\"dataset wrapper for hdf5 dataset to allow for lazy loading of data. This \n",
        "    allows ram to be conserved. \n",
        "    \n",
        "    As the hdf5 dataset is not partitioned into test and validation, the dataset \n",
        "    takes a shuffled list of indices to allow specification of training and \n",
        "    validation sets.\n",
        "    \n",
        "    MAKE SURE TO CALL DEL ON GENERATED OBJECTS OTHERWISE WE WILL CLOG UP RAM\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, path, index_map, avg, std, application_boolean, transform = None):\n",
        "        \n",
        "        %cd /content/drive/My \\Drive/masters_project/data \n",
        "        # changes directory to the one where needed.\n",
        "        \n",
        "        self.path = path\n",
        "        \n",
        "        self.index_map = index_map # maps to the index in the validation split\n",
        "        # due to hdf5 lazy loading index map must be in ascending order.\n",
        "        # this may be an issue as we should shuffle our dataset.\n",
        "        # this will be raised as an issue as we consider a work around.\n",
        "        # we should keep index map shuffled, and take the selection from the \n",
        "        # shuffled map and select in ascending order. \n",
        "        self.avg = avg\n",
        "        self.std = std\n",
        "        self.application_boolean = application_boolean\n",
        "        \n",
        "        self.file = h5py.File(path, 'r')\n",
        "        \n",
        "#         for i in range(len(application_boolean)):\n",
        "#             # i.e gaussian transformation doesnt happen. (x - mu / sigma)\n",
        "#             if application_boolean == 0:\n",
        "#                 self.avg[i] = 0\n",
        "#                 self.std[i] = 1\n",
        "        \n",
        "        \n",
        "         \n",
        "          \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "    \n",
        "    def __getitem__(self,i):\n",
        "        \n",
        "        i = self.index_map[i] # index maps from validation set to select new orders\n",
        "#         print(i)\n",
        "        if isinstance(i, list): # if i is a list. \n",
        "            i.sort() # sorts into ascending order as specified above\n",
        "            \n",
        "        \"\"\"TODO: CHECK IF THIS RETURNS DOUBLE\"\"\"\n",
        "        \n",
        "        predictor = torch.tensor(self.file[\"predictor\"][i])\n",
        "        print(\"predictor shape:\", predictor.shape)\n",
        "        # is of batch size, seq length, \n",
        "        \n",
        "        \n",
        "        truth = torch.tensor(self.file[\"truth\"][i])\n",
        "        print(\"truth shape:\", truth.shape)\n",
        "        # only on layer so not in loop.\n",
        "        truth -= self.avg[0]\n",
        "        truth /= self.std[0]\n",
        "        \n",
        "        if isinstance(i, list):\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,:,j] -= self.avg[j]\n",
        "                    predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            for j in range(len(self.avg)):\n",
        "                if self.application_boolean[j]:\n",
        "                    predictor[:,j] -= self.avg[j]\n",
        "                    predictor[:,j] /= self.std[j]\n",
        "                \n",
        "            \n",
        "#             #i.e if we are returning a single index.\n",
        "# #         # the value of truth should be [0] in the predictor array. \n",
        "#         for j in range(len(self.avg)):\n",
        "#             if self.application_boolean[j]:\n",
        "#                 predictor[:,:,j] -= self.avg[j]\n",
        "#                 predictor[:,:,j] /= self.std[j]\n",
        "                \n",
        "#                 # sort out dimensions of truth at some point \n",
        "        \n",
        "        \n",
        "                \n",
        "            \n",
        "        \n",
        "        return predictor, truth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG-2m6rnOLPz",
        "colab_type": "text"
      },
      "source": [
        "# wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS33z7MHOMIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrapper_full(name, optimizer,  structure, loss_func, avg, std, application_boolean, lr = None, epochs = 50, kernel_size = 3, batch_size = 50):\n",
        "    f = open(name + \".csv\", 'w') # open csv file for saving\n",
        "    \n",
        "    model = LSTMencdec_onestep(structure, 5, kernel_size = kernel_size).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, amsgrad= True)\n",
        "\n",
        "    # put in structure.\n",
        "    f.write(\"Structure: \\n\")\n",
        "\n",
        "    for i in range(len(structure)):\n",
        "        for j in range(len(structure[0])):\n",
        "            f.write(\"{},\".format(structure[i,j]))\n",
        "        f.write(\"\\n\") # new line\n",
        "\n",
        "    f.write(\"Parameters:\\n\")\n",
        "    f.write(\"optimizer, epochs, learning rate, kernel size \\n\")\n",
        "    \n",
        "    if lr != None:\n",
        "        # optimizer problems\n",
        "        \n",
        "        f.write(\"{},{},{},{},{}\\n\".format(\"test\", epochs, lr, kernel_size, batch_size))\n",
        "    else:\n",
        "        f.write(\"{},{},{},{},{}\\n\".format(\"othertest\", epochs, \"Default\", kernel_size, batch_size))\n",
        "        \n",
        "    f.write(\"loss_func:\\n\")\n",
        "    f.write(loss_func.__repr__() + \"\\n\")\n",
        "    \n",
        "    f.write(\"optimizer:\\n\")\n",
        "    f.write(optimizer.__repr__() + \"\\n\")\n",
        "    \n",
        "    f.write(\"\\n\\n\\n\")\n",
        "    f.write(\"TRAINING\\n\")\n",
        "    \n",
        "    # now we define the training functions\n",
        "#     train, valid = initialise_dataset_HDF5()\n",
        "    \n",
        "    index_map = np.arange(0, 84485,1)# dummy index_map just to make sure trains correctly. \n",
        "    train = HDF5Dataset_with_avgs('data_prio_run_test4.hdf5', index_map,avg, std, apbln)\n",
        "    \n",
        "    # put model together with different size\n",
        "    # model test - put it together.\n",
        "    # now we train the model\n",
        "    \n",
        "    \n",
        "    \n",
        "    # ignore this one.\n",
        "    \n",
        "#     model, optimizer = train_main(test_model, 1, train, valid, epochs = epochs, batch_size = 50)\n",
        "\n",
        "    loss_func = loss_func\n",
        "    \n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr = lr, amsgrad= True)\n",
        "    \n",
        "#     loss_func = nn.BCELoss()\n",
        "#     loss_func = pytorch_ssim.SSIM()\n",
        "    f.close()\n",
        "    \n",
        "    train_loader = DataLoader(train, batch_size = batch_size, shuffle = True) # implement moving MNIST data input\n",
        "#     validation_loader = DataLoader(valid, batch_size = batch_size, shuffle = False) # implement moving MNIST\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        # does is matter that we arent returning the model?\n",
        "        _, loss = train_enc_dec(model, optimizer, train_loader, loss_func = loss_func) # changed\n",
        "        \n",
        "        f = open(name + \".csv\", 'a') # open csv file for saving\n",
        "\n",
        "        f.write(str(loss) + \"\\n\")\n",
        "        \n",
        "        f.close()\n",
        "        \n",
        "        torch.save(optimizer.state_dict(), name+str(epoch)+\".pth\")\n",
        "        torch.save(model.state_dict(), name+str(epoch)+\".pth\")\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "#     f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mdz5tVBMlur",
        "colab_type": "text"
      },
      "source": [
        "# RUN CODE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TPlTW4-Mnnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# changed \n",
        "structure = np.array([[24,48,0,0,0],[0,48,24,12,5]])\n",
        "\n",
        "# test_model = LSTMencdec_onestep(structure, 1, kernel_size = 3).to(device)\n",
        "# print(\"seq length\", test_model.decoder.seq_length)\n",
        "# optim = torch.optim.Adam(test_model.parameters())\n",
        "\n",
        "\n",
        "\n",
        "# train_enc = train_enc_dec(test_model,)\n",
        "\n",
        "\n",
        "# train_main(test_model, 1, train, valid, epochs = 2, batch_size = 50)\n",
        "\n",
        "# model, optimizer = train_main(test_model, 1, train, valid, epochs = 50, batch_size = 50)\n",
        "\n",
        "\n",
        "# torch.save(optimizer.state_dict(), F\"Finished_opt_bce.pth\")\n",
        "# torch.save(model.state_dict(), F\"Finished_mod_bce.pth\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q29pFNMoOg4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test1 = np.zeros((50,1,1,16,16))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nefIDQzoOnbx",
        "colab_type": "code",
        "outputId": "470d7b2b-a00f-4dfb-c83e-038a6d2ceb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test1[:,0,0].shape\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kulxvr-3G9RZ",
        "colab_type": "code",
        "outputId": "63f5a558-b0f3-422a-efc8-7e60f55a9419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pwd\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/masters_project/data/models'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuOuuQUBW8v6",
        "colab_type": "text"
      },
      "source": [
        "# trying to unfuck averaging \n",
        "\n",
        "todo - put simple occurance of conflict as prediction - occurance of massive events - way bigger than average lead to issues with non gaussian distribution of event sizes. \n",
        "use binary to predict.\n",
        "re run dataset creation tonight and add single binary prediction\n",
        "- powerlaw transformation?\n",
        "- double check what the loss function is comparing. \n",
        "- make example where it imshows just for fun so can see what the inputs are.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L2iuV5GqdMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbc90d43-605a-4b4c-a5ea-e174735e70f8"
      },
      "source": [
        "%pwd\n",
        "%cd .."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKk1EIXUXwFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg = np.load(\"dset4_avg.npy\")\n",
        "std = np.load(\"dset4_std.npy\")\n",
        "apbln = [1,0,0,0,1] # think this is correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRTblglIXyl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_ELkwJ8HMMw",
        "colab_type": "code",
        "outputId": "ac543f84-1cf5-497e-92a9-32ab602080db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# %cd ..\n",
        "index_map = np.arange(0,84485,1)\n",
        "train = HDF5Dataset_with_avgs('data_prio_run_test4.hdf5', list(index_map),avg, std, apbln)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masters_project/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nquiRWWaeV_x",
        "colab_type": "code",
        "outputId": "658c5349-6ac8-4964-a059-f40ecf034d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "pred = train[0:200][0]\n",
        "\n",
        "tru = train[0:200][1] "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictor shape: torch.Size([200, 10, 5, 16, 16])\n",
            "truth shape: torch.Size([200, 16, 16])\n",
            "predictor shape: torch.Size([200, 10, 5, 16, 16])\n",
            "truth shape: torch.Size([200, 16, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CghmBSi7nkrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWef8waDnqAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tru.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGalPmdIqvg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj9hQMw7siXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cc15c0b-0ca8-49b8-c482-cdcefd13f05c"
      },
      "source": [
        "-4.4e-2 == -0.044"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LdOToLkq_Be",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f685ac1-e03c-4f32-b109-6d9263598998"
      },
      "source": [
        "# event 164 has massive mseloss due to large error\n",
        "pred[164][-1][0].unique(return_counts = True) # why not lots of -0.44 ect \n",
        "# plt.imshow(pred[164][-1][0])\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.0440,  0.0822], dtype=torch.float64), tensor([255,   1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubUHdh1qrSr_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "93d69b9e-2cea-4c1f-8d91-175c40aacdb2"
      },
      "source": [
        "tru[164].unique(return_counts = True) \n",
        "# pred[164][-1][0].shape\n",
        "\n",
        "\n",
        "# plt.imshow(tru[164])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-4.4027e-02,  3.3456e-01,  1.7227e+00,  2.2711e+02],\n",
              "        dtype=torch.float64), tensor([253,   1,   1,   1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5lUsVkdnzIn",
        "colab_type": "code",
        "outputId": "0d8a2710-f216-4ef9-b323-2c92fcea03dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b(pred[164][-1][0],tru[164])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(201.5630, dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQpw9FEKoYZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a6c1524-3d60-467b-c85b-0b45de89f440"
      },
      "source": [
        "tot = 0\n",
        "for i in range(200):\n",
        "    print(i)\n",
        "    print(b(pred[i][-1][0], tru[i]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor(0.1703, dtype=torch.float64)\n",
            "1\n",
            "tensor(0.0004, dtype=torch.float64)\n",
            "2\n",
            "tensor(0.0002, dtype=torch.float64)\n",
            "3\n",
            "tensor(6.2207e-05, dtype=torch.float64)\n",
            "4\n",
            "tensor(0.0002, dtype=torch.float64)\n",
            "5\n",
            "tensor(0.0001, dtype=torch.float64)\n",
            "6\n",
            "tensor(0.2046, dtype=torch.float64)\n",
            "7\n",
            "tensor(0.0002, dtype=torch.float64)\n",
            "8\n",
            "tensor(0.0002, dtype=torch.float64)\n",
            "9\n",
            "tensor(0.0364, dtype=torch.float64)\n",
            "10\n",
            "tensor(0.2119, dtype=torch.float64)\n",
            "11\n",
            "tensor(0.4460, dtype=torch.float64)\n",
            "12\n",
            "tensor(0.4458, dtype=torch.float64)\n",
            "13\n",
            "tensor(0.4536, dtype=torch.float64)\n",
            "14\n",
            "tensor(0.4460, dtype=torch.float64)\n",
            "15\n",
            "tensor(0.0002, dtype=torch.float64)\n",
            "16\n",
            "tensor(0.0001, dtype=torch.float64)\n",
            "17\n",
            "tensor(0.2048, dtype=torch.float64)\n",
            "18\n",
            "tensor(0.0003, dtype=torch.float64)\n",
            "19\n",
            "tensor(0.0002, dtype=torch.float64)\n",
            "20\n",
            "tensor(0.2122, dtype=torch.float64)\n",
            "21\n",
            "tensor(0.2047, dtype=torch.float64)\n",
            "22\n",
            "tensor(0.2044, dtype=torch.float64)\n",
            "23\n",
            "tensor(0.2044, dtype=torch.float64)\n",
            "24\n",
            "tensor(0.2119, dtype=torch.float64)\n",
            "25\n",
            "tensor(0.2122, dtype=torch.float64)\n",
            "26\n",
            "tensor(0.2123, dtype=torch.float64)\n",
            "27\n",
            "tensor(0.2042, dtype=torch.float64)\n",
            "28\n",
            "tensor(0.2119, dtype=torch.float64)\n",
            "29\n",
            "tensor(0.2044, dtype=torch.float64)\n",
            "30\n",
            "tensor(0.2044, dtype=torch.float64)\n",
            "31\n",
            "tensor(0.2044, dtype=torch.float64)\n",
            "32\n",
            "tensor(0.2048, dtype=torch.float64)\n",
            "33\n",
            "tensor(0.2123, dtype=torch.float64)\n",
            "34\n",
            "tensor(0.2042, dtype=torch.float64)\n",
            "35\n",
            "tensor(0.1853, dtype=torch.float64)\n",
            "36\n",
            "tensor(0.1854, dtype=torch.float64)\n",
            "37\n",
            "tensor(0.1685, dtype=torch.float64)\n",
            "38\n",
            "tensor(0.1851, dtype=torch.float64)\n",
            "39\n",
            "tensor(0.1778, dtype=torch.float64)\n",
            "40\n",
            "tensor(0.2077, dtype=torch.float64)\n",
            "41\n",
            "tensor(0.0108, dtype=torch.float64)\n",
            "42\n",
            "tensor(0.0033, dtype=torch.float64)\n",
            "43\n",
            "tensor(0.2152, dtype=torch.float64)\n",
            "44\n",
            "tensor(0.0111, dtype=torch.float64)\n",
            "45\n",
            "tensor(0.1760, dtype=torch.float64)\n",
            "46\n",
            "tensor(0.0085, dtype=torch.float64)\n",
            "47\n",
            "tensor(0.2119, dtype=torch.float64)\n",
            "48\n",
            "tensor(0.0075, dtype=torch.float64)\n",
            "49\n",
            "tensor(0.2119, dtype=torch.float64)\n",
            "50\n",
            "tensor(0.0010, dtype=torch.float64)\n",
            "51\n",
            "tensor(0.0010, dtype=torch.float64)\n",
            "52\n",
            "tensor(0.0010, dtype=torch.float64)\n",
            "53\n",
            "tensor(0.0016, dtype=torch.float64)\n",
            "54\n",
            "tensor(0.0010, dtype=torch.float64)\n",
            "55\n",
            "tensor(0.0560, dtype=torch.float64)\n",
            "56\n",
            "tensor(0.0560, dtype=torch.float64)\n",
            "57\n",
            "tensor(0.0560, dtype=torch.float64)\n",
            "58\n",
            "tensor(0.0560, dtype=torch.float64)\n",
            "59\n",
            "tensor(0.3232, dtype=torch.float64)\n",
            "60\n",
            "tensor(0.0006, dtype=torch.float64)\n",
            "61\n",
            "tensor(0.0006, dtype=torch.float64)\n",
            "62\n",
            "tensor(0.0016, dtype=torch.float64)\n",
            "63\n",
            "tensor(0.0006, dtype=torch.float64)\n",
            "64\n",
            "tensor(0.0006, dtype=torch.float64)\n",
            "65\n",
            "tensor(0.2270, dtype=torch.float64)\n",
            "66\n",
            "tensor(0.0379, dtype=torch.float64)\n",
            "67\n",
            "tensor(1.2117, dtype=torch.float64)\n",
            "68\n",
            "tensor(0.2130, dtype=torch.float64)\n",
            "69\n",
            "tensor(1.2117, dtype=torch.float64)\n",
            "70\n",
            "tensor(0.9961, dtype=torch.float64)\n",
            "71\n",
            "tensor(0.9006, dtype=torch.float64)\n",
            "72\n",
            "tensor(0.8215, dtype=torch.float64)\n",
            "73\n",
            "tensor(0.9867, dtype=torch.float64)\n",
            "74\n",
            "tensor(0.9563, dtype=torch.float64)\n",
            "75\n",
            "tensor(0.2126, dtype=torch.float64)\n",
            "76\n",
            "tensor(1.1056, dtype=torch.float64)\n",
            "77\n",
            "tensor(0.4027, dtype=torch.float64)\n",
            "78\n",
            "tensor(1.1056, dtype=torch.float64)\n",
            "79\n",
            "tensor(1.1056, dtype=torch.float64)\n",
            "80\n",
            "tensor(0.3084, dtype=torch.float64)\n",
            "81\n",
            "tensor(1.1016, dtype=torch.float64)\n",
            "82\n",
            "tensor(1.1482, dtype=torch.float64)\n",
            "83\n",
            "tensor(1.0634, dtype=torch.float64)\n",
            "84\n",
            "tensor(1.0195, dtype=torch.float64)\n",
            "85\n",
            "tensor(0.7416, dtype=torch.float64)\n",
            "86\n",
            "tensor(1.0494, dtype=torch.float64)\n",
            "87\n",
            "tensor(0.8814, dtype=torch.float64)\n",
            "88\n",
            "tensor(1.0213, dtype=torch.float64)\n",
            "89\n",
            "tensor(0.7416, dtype=torch.float64)\n",
            "90\n",
            "tensor(1.4054, dtype=torch.float64)\n",
            "91\n",
            "tensor(0.4679, dtype=torch.float64)\n",
            "92\n",
            "tensor(0.2717, dtype=torch.float64)\n",
            "93\n",
            "tensor(1.3633, dtype=torch.float64)\n",
            "94\n",
            "tensor(0.6695, dtype=torch.float64)\n",
            "95\n",
            "tensor(0.2044, dtype=torch.float64)\n",
            "96\n",
            "tensor(0.9703, dtype=torch.float64)\n",
            "97\n",
            "tensor(0.2854, dtype=torch.float64)\n",
            "98\n",
            "tensor(0.8473, dtype=torch.float64)\n",
            "99\n",
            "tensor(0.2854, dtype=torch.float64)\n",
            "100\n",
            "tensor(0.4748, dtype=torch.float64)\n",
            "101\n",
            "tensor(0.4307, dtype=torch.float64)\n",
            "102\n",
            "tensor(0.4748, dtype=torch.float64)\n",
            "103\n",
            "tensor(0.3913, dtype=torch.float64)\n",
            "104\n",
            "tensor(0.4748, dtype=torch.float64)\n",
            "105\n",
            "tensor(0.0427, dtype=torch.float64)\n",
            "106\n",
            "tensor(0.0604, dtype=torch.float64)\n",
            "107\n",
            "tensor(0.6996, dtype=torch.float64)\n",
            "108\n",
            "tensor(0.7416, dtype=torch.float64)\n",
            "109\n",
            "tensor(0.0427, dtype=torch.float64)\n",
            "110\n",
            "tensor(0.9942, dtype=torch.float64)\n",
            "111\n",
            "tensor(0.3384, dtype=torch.float64)\n",
            "112\n",
            "tensor(0.0610, dtype=torch.float64)\n",
            "113\n",
            "tensor(1.0374, dtype=torch.float64)\n",
            "114\n",
            "tensor(0.0081, dtype=torch.float64)\n",
            "115\n",
            "tensor(0.0030, dtype=torch.float64)\n",
            "116\n",
            "tensor(0.0030, dtype=torch.float64)\n",
            "117\n",
            "tensor(13.6388, dtype=torch.float64)\n",
            "118\n",
            "tensor(9.9637, dtype=torch.float64)\n",
            "119\n",
            "tensor(10.5319, dtype=torch.float64)\n",
            "120\n",
            "tensor(0.2119, dtype=torch.float64)\n",
            "121\n",
            "tensor(0.2147, dtype=torch.float64)\n",
            "122\n",
            "tensor(2.9645, dtype=torch.float64)\n",
            "123\n",
            "tensor(3.0364, dtype=torch.float64)\n",
            "124\n",
            "tensor(2.8430, dtype=torch.float64)\n",
            "125\n",
            "tensor(0.1104, dtype=torch.float64)\n",
            "126\n",
            "tensor(0.1104, dtype=torch.float64)\n",
            "127\n",
            "tensor(0.1104, dtype=torch.float64)\n",
            "128\n",
            "tensor(0.1104, dtype=torch.float64)\n",
            "129\n",
            "tensor(0.1104, dtype=torch.float64)\n",
            "130\n",
            "tensor(2.9500, dtype=torch.float64)\n",
            "131\n",
            "tensor(65.0352, dtype=torch.float64)\n",
            "132\n",
            "tensor(2.9545, dtype=torch.float64)\n",
            "133\n",
            "tensor(3.1172, dtype=torch.float64)\n",
            "134\n",
            "tensor(2.9945, dtype=torch.float64)\n",
            "135\n",
            "tensor(0.2307, dtype=torch.float64)\n",
            "136\n",
            "tensor(0.2307, dtype=torch.float64)\n",
            "137\n",
            "tensor(0.2307, dtype=torch.float64)\n",
            "138\n",
            "tensor(0.2307, dtype=torch.float64)\n",
            "139\n",
            "tensor(0.2307, dtype=torch.float64)\n",
            "140\n",
            "tensor(0.0010, dtype=torch.float64)\n",
            "141\n",
            "tensor(0.0100, dtype=torch.float64)\n",
            "142\n",
            "tensor(2.9270, dtype=torch.float64)\n",
            "143\n",
            "tensor(65.0441, dtype=torch.float64)\n",
            "144\n",
            "tensor(62.2218, dtype=torch.float64)\n",
            "145\n",
            "tensor(0.0901, dtype=torch.float64)\n",
            "146\n",
            "tensor(0.0974, dtype=torch.float64)\n",
            "147\n",
            "tensor(0.1780, dtype=torch.float64)\n",
            "148\n",
            "tensor(0.0974, dtype=torch.float64)\n",
            "149\n",
            "tensor(0.1780, dtype=torch.float64)\n",
            "150\n",
            "tensor(0.0901, dtype=torch.float64)\n",
            "151\n",
            "tensor(0.0130, dtype=torch.float64)\n",
            "152\n",
            "tensor(0.2603, dtype=torch.float64)\n",
            "153\n",
            "tensor(0.0901, dtype=torch.float64)\n",
            "154\n",
            "tensor(0.0901, dtype=torch.float64)\n",
            "155\n",
            "tensor(65.0366, dtype=torch.float64)\n",
            "156\n",
            "tensor(264.5313, dtype=torch.float64)\n",
            "157\n",
            "tensor(2.9067, dtype=torch.float64)\n",
            "158\n",
            "tensor(2.9142, dtype=torch.float64)\n",
            "159\n",
            "tensor(2.9150, dtype=torch.float64)\n",
            "160\n",
            "tensor(201.5629, dtype=torch.float64)\n",
            "161\n",
            "tensor(201.5632, dtype=torch.float64)\n",
            "162\n",
            "tensor(201.5632, dtype=torch.float64)\n",
            "163\n",
            "tensor(201.5642, dtype=torch.float64)\n",
            "164\n",
            "tensor(201.5630, dtype=torch.float64)\n",
            "165\n",
            "tensor(0.1058, dtype=torch.float64)\n",
            "166\n",
            "tensor(0.0130, dtype=torch.float64)\n",
            "167\n",
            "tensor(201.5630, dtype=torch.float64)\n",
            "168\n",
            "tensor(201.5632, dtype=torch.float64)\n",
            "169\n",
            "tensor(201.5632, dtype=torch.float64)\n",
            "170\n",
            "tensor(201.5631, dtype=torch.float64)\n",
            "171\n",
            "tensor(201.5706, dtype=torch.float64)\n",
            "172\n",
            "tensor(263.7702, dtype=torch.float64)\n",
            "173\n",
            "tensor(201.5623, dtype=torch.float64)\n",
            "174\n",
            "tensor(201.5623, dtype=torch.float64)\n",
            "175\n",
            "tensor(62.2119, dtype=torch.float64)\n",
            "176\n",
            "tensor(63.6115, dtype=torch.float64)\n",
            "177\n",
            "tensor(263.7626, dtype=torch.float64)\n",
            "178\n",
            "tensor(263.7626, dtype=torch.float64)\n",
            "179\n",
            "tensor(62.2124, dtype=torch.float64)\n",
            "180\n",
            "tensor(0.0947, dtype=torch.float64)\n",
            "181\n",
            "tensor(0.0310, dtype=torch.float64)\n",
            "182\n",
            "tensor(0.0947, dtype=torch.float64)\n",
            "183\n",
            "tensor(0.0947, dtype=torch.float64)\n",
            "184\n",
            "tensor(0.0947, dtype=torch.float64)\n",
            "185\n",
            "tensor(0.0967, dtype=torch.float64)\n",
            "186\n",
            "tensor(0.0967, dtype=torch.float64)\n",
            "187\n",
            "tensor(0.0028, dtype=torch.float64)\n",
            "188\n",
            "tensor(0.1102, dtype=torch.float64)\n",
            "189\n",
            "tensor(0.0967, dtype=torch.float64)\n",
            "190\n",
            "tensor(0.0957, dtype=torch.float64)\n",
            "191\n",
            "tensor(0.1013, dtype=torch.float64)\n",
            "192\n",
            "tensor(0.1101, dtype=torch.float64)\n",
            "193\n",
            "tensor(0.0957, dtype=torch.float64)\n",
            "194\n",
            "tensor(0.0971, dtype=torch.float64)\n",
            "195\n",
            "tensor(0.0972, dtype=torch.float64)\n",
            "196\n",
            "tensor(0.0950, dtype=torch.float64)\n",
            "197\n",
            "tensor(0.0950, dtype=torch.float64)\n",
            "198\n",
            "tensor(0.0939, dtype=torch.float64)\n",
            "199\n",
            "tensor(0.0949, dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca_cEykSooKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQUaOOUregUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(pred[0][-1][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tttv1E-ck9GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(tru[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMj4c3H9eoG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(tru)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHqZrAbPk_bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tru"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAQ4Ai4nervO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b(pred[0][0], tru)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oFJmQRyRI-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d = train[20000]\n",
        "d = train[20000:20010]\n",
        "print(d[0].shape)\n",
        "plt.imshow(d[1][0])\n",
        "o = d[0][0]\n",
        "for i in range(10):\n",
        "    plt.figure()\n",
        "    plt.imshow(o[i][0])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJHzwzCmHogM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pwd\n",
        "%cd data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9IZQNO4Ft1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \n",
        "b = nn.MSELoss()\n",
        "\n",
        "avg = np.load(\"dset4_avg.npy\")\n",
        "std = np.load(\"dset4_std.npy\")\n",
        "apbln = [1,0,0,0,1] # think this is correct\n",
        "    \n",
        "wrapper_full(\"first_run_full_data\", 10, structure, b, avg, std, apbln, lr = 0.005, epochs = 200, batch_size = 200)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}