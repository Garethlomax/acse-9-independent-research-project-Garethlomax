{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_model_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqF8p1rz-Sht",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJMHV0xH-Vbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from conflict_lstm.latest_run import *\n",
        "from conflict_lstm.hpc_construct import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.enabled = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veK5KTD6-yOL",
        "colab_type": "text"
      },
      "source": [
        "## Functional form\n",
        "The wrapper function carries out most of the heavy lifting when we wish to train our models. We pass the structure argument (as detailed in the LSTMencdec docstrings) alongside the loss function and normalising averages calculated from our dataset. Here we modify the weights of a binary cross entropy loss function. The class weights, average, std and hdf5 dataset should be located in the local directory from which the script is run. Please note the example below is extracted from a script run on the HPC. It is not recommended to try to run this in an .ipynb notebook due to the overhead. For previously run scripts please see the results folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkpzfetM-w4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzrk_ERp-PHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining the structure of the encoder decoder to be produced inside wrapper\n",
        "structure = np.array([[12,24,0,0,0],[0,24,12,6,5]])\n",
        "\n",
        "# here we produce a weighted binary cross entropy loss function.\n",
        "d = np.load(\"weights_bce.npy\")\n",
        "weights = torch.tensor(d)\n",
        "weights = weights // 3\n",
        "weights = weights.to(device)\n",
        "b = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
        "\n",
        "# here we load in the average and standard deviation of out image sequence channels \n",
        "# for standard score normalisation\n",
        "avg = np.load(\"min_event_25_avg.npy\")\n",
        "std = np.load(\"min_event_25_std.npy\")\n",
        "\n",
        "# here we define which of our image channels need to be normalised (the channels\n",
        "# which are not normalised are pre normlised)\n",
        "apbln = [0,1,0,0,1]\n",
        "\n",
        "wrapper_full(\"bce_3\", 10, structure, b, avg, std, apbln, lr = 0.001, epochs = 2000, batch_size = 200)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}